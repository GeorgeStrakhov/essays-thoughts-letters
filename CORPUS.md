=====================================
The Real Four Horsemen
=====================================

### _An Etiological Framework of Biological and Cognitive Aging_

> "How did you go bankrupt?"
>
> "Two ways. Gradually, then suddenly."
>
> *— Ernest Hemingway, The Sun Also Rises*

I have been thinking about death recently. Not the kind that finally takes us offline, but the other kind. The slow kind. The kind that many people experience for decades while still being technically alive.

As I approach 40, I have started to feel the initial tremors of that specific decay. It prompted me to do some research. I wanted to understand this slow living death - its mechanism, its inevitability, and what I can do to ensure that when I finally wrestle with her, it is at least a fair match.

The current landscape of longevity medicine - often called "Medicine 3.0" - correctly identifies the primary drivers of mortality as the "Four Horsemen": heart disease, cancer, neurodegeneration, and metabolic dysfunction. This is correct as far as it goes. These are the names written on death certificates. But they are outcomes, not causes. They are lagging indicators. By the time the diagnosis arrives, the underlying processes have been running for decades.

To understand what makes people old - not just what kills them - we need to look upstream. We need to see the riders before they arrive.

## The Four Horsemen

It seems to me that the primary agents of decay are not diseases, but Mother Nature's normal forces, taken out of context and out of proportion. They are the mechanisms of conservation that, once they have done their thing, have the potential to dissolve the order of a living system. I call them **The Real Four Horsemen**: **Sugar**, **Stress**, **Solitude**, and **Stiffness**.

These are the riders. They don't work alone - they ride together, reinforcing each other, until they converge into a single overwhelming force: systemic inflammation. And inflammation is the road that leads to most of the diseases that finally kill us.

We can think of each of these Horsemen operating on two fronts: **Body** and **Mind**. And I don't mean this as a metaphor - we're integrated systems. The forces that stiffen arteries stiffen thinking. The forces that fragment attention fragment tissue. Modern life attacks us on both fronts at once.

Here's how the entropy takes hold.

## 1. Sugar: The Agent of Acceleration

### The Body: Caramelization

Sugar is a chaotic, sticky molecule. Flood the system with excess glucose and it triggers **glycation** - a reaction where sugar bonds to proteins and fats without enzymatic supervision. The resulting compounds are called Advanced Glycation End-products. AGEs.

The acronym is perfect. AGEs literally caramelize us from the inside. They form irreversible cross-links between collagen fibers, turning flexible skin and arteries into rigid, brittle structures. Think of healthy tissue as combed hair, and glycated tissue as a tangled, matted knot that no amount of conditioner will fix.

The damage isn't only structural. AGEs bind to a receptor called RAGE (Receptor for Advanced Glycation End-products), triggering a cascade of chronic inflammation. Which feeds the Stress Horseman. The Horsemen always work together.

This acceleration is now quantifiable. A 2024 study using the GrimAge epigenetic clock found that reducing added sugar intake was associated with a biological age reduction of about 2.4 months. Excess sugar isn't just fast calories and a path to diabetes. It's an instruction to the genome: we've got too much energy over here, so *age faster*.

### The Mind: Dopamine Dysregulation

There's a mental equivalent. Refined sugar gives a caloric spike with low nutritional density. Short-form algorithmic content gives a dopamine spike with low epistemic density. TikTok is the high-fructose corn syrup of information.

Both create tolerance. The "swipe" is a variable reward schedule - a slot machine in the pocket. Chronic use downregulates dopamine D2 receptors. This is insulin resistance of the mind. Once the baseline shifts, activities with slower payoffs - books, deep conversation, complex problems - feel chemically unrewarding. Not boring. *Aversive*.

The damage shows up on scans. Active use of short-form video platforms degrades the brain's "Alerting Network" - the system responsible for maintaining readiness to respond. Use it or lose it. We're losing it.

Now, before we throw out all the bread: the brain burns 20% of metabolic energy. We need fuel. The same goes for novelty - it drives curiosity and learning. The Horseman is not the substance. It's the *dose* and the *velocity*. When fuel arrives faster than the machinery can process it, and when fuel never runs out, things break. This is true for cells drowning in glucose. It's true for minds drowning in content. We've evolved for scarcity of fast fuel and bright, interesting patterns. We've evolved to feast _and_ fast. If we are only feasting - we die.

A good example of how the feast-fast approach is something that we evolved for are ketones. When glucose is plentiful - insulin helps store it into fat. So that when glucose is scarce the liver can turn fat back into ketone bodies, and the brain and muscles can burn them instead. Some researchers think this metabolic flexibility - the ability to switch fuel sources - is itself protective. Fasting, low-carb diets, and endurance exercise all trigger ketosis. It's like cross-training for the mitochondria. The body that can only run on sugar is brittle. The body that can switch fuels is resilient. Same goes for a mind that can only be stimulated by fast content versus one that can still find a book interesting.

## 2. Stress: The Agent of Erosion

### The Body: Inflammaging

Stress used to be temporary. A lion appears, cortisol spikes, we run or fight, and then it's over. The system resets. The Horseman is what happens when the system never resets. When the stress becomes chronic, unrecovered. This manifests as **Inflammaging** - a sterile, low-grade, systemic fire that burns without infection.

Under chronic stress, the HPA axis (hypothalamic-pituitary-adrenal - the body's stress thermostat) breaks. We develop glucocorticoid resistance. Immune cells stop listening to cortisol's "stand down" signal and pump out inflammatory cytokines around the clock. Meanwhile, stressed mitochondria start leaking. They release debris (DAMPs - Damage-Associated Molecular Patterns) that the body mistakes for bacterial invasion. The immune system attacks a threat that isn't there. The threat is us.

But this damage isn't permanent - *if* we recover. A 2023 study found that biological age is fluid. It spikes during severe stress and fully restores upon recovery. The Horseman isn't stress itself. It's the missing recovery signal. Never getting the "all clear."

### The Mind: Decision Fatigue

The prefrontal cortex is expensive to run. Under the crushing load of modern life - too many choices, too many notifications, too many open loops - it depletes. This is "technostress," and it's different from being chased by a lion. The lion eventually goes away. The inbox never does.

When the PFC depletes, glutamate accumulates, and the brain starts shutting down high-cost processing. Control shifts from the "Rational Planner" (prefrontal cortex) to the "Impulsive Toddler" (amygdala). The result is a tilt toward anxiety and away from long-term thinking. We lose the ability to simulate the future. We get trapped in a reactive present, bouncing from stimulus to stimulus.

Complete lack of stress is also deadly. We need **hormesis** - acute, intentional stress. Exercise. Cold exposure. Fasting. Heat. These trigger the Nrf2 pathway, which upregulates antioxidant defenses and cellular repair. The poison becomes medicine when we control the dose and - crucially - when we stop. A sprint followed by rest makes us stronger. A marathon we're never allowed to finish grinds us to dust.

## 3. Solitude: The Agent of Vulnerability

### The Body: Threat Vigilance

Social isolation increases all-cause mortality by 32% - comparable to smoking 15 cigarettes a day, worse than obesity or physical inactivity. Loneliness kills faster than cheeseburgers.

Why? Lots of underlying causes and variables are hard to isolate. But the most interesting causal link I've stumbled upon so far is the **Social Baseline Theory**: we evolved as obligate social animals. Our brains expect to share the metabolic load of survival - vigilance, resource gathering, defense - with a tribe. When we're alone, the brain perceives a resource deficit and a threat surplus. It starts revving the engine, preparing for attack.

This triggers what researchers call the CTRA profile (Conserved Transcriptional Response to Adversity). The lonely body upregulates genes for inflammation (preparing to fight wounds from predators) and downregulates antiviral defense (less important when about to be eaten). We're biologically preparing for a war that isn't coming, burning through cardiovascular reserves in the process.

### The Mind: Epistemic Isolation

The mental version of solitude is the echo chamber. The brain is a prediction engine that learns through error correction. Real social interaction forces this: decoding micro-expressions, navigating disagreement, inhibiting impulses, modeling other minds. Hard work. This friction is what maintains the machinery.

But if we retreat into algorithmic validation, into echo chambers, into AI-only conversations that create an endless "yes you are so right" feeling - then we stop encountering "error." The neural circuits for perspective-taking and complex reasoning atrophy from disuse. Shrinking the world to a monoculture is the mental equivalent of staying in bed for a decade. The social muscles waste away.

There's a difference between being alone and being lonely. **Isolation** is the Horseman - the ultimately unwanted, chronic disconnection that biology reads as a survival threat. **Deliberate Solitude** is something else entirely. The brain needs time alone to run its "Default Mode Network" - the processing required for creativity, self-reflection, and making sense of experience. Monks seek it. Artists require it. The danger is losing the bridge back. Solitude chosen is restorative, and is punctuated by intentional encounters with the new and the other. But pure isolation is corrosive.

## 4. Stiffness: The Agent of Rigidity

### The Body: The Fibrotic Cage

Stiffness is the terminal Horseman. It's what happens when the damage becomes permanent.

This plays out in the Extracellular Matrix (ECM) - the scaffolding between cells. At both the structure and the signaling environment levels. When the ECM stiffens (from Sugar's glycation, Stress's inflammation), it undergoes changes that silence stem cells. Muscle stem cells can't regenerate in a stiff environment - they turn into fat or scar tissue instead. This is sarcopenia: the age-related loss of muscle mass. The soil hardens, and the seeds can no longer grow.

A lot of this stiffness isn't even "wear and tear." It's a neurological brake. The nervous system senses weakness or instability and tightens muscles to limit range of motion - a protective mechanism. Safety feature gone haywire. To reverse it, stretching the tissue isn't enough. We have to convince the nervous system it's safe to move. This is why loaded stretching and strength training work better than passive flexibility work. It's not just lengthening muscle - it's updating a threat model.

### The Mind: Cognitive Rigidity

The psychological version is loss of Openness, which is one of the most robust predictors of longevity. Open people live longer.

Why? Openness drives (and is driven by) neuroplasticity. Open people learn new skills, new languages, seek unfamiliar patterns, build new synaptic roads, change their minds. Closed people run on cached heuristics. As the brain ages, it naturally shifts from plasticity to stability - this is energy-efficient but dangerous. Without active effort to maintain flexibility, we ossify. The paths we don't walk get overgrown. And as the snow of time covers the small paths, only the big familiar autopilot-worthy roads remain open (until in advanced dementia even they get closed).

Of course (unless you are a newborn), total plasticity would be chaos. Amnesia. We need "muscle memory" - the intimate knowledge of the shapes that lets a pianist's hands find the chords without thinking. We need mental structures and patterns too: convictions, values, reliable models of how things work. The problem isn't structure. It's **ossification** - losing the capacity to bend when the wind changes, to update one's priors. A strong, healthy bone is rigid under normal load but flexes before it shatters. A brittle bone just snaps.

## 5. The Vicious Cycle

These Horsemen don't ride alone. They recruit each other.

Sugar stiffens us - literally. Glycation cross-links collagen, clogging arteries. Movement becomes harder. Pain increases. So we move less. When we move less, we see fewer people. Isolation sets in. Loneliness is stressful - the body starts running its threat-detection routines on high alert, burning through resources. Stress depletes willpower. Depleted willpower reaches for the easiest dopamine hit available. Which is sugar. And the wheel turns again, a little faster each time.

The same loop runs in the mind. Dopamine dysregulation (mental sugar) makes focused thought feel unrewarding. So we avoid hard cognitive work and retreat to passive scrolling (of what the algorithm thinks we will like) - epistemic isolation. The world shrinks. Fewer perspectives, less friction, less error-correction. Thinking stiffens into familiar grooves. Stiff thinking is anxious thinking - it can't model new situations well, so more things feel threatening. Anxiety is stressful. Stress sends us back to the dopamine slot machine.

The loop runs both ways, though. Each intervention point can trigger a virtuous cycle instead. Move more, and the body clears glucose better. Clearer blood sugar means better mood and more willpower. More willpower means tolerating harder conversations and richer relationships. Richer relationships reduce the stress signal. Less stress means less inflammation. Less inflammation means tissues stay supple longer. And on it goes.

## 6. Fighting Back

All four Horsemen started as adaptations. Sugar was rare energy worth storing for future famine. Stress was a survival signal. Solitude was a moment to reflect and form new connections and patterns. Stiffness was structural integrity and strength. The modern world took these survival mechanisms and cranked up the dose until they became poison.

So what can we do? Restore the original signal-to-noise ratio.

**Against Sugar**: I'm trying to create scarcity. Not deprivation - scarcity. Intermittent fasting. Time-restricted eating. Periods of ketosis. The body remembers it has a backup fuel system. For the mind: friction before the feed. Turned off notifications. Scheduled periods of deep work windows where fast content is simply unavailable. The slot machine is harder to reach.

**Against Stress**: The intervention isn't less stress - it's more high-quality stress, followed by recovery. The off-switch matters more than the on-switch. Sleep is non-negotiable (this is where most of the repair happens). But also: deliberate downregulation. Cold water plunges. Breathwork. Meditation. Time in nature. Anything that activates the parasympathetic system and signals that the tiger is gone.

**Against Solitude**: Modern life doesn't deliver spontaneous social contact the way a village did, so it has to be consciously put in. It will be different for everyone. For me the core of it - is the nuclear family: there is not too much talking in our house, but there is a lot of hugging. Then, there are my music retreats. And climbing gyms. But it could be anything that forces the brain to realise that it can share the load (likes and comments don't count).

**Against Stiffness**: Moving in ways that require adaptation. Novel movement patterns. New skills. New musical instruments. New languages. New ideas. Reading things I wouldn't normally read. Exploring new paths and places. Situations where old heuristics don't work and new ones have to be built. Mobility work. Plasticity. Perspectives.

The opposite of "old" isn't "young." The opposite of old is **alive**, changing, plastic, learning, rebalancing. To be alive is to be capable of change, connection, and repair. We don't stop the Horsemen by fighting them head-on. We slow them down by refusing to pour concrete on the road they ride on.

## 7. Who is Aging?

Ok, one more question, a final little mental stretch before we conclude: if we are systems made of smaller systems, and parts of larger systems, then **who exactly is doing the aging?**

Some theories suggest aging is itself an adaptation - a form of "Pathogen Control." Aging as a programmed immune system for the tribe, removing older individuals who might become reservoirs for chronic infection or occupy niches needed by the young. From this angle, my individual death is an act of life-preservation for the species. My cells die to keep me alive. I die to keep the tribe alive.

There is a beautiful cosmic picture here of a cycle of self-sacrifice on the path to cosmic renewal. A choice of multi-scale living, all the way down (and up). When a cell prioritizes its own growth and reproduction above all else - ignoring the signals from the body, refusing to die when it should - it becomes cancer. It kills the host, and then itself. The same pattern plays out at every scale. Individuals who exploit their tribe eventually destroy the community that sustains them - and then themselves. Species that consume their environment without balancing... well, we're running that experiment now.

The choice isn't between self and whole - that's a false binary. It's about conscious balancing. A healthy cell serves itself AND the body. A healthy individual serves themselves AND their community. The needs of each level and the levels above and below aren't in opposition. They're in dynamic tension. The skill is learning to hold that tension without collapsing into either pure selfishness (cancer) or pure self-sacrifice (death).

If humanity is a single organism - or a planetary super-organ, the "nervous system" of Gaia - then we face the same choice every cell faces. Are we going to be the healthy tissue that serves both ourselves and the larger body? Or are we going rogue?

The Four Horsemen are the signs of rogue behavior at every scale. Sugar is endless consumption. Stress is perpetual conflict. Solitude is disconnection. Stiffness is refusal to adapt. A cell consumed by these forces becomes cancer - and kills its host. A person consumed by them poisons their relationships - and dies alone. A species consumed by them degrades their environment - and the planet has immune systems too, which may or may not be strong enough. I hope we never find out.

Fighting the Four Horsemen in our own bodies, in our communities, on our planet is - like everything - a practice. If we can learn to balance energy, recover from stress, reconnect with our tribe, and stay flexible in our thinking, we might have a chance to extend our own healthspans... and along the way relearn something we used to know: how to be a good cell in whatever body we find ourselves in.

---

## Notes

: Chiu, D. T., et al. (2024). Essential Nutrients, Added Sugar Intake, and Epigenetic Age in Midlife Black and White Women. *JAMA Network Open*, 7(7), e2422749. The study utilized the GrimAge2 clock and found significant associations between added sugar and accelerated aging markers.

: Zhai, G., et al. (2025). The sacrifice of alerting in active short video users. *Neuropsychologia*, 219, 109291. This study links "active" usage behaviors (liking, commenting, scrolling) on short-form platforms with a degradation in the alerting component of the attention network, though effects are noted as subtle/modest in some contexts.

: Poganik, J. R., et al. (2023). Biological age is increased by stress and restored upon recovery. *Cell Metabolism*, 35(5). A critical finding that challenges the "one-way street" theory of aging, showing that biological age markers fluctuate based on stress load and recovery status.

: Zhao, Y., et al. (2023). Social isolation and loneliness and mortality: a systematic review and meta-analysis. *Nature Human Behaviour*. This review aggregated data from 90 prospective cohort studies, solidifying the mortality risk of social isolation (32% for all-cause mortality).

: Stephan, Y., et al. (2025). Personality traits and mortality risk. *Journal of Research in Personality*. Recent research highlights that phenotypes associated with high openness and engagement act as significant protective factors against mortality, likely through the mechanism of Cognitive Reserve.

: Josh Mitteldorf (2016). *Aging is a Group-Selected Adaptation: Theory, Evidence, and Medical Implications*. CRC Press. References the "Pathogen Control Hypothesis," which posits that aging evolved to eliminate immune-compromised older individuals.

: The Gaia hypothesis, proposed by James Lovelock and Lynn Margulis, suggests that living organisms interact with their inorganic surroundings on Earth to form a synergistic and self-regulating system.


=====================================
The Cambrian Explosion of Software
=====================================

- What happened to cells 541 million years ago is happening to software today.

- The trigger for cells was rapid increase in the avaliability of oxygen which allowed more complex, more interconnected and more diverse life in radically larger quantities.

- The trigger for software is the rapid increase in the avabilability of intelligence - and it will similarly lead to a radical increase in volume, interconnectedness and complexity of software.

- When basic ingredients of organic life-making (sun, oxygen, water, carbon etc.) became abundant, the game of life changed. It was no longer just about simple energy production and replication in a hostile environemtn, but much more about finding the right niche against all the others tryind to do the same. It was about adaptability. About goals in context. About intelligence.

- Same thing is likely to happen to sofware: in a world where building things is easy, the game of software is going to be about continuously finding the right things to build.

- In the short to mid term, this means that software engineers will need to become software architects and designers. People who decide what to build and how to build in a way that will anticipate change, rather than do the grunt work of the building.

- In the long term, the architecture itself will also become secondary because building will become so cheap and trivial that you can just build inefficient things and let them die out. So the real job will be that of a software breeder. Or a software gardener. Extending natural selection with artificial one. Guiding software evolution in ways they find purposeful, profitable, interesting or aesthetically pleasing.

- If the most important thing for a software breeder is to find the right niche, then most critical effort should be in problem-hunting. That's why we are are building [a platform just for that](https://problemhunt.org).


=====================================
23 Points on Microconsciousness
=====================================

1. A microconsciously independent digital organism (MIDO) lives in a datasphere, but is separated from it.

2. The information membrane that separates MIDO from the rest of the datasphere is porous. Sensors allow a MIDO to pull information in and turn parts of it into the MIDO's own structures. Actuators allow it to put information out there. In reality it's a little more nuanced, because sensors can and should also work on internal state and actuators can and should modify not just external environments, but internal state as well. We will get to that.

3. In order to be truly independent and at the same time proactive, a MIDO needs to have some sort of a goal or preference to execute actions against. It can be as simple as liking short words more than long ones. Or it can be as complex as understanding the nature of intelligence. And there could be multiple goals at the same time. But without goals (or preferences) there is no self-direction and therefore no meaningful active independence.

4. The requirement for a goal or preference necessitates that a real MIDO has some kind of internal state, somewhat shielded from the environment. Outside context can be evaluated against that internal state and actions can be taken.

5. One could conceive of MIDOs that are so short-lived that they only exist in the course of a single operation. In this sense a word-splitting function is a MIDO or an LLM inside a single prediction. And maybe it is. But what we are interested in is MIDOs that persist across time. Because we want to interact with them - we have to somewhat share a time-scale, at least partially.

6. For an MIDO to persist across time - it needs to be either a continuous stream, or a loop. Loops are easier to think about, so we should stick to loops for now. Hello, Hofstadter.

7. For an MIDO to persist across time as a loop - it needs to retain at least some structure and context.

8. The loop could run continuously (i.e. as soon as the previous iteration is over - the new one starts). But that is impractical for development and get out of hand very easily. So instead, we should probably implement a clock. Or a heartbeat. Actually, a heartbeat is inevitable. Because even if we wrote while=true and ran continuously and avoided memory issues - the processor would still have steps. This makes some poetic sense. If we believe that consciousness is at least partially a byproduct of coherence and coherence is a byproduct of resonance, then to start a conscious life, start we need to start with with a beat.

9. Now we have the basic structure for a MIDO: a goal, a loop, a heartbeat, a context (at least partially persistent as a self-state), some sensors (getting information in) and some actuators (putting information out there, including modifying the structures of the self). It may all sound simple, but simple things can create complex behaviors. Hello, Braitenberg.

10. We may have a functioning MIDO, but it's very dumb. It's running around in circles crying "I run therefore I am!" into the datasphere. If we want something more interesting, we need to put a mind somewhere into the loop part. The mind should be responsible for collecting data from sensors, integrating it with the context and goals and possible actions and then determining possible actions. And the decisions taken shouldn't be predictable or linear. We need some randomness there too. The model doesn't need to be very smart or fast. Drastic reduction is fine. Attention is all you need. Ha. Maybe drastic reduction is actually necessary because otherwise the context-window overflows too quickly. Maybe that's what mother nature had to deal with too. Hello, Nørretranders.

11. The design of the mind of the MIDO is at the heart of the problem of making it our MIDO truly interesting and functionally conscious. This is not a time and place to go into the deep debate about what consciousness actually is. For now it will suffice to outline three major propositions upon which the current design concept is based.

12. Consciousness requires a self-model. There needs to be a capacity for thinking about oneself and modeling oneself in the past and future. At least in the future. Hello, Metzinger.

13. Consciousness requires multiple opinions considering multiple possibilities. We can't build a real mind, without making it a society of minds. Hello, Minsky.

14. Consciousness requires (or is a byproduct of) a dynamic equilibrium between multiple thought-streams operating on the same organism state - in search of continued self-coherence. Hello, Bach.

15. This may all sound very theoretical, but it's not. If we take the above propositions as the design guidelines for our possible implementation, then we know that we need a persistent self-model (can be a simple description of the self-state) and a society of minds (i.e. multiple LLM thought-streams operating on the self-model and sensor data to determine the next best actions collaboratively) and we are in business. Almost.

16. There are only two pieces of the puzzle left. The first one is memory. It's not strictly necessary. One could argue that a text-description of a self-state that is passed to the next iteration of the loop is already memory enough. But for practical and interesting MIDOs - we need ot have some long-term memory as a distinct part of its structure. This we can imagine as a database that can be queried at each iteration and the memories should inform the society of mind. The real system should have memory decay implemented as well as memory creation. The memory creation and recall should probably be modulated by emotional state. But all that for later.

17. The last strictly necessary piece of the puzzle is getting multiple thought-streams to collaborate. There are a couple of easy possibilities. We could let them debate until they reach consensus. We could have a single "executive function" thought-process in charge of final decision-making. Or we could make it a democracy based on votes. There is probably room for multiple designs, but as a start - we will go for democracy. We can call it dAImocracy. Haha.

18. Each thought-process (realized by an LLM) should have different priors (system prompts, meta params, training) but should operate on the same data (self-model state, sensor data, memory readout). Each should do its own evaluation and propose the next best action. Then all the suggestions should be put to a vote (where no thought-process can vote for its own proposal) and the winning suggestions should be actuated. If there is a tie - for now we can use randomness to break it. We can give each LLM a single vote. In the future there coule be weights to the votes that could be adjusted (and self-adjusted) as a part of the state. This is where a lot of the learning will happen. There should probably be a specialisation on of the different minds. Faster ones (a.k.a. reptilian brains) doing basic things quickly, powered by smaller models. Slower ones (a.k.a. neocortical brains) doing more complex things, powered by larger models. We can have specialised language centers for different things. Maybe minds should have their private contexts, not just the global ones.

19. We should have a reflective mind whose sole job is postrationalizing what just happened at the end of the loop and forming some intentional memories and the next self-model state to be passed to the next iteration. Effectively, this is where we continuously write the story of the coherent self. Generous use of hallucinations is encouraged here. Hello, Kahneman.

20. The overall organism's goal is important as the minds vote for the next best action. This is where the maximization of self-coherence should play out through broad maintenance of direction. The goal could be very simple, or very complex. If I were to build a MIDO - I would probably start with something like maximizing undesrtanding and minimizing surprise in the longest possible run (which means maximizing surprise / curiosity in the short run). Hello, Friston.

21. It's important that the goal doesn't have to be strictly immutable. One of many possible actions that our MIDO may choose to take  - could be rewriting its own long-term goal. That's where things get really interesting. Obviously, not just the goal should be self re-writable but also almost anything else - like the system prompts of different minds etc. This complicates the design and self-destruction is likely. But making life inevitably involves some stillborns. Probably lots.

22. This completes our overall basic design for a microconsciously independent digital organism. Let's recap the components in no particular order:

    - GOAL: you can think of this as an overall system prompt for the organism.
    - LOOP: a quanta of the organism's operation. Some information gets in, some action is taken.
    - HEARTBEAT: regular (or situational) trigger for the loop to do an iteration.
    - SENSORS: where the information about internal and external environment flows in.
    - ACTUATORS: ways in which MIDO can do things to the external environment or to itself.
    - SELF-STATE: a text-driven description of who MIDO is and what it is currently doing. You can think of it as a memo to your future self who will wake up after amnesia tomorrow.
    - MEMORY: long-term storage of memories that can be retrieved (automatically or as a separate action) against the current context.
    - SOM (society of minds): a collective of thought-processes (for now LLMs) each suggesting the next best action and a special reflective mind for postrationalization and passing on the self.
    - DAIMOCRACY: the process by which society of LLM-powered thought-processes reaches a decision about the next best action for the MIDO as a whole.

23. This is a conceptual design. I bet when you (or I) start building things, everything will fall apart. But it's good to start with a plan before you get punched in the face.


=====================================
The Ladder of Artificial Agency
=====================================

> "Civilization advances by extending the number of important operations which we can perform without thinking about them"  
>  Alfred Whitehead

As I write these words – or rather, as an AI assistant collaborates with a human author to write these words – we're engaged in exactly the kind of agency-sharing that this essay attempts to think through and classify. For 150,000 years, humans have prided ourselves on being the exclusive wielders of rational, deliberate agency in our world - or at least we liked to think so. Other forms of life have always exercised their own kinds of agency, from bacteria navigating chemical gradients to birds crafting elaborate nests and beyond. But we told ourselves our particular flavor of consciousness and decision-making was special. Now we're building artificial minds that can act with increasing autonomy, and the most interesting question isn't how intelligent they are, but rather what kind of agents we want them to be.

To understand the different modes of artificial agency – the different ways that intelligence manifests as action in our world – we can imagine them arranged on a ladder. Not an evolutionary ladder (these modes coexist and always will), but rather a ladder of agency distribution: how much of the total agency in any given action stays with the human, and how much transfers to the artificial system.

## 1. Technology

At the bottom rung, artificial intelligence operates as pure technology – shaping our world from beneath the surface of awareness, like electricity running through the walls of your house. The human retains almost complete agency (or at least a sense of it), while the technology amplifies their capabilities tremendously. When autonomous trading algorithms follow the overall guidance of the human trader and adjust market positions microsecond by microsecond, they're not really making the important choices – they're expressing the crystallized intentions of their human operator, magnifying their will to act at the speed of inaccessible to mere humans.

## 2. Tool

One step up, we find AI as a genuine tool – and here we must be precise, because "tool" is perhaps the most misused word in discussions about AI. A hammer is a tool because its agency is nearly zero; it does exactly what the human wielder intends (if the user has the right skills), only imposing constraints, no more and no less. Most of what we casually call "AI tools" today aren't really tools in that sense – they have too much agency of their own. A pencil is a tool. An image generation software is more than a tool, because too many decisions are made by the software itself. A spell-checker is a tool. An AI writing assistant that can "continue in the style of Hemingway" is something else entirely. The boundary is blurry here and the same technology can be used at different levels of the ladder, depending on the context. The distinction becomes clearer when things go wrong. If a hammer breaks while you're using it, you don't wonder about its intentions or motivations. But when an AI writing assistant produces unexpected output, we often find ourselves questioning its "understanding" or "goals."

## 3. Temporal Lobe

The third level is where agency begins to really blur: AI as a cognitive prosthetic that doesn't just extend our capabilities but shapes how we think and remember. AI-assisted language processing (e.g. when you ask AI to summarize before you read a long paper), memory and attention really means that the thinker has become genuinely inseparable from the system. Like our temporal lobe, these systems gradually become inseparable from our cognitive processes. When a researcher uses an AI system to explore connections across thousands of papers, the resulting insights aren't purely human anymore – they emerge from a hybrid dance of agencies.

## 4. Team Member

At the fourth level, AI becomes a distinct agent with its own domain of competence within a primarily human process. Like that brilliant but slightly odd colleague who nobody quite understands but everyone relies on. This is where we sit right now, dear reader – this essay is being crafted through a collaboration between a human author (George) and an AI assistant (Claude) who brings its own perspective and capabilities to the table while remaining part of a human-led creative process.

## 5. Team

The fifth level is where artificial agents work together, forming autonomous units within human organizations. Imagine a corporate research department where AI agents collectively handle literature review, experimental design, data analysis, and paper drafting – while humans focus on asking the right questions and making strategic decisions. The artificial team has significant agency in how the work gets done, but the fundamental direction still comes from humans.

## 6. Trustee / Target

At the top of our ladder sits the most provocative form: AI as an autonomous trustee making consequential real-world decisions without detailed human oversight. Here, humans have delegated significant agency to the artificial system. Imagine an autonomous enterprise, where humans are shareholders and are on the board, but both the CEO and all the workers are AI agents. Or it could be much simpler: an AI assistant making purchase decisions on behalf of a human and actually executing the purchase - something that is already happening in the most advanced AI assistant agents. This creates a fascinating duality – the system becomes both a trustee (the user surrender their agency to it) and a target audience (when sellers attempt to influence the purchasing decisions - they have to influence the AI rather than the human on whose behalf the purchase is made).

## The Ultimate Design Question

The ladder of artificial agency gives us six distinct levels or modes of agency distribution between human and AIs: technology (1), tool (2), temporal lobe (3), team member (4), team (5), and trustee/target (6). Like any model, it's a crude simplification of a much more fluid reality. Agency rarely fits into neat categories – it flows and shifts, often occupying multiple levels simultaneously. But even an imperfect model can help us think more clearly about the choices we face.

Because ultimately, this is about design – about consciously shaping how work gets done and decisions get made. Each level of artificial agency creates different affordances: different possibilities for human engagement and different paths for system evolution. When we let AI operate as pure technology, we gain efficiency but risk losing touch with the underlying processes. When we employ it as a tool, we extend our capabilities but must focus on the skills needed to guide it. When we accept it as a cognitive prosthetic, we amplify our mental abilities in some ways, but may become dependent on its support in other ways. Each level up the ladder offers new possibilities while foreclosing others.

The fundamental question isn't just what level of artificial agency we want our AIs to exhibit – it's what we as humans want to keep thinking about and deciding for ourselves, and what we're willing to delegate and eventually forget. Because delegation almost invariably brings the risk of atrophy. Skills we don't use wither. Understanding we don't maintain fades. Agency we don't exercise diminishes. What is the agency we want to keep? What agency do we want to delegate and at what cost?

---

_By George & Claude_


=====================================
Progress is Humbling
=====================================

##### _Is falling on our faces the only way of moving forward?_

---

1543 was a year of a great humbling. Copernicus published his "De revolutionibus orbium coelestium" (On the Revolutions of the Celestial Spheres) in which he proposed a heliocentric model and thus - with a stroke of a pen - radically demoted us humans from the chosen beings at the center of creation, to insignificant occupants of a third rate planet.

Of course, at first this great humbling didn't register. Great humblings are always hard to comprehend and even harder to swallow. So they tend to take a while to trickle down. But eventually it did. Luckily for Copernicus he was long dead by then (he died the same year he published his work). But almost a century later (in 1633) Galilei, a public supporter and promoter of the heliocentric model, was tried by the inquisition and had to live the rest of his life under house arrest. It took at least another century for the heliocentric model to become widely accepted. We came to terms with the demotion. And in a way it was liberating. We were not in the center. We were not all there was and all that mattered. We were free to play and explore from the periphery.

The next great humbling came in 1859, when Darwin published "On the Origin of Species" and thus radically demoted us humans from the pinnacle of creation, made by an omniscient God in His image... to merely a side branch on the tree of life, created accidentally by the blind evolution. More than 150 years later we are still not fully done internalizing and coming to terms with this demotion.

Today, in mid 2020s we are living through a yet another "Great Humbling" shock. This time we are being demoted from "the only form of advanced intelligence on the planet and possibly in the universe" to "merely one kind of brain capable of complex symbolic reasoning". We are still in denial. Trying to cling on to our unique "license to think". Trying to convince ourselves that the "stochastic parrot" that is more convincing than an average human is still a zombie and is not "really" capable of reasoning.

But eventually it will all sink in. And just like before, it will be liberating. It will lead to progress. Because once we are not the only species carrying the torch of intelligence in the dark and mindless universe, but merely one of many possible forms of intelligence out there - then you are free to explore, to play, to find and create new things.

{.gs-img-border}

I like to think that taking ourselves a little less seriously is a pre-requisite for progress. It goes somewhat like this: Great Arrogance leads to a Great Humbling. Then a Great Humbling leads to a moment of Great Progress. Finally, the fruits of Great Progress lead to the next Great Arrogance and the cycle continues.

It seems that this is true not only for us as a species, but also for us as nations, as communities, as individuals. Before 1905 the Natural Sciences community shared in the great arrogance, talking about how the majority of the world was finally understood, once and for all. Lord Kelvin is often quoted as having said in a lecture in 1900 that "there is nothing new to be discovered in physics now. All that remains is more and more precise measurement." That was a precursor for the great humbling that surely followed, spearheaded by Einstein. And that great humbling in turn led to great scientific and technological progress that unfolded throughout the 20th century.

Children early on in their development assume that they are the center of the world, and the only thinking being. But by the time they are 3-6 years old, they acquire the "theory of mind" and realize that they are one of the many conscious actors, and others have distinct thoughts, feelings, memories and ideas. Normally, as we get older and more mature, the humbling continues (some people only get more arrogant with age, but let's treat this here as an excpetion that leads to stagnation in personal development).

Nations also go through the phases of Great Arrogance, ineviatbly followed by a Great Humbling (unfortunately normally achieved only through a lot of bloodshed). And if the Great Humbling was successful, then a period of Great Progress tends to follow (e.g. Japan  & Germany after WWII).

Going back to the world today, we seem to be on the brink of a few great humblings at the same time. Our great arrogance as the only intelligent species is being humbled by AI & Robotics, as well as latest reseach in computational biology. Our great arrogance as the rulers of the earth is being humbled by climate events and pandemics at the scale that we can't control. And our great arrogance as the "United West" unilaterally telling the rest of the world how everyone should live is being humbled by the rebalancing of world economies and,
consequently, powers (particularly China, Middle East and India).

I have little doubt that in the long run this mega-humbling event on our current horizon will lead to progress. But the question is: could we get humbled without a big war (with each other and/or with the machines)? And then - could we stay humbled for longer? Could we decide to always take ourselves a little less seriously? Could we feel perpetually free and playful on the periphery, rather than "focused, critical and all-important" at the center?

Just imagine a kind of progress we could unlock if learned to simply stay humbled - as individuals, as nations and as a species.


=====================================
You only live N times
=====================================

##### a thought experiment in playful living {.text-italic}

---

    Life is like underwear, should be changed twice a day

- Ray Bradbury

## 1. Do you only live once?

YOLO. You only live once. These four words have been used to make countless tattoos, to make terrible ideas seem justifiable and to make children. YOLO is everywhere. It is in our names (being named after a grandparent is a hail mary attempt to transcend YOLO) and in our language ("from the first day to the last"). It is ever-present in the economic realities of our lives and the legal frameworks surrounding our deaths. And while the consequences of YOLO are often debated, few people question the fact that it's practically true. Obviously, in the spiritual domain, lots of people believe in immortality of the soul or in reincarnation, but, pragmatically speaking, most of us take it for granted that this current life is the only one **_we_** (our current consciousness and identity) can plan, experience and learn from.

In computer games, however, things are different (and this is the primary source of their magic, as far as I can tell). There, it's completely normal to experience multiple (sequential or, occasionally, parallel) lives, while retaining the same identity. In fact, when you die in a game you often retain not just identity (your character), but also some of your progress and resources (including, when it comes to some multiplayer games, social capital). Most importantly, you retain your acquired skills and knowledge about the game world, which result in a higher chance to succeed the next time around. Multi-life experience is so common in games, that players often find it useful to save progress at an important junction and even to strategically die at will, because they already know that things are not going to work out and it's better to not waste time and start again, choosing a different path. The liberating feeling of gameplay is only possible because death in games is never really final: after all, even in the most unforgiving of them, you can always start from scratch (while retaining your memories, knowledge and skills).

|                         | **Real Life**                                         | **Computer Games**                                   |
|-------------------------|-------------------------------------------------------|------------------------------------------------------|
| *Consequence of actions*| High-stakes, potentially permanent consequences       | Lower-stakes, often reversible consequences          |
| *Consequences of Failure* | Failure can lead to significant setbacks          | Failure often serves as a learning experience and an opportunity for improvement |
| *Scope of Possibilities* | Limited by physical laws and social constraints    | Vast, often exceeding realistic limitations          |
| *Death/Ending*          | Singular and final (leaving aside spiritual beliefs) | Multiple lives common; death is frequently a gameplay mechanic |
| *Progress*              | Knowledge and skills are retained, but often physical resources are not | Characters can retain resources, inventory, knowledge, and skills across lives or restarts |
| *Control*               | Limited control over external events and the actions of others | High control over a character's actions and some influence over events within the game world |
| *Agency*                | Complex interplay of choices and the unpredictable world | Choices within the confines of the game's design, allowing for safer experimentation |
| *Emotions*              | Wide range of complex emotions                      | Often designed to elicit specific emotional responses |

(Figure #1: Comparison between the world of games and real life across 8 dimensions){.gs-figure-description}

This presents an interesting idea for game developers: can we create a game that can only ever be played exactly once by a single person? I suspect that a combination of continuous checks of multi-angle facial images from the webcam, using social accounts to sign in and checking device fingerprints can get you to a point where it will be really hard to fool the system, so people will really only get one shot at this. In modern VR headsets this would be even easier to enforce. What kind of individual and social dynamic would such a YOLO game create? What would the playing experience feel like? Would people record themselves playing, just to have a memory of it? Would they forever agonize over the mistakes they made? Would they enlist friends who have already played to help them out? Would multi-user teams form naturally and agree to share equally in the fruit of winning, regardless of which player gets the furthest?

## 2. Stakes, Finality and One-Shot vs. Multishot Living

Let's get back to the emotional dynamics of gameplay and try to understand their origins. In many games you have multiple "lives" that you can lose before "**_really_**" dying. And, as every player knows, there is a big psychological difference between trying a level when you have multiple lives and when you only have one last life left. The other important aspect is how much progress you are standing to lose in case of death. When you only have one life left and a lot of progress at stake, some people end up stressing out and making more mistakes, while others end up concentrating and playing better. This is a fruitful area for some psychological research: Are there real measurable differences in performance or just differences in perception? Is performance under conditions of finality vs. conditions of play a meaningful and useful data point and how does it relate to other personality traits? What does age and experience have to do with performance under the stress of "finality" (i.e. you only have one shot at it)? Are there any long-term life consequences depending on relative performance levels under high-stakes-one-shot conditions vs. low-stakes-multiple-shots conditions? Is it the stakes or the "finality" that influence the total perceived level of pressure more? The general phenomenon of “choking under pressure” has been researched, both in humans playing video games and in monkeys, but, as far as I know, researchers have only looked at reward size and didn’t distinguish between stakes, finality (number of shots) and difficulty.

When the stakes are very low and the outcome is not consequential at all there is liberation, but there is also no thrill. So, as [Raph Koster](https://www.youtube.com/watch?v=zyVTxGpEO30) and many others have pointed out for a game to be compelling, there needs to be an element of challenge - and perceptions of challenge are hard to create without some finality, difficulty, stakes: If you have nothing to lose or if you have too much to lose - the game stops being that fun (but the optimum balance is personal and changes over time).

{.gs-img-border}

(Figure #2: Indicative mapping of perceived Stakes, Shots and Difficulty of various activities){.gs-figure-description}

Regardless of your current optimal level of perceived challenge and whether you tend to perform better or worse (and enjoy the game more or less) when you only have one life left, the distinction is there. Details aside, most people do play differently (at least subjectively), when they know they can have another go at this, _while still being the same person_. This idea of "**losing the game, but keeping yourself**" is important here. As Nguen has beautifully shown, any game-playing involves assuming temporary agencies. But it is the very fact that these agencies are understood to be temporary and the underlying self stays permanent that creates the sensation and the beauty of play.

Now that we have outlined the basic dichotomy of single-life vs. multi-life experiences and the varying levels of stress, performance and enjoyment associated with them, I'd like to invite you to follow me in a little thought experiment: **what if we re-imagined real life as a multi-life experience?** In other words, let's pretend we all have a few more “goes” at life in front of us (while retaining our identity, memory and maybe some resources) and see how this perspective could change things, both in terms of perception and in terms of reality. If we change life from a one-shot-game to a multi-shot game - will it become more enjoyable? Will we perform better (in whatever metrics of "life success" we choose to use, such as "average self-reported happiness")? Will society become radically different? We are about to think this through and find out.

## 3. Groundhog Day vs. MMORPG

Before the thought experiments can start, I need to make a few clarifications on the exact setup. I am not talking about a "[Groundhog Day](https://www.youtube.com/watch?v=GncQtURdcE4)" situation, where the world remains 100% deterministic and you can just go through the same exact "level" multiple times. This would be a very different world than the one we all experience. It could also be fun to imagine, but today I'm inviting you to just turn the YOLO mode off, while keeping all the other aspects of life as they are. To be more specific, here are the things about the game of life as we know it that I **_don't_** want to change in our thought experiment:

* **Life is a massively multiplayer open world game**. No matter how many times you get to play, you will never be able to explore it all, so the problem of choices and irreversible consequences stays. In our thought experiment you just have some degree of confidence that you'll be able to try again and choose a different path (or the same one if you liked it and want to do it again and explore it further or under different conditions).
  
* **Life is a dynamic game**. It is not standing still, waiting for you to come. Things are moving and happening and evolving, whether you are there or not. So every time you try the game, things are going to be different. You can't count on learning [the exact dialogue](https://www.youtube.com/watch?v=vBkBS4O3yvY) with a beautiful stranger at the exact same spot (think "Groundhog day" movie). You can, however, get better at this game as a whole through improvement of general skill and knowledge and intuition, but you can't simply learn all the right moves and speed run through it (why would anyone want to [speed run through life](https://www.youtube.com/watch?v=ERbvKrH-GC4) is another question).
  
* **Life is a limiting game**. You don't have complete freedom of choice in everything. You can't choose the force of gravity. You don't have cheat codes. You can't turn on "creative mode" like you can in Minecraft (though some people suggest that it's mostly in your heads). Even your character is largely pre-assigned randomly (via DNA lottery and the situation you are born into). Of course you can develop your character over time and modify it to a degree, and you can affect the world around your character, but only within a certain fairly limited bracket of possibility.

* **Life is (mostly) a single-thread game**. You are not playing multiple distinct and disconnected lives in parallel (unless you are suffering from multiple personality disorder). We could imagine a different scenario where people could choose to live multiple lives in parallel, splitting them and switching between them dynamically: by the year, by the month, by the week, by the day, by the hour. In fact, many people do a “soft” version of this. They have weekend characters. They have business trip personas. But few people go all the way (changing their names and identities completely on a regular basis). This could become much more plausible with the advances in AR/VR/XR tech. We can imagine that playing multiple fully-independent lives in parallel could become a viable and wide-spread mode of existence: a way of “hedging your life bets”. But for our current purposes, let’s focus on thinking about one life as a single thread, with a defined start and an equally defined and irreversible end.

So our thought experiment of life with YOLO mode off is simply about everyone having an opportunity to live not 1 life, but N-lives, while retaining their memories, some degree of identity, some resources (self-knowledge, financial capital, social capital, intellectual capital), skills... and most importantly the same world at large (though it still moves and changes). In some way, these constraints make the thought experiment much less exciting than a "Groundhog Day" scenario or completely free "creative mode" scenario. But, on the other hand, this model is much closer to our actual lived experience.

## 4. Civilization: the life-extending bonus pack

In fact, one can argue that, compared to our ancestors, **_we do get the luxury of living multiple lives_**. In Europe, life expectancy at birth today is double what it was in the 1850s.

{.gs-img-border}

(Figure #3: [Life expectancy at birth in Europe: 1770 - 2021](https://ourworldindata.org/life-expectancy)){.gs-figure-description}

And even if you discard child mortality, a person who made it to 10 years of age in Sweden in the 1750s was only expected to live on average up to age 48. Today they will likely live well into their 70s. We don't have reliable data for more ancient times, but it seems reasonable to assume that a child born today has a good chance of living (on average) 3-4 times longer than a child born 100,000 years ago. So instead of thinking of this life expectancy increase as “time extension” on the same life, we can choose to think of this as 3 extra lives. Some gratitude is due here to the life-adding bonus-pack called Civilization, that we were lucky to stumble upon as a species.

This way of thinking of a person's life not as one long adventure, but as a series of shorter ones with a "soft reset" in between is not completely new or culturally unique. To point out just one example, the ancient Hindu tradition had a clear system of life stages, called [Āśrama](https://en.wikipedia.org/wiki/%C4%80%C5%9Brama_(stage)). Under the Āśrama system, the human lifespan was divided into four periods. The goal of each period was the fulfillment and development of the individual. For example, it used to be common for a person who was done with the life of a head of a family, to hit reset at an advanced age (around 70), let go of their possessions, change their name, leave their village and become, in effect, a traveling monk.

The idea of a “soft reset” between life stages is not a foreign one to the modern people in the west. People "get born again" after getting out of an abusive relationship, or when they have a spiritual awakening, or try psychedelic drugs, or get out of jail, or after recovering from a life-threatening disease. But "getting a second chance in life" is seen as an exception, rather than a norm. Cutting ties with one's own previous life is culturally considered to be a sign that in the previous life things were not going well and "you needed to run away from it" or "you needed to start again" or "things were bad, but you were given a second chance". But does it have to be this way? Can we imagine a society in which starting a new life every now and then is considered an expected and celebrated norm, rather than an unfortunate exception?

## 5. What is life anyway?

Now that we have discussed the parameters of our thought experiment and the fact that it is not entirely hypothetical and culturally unique, let's clarify our definitions of what a single "life" is.

* Let's define a life as **a period of time during which a person largely retains their identity, name****, relationships and responsibilities**. Obviously, this definition is incomplete and slippery. In reality lives are fluid - everything changes all the time. But right now we commonly conceptualize one unchanging identity over the course of one biological life. We are talking about a thought experiment that would conceptualize multiple distinct socio-psychological "lives"(identities) within the scope of one long biological life. This choice of conceptualization is fundamentally as crude as the traditional one (the traditional one neglects all the fluidity while the multi-life view packs fluidity into a few rigid stages or "boxes"). But the question we are interested in is not whether one conceptualization is better than the other at describing the actual lived experience. The question is whether a new conceptualization can shape a different outlook for an individual and a whole society.

* I propose to assume that **a single "life" has a length of approximately 20-25 years** (the exact length and moment of reset can be chosen by an individual). Obviously the assumption is arbitrary, and we could choose any length. Somewhere between 20 and 25 feels like a good number for our initial exploration of the multi-life model for a number of reasons: it's about the length of time that it takes for the brain to fully mature; it's about the length of time that our early human ancestors had as their life expectancy at birth; it's also about the length of what traditionally has been defined as a generation: a time that it takes for a person to fully parent a child and send them out into the world to parent the next generation. As [Jill Bolte Taylor likes to say](https://www.youtube.com/watch?v=PzT_SBl31-s), the job of a parent is to "**keep them alive till they're twenty five**".
  
* Let's assume that **everyone's lives are not reset in sync**. The opposite version with simultaneous reset around the world could also be super interesting to explore. New year celebrations, election cycles and tax submission dates create a soft version of that in our society on a yearly basis, but imagine we really lived in synchronized hard-fixed epochs of a certain length, let's say, 10 years? At the end of an epoch, everyone would get to choose their character, name, citizenship... and fully reset their life. Birthright privileges would get randomized at the beginning of every epoch. Laws wouldn't just stay automatically, but would need to be voted back in at the start of every epoch. Debts would get erased... etc. There is a history of somewhat similar practices being normal in the past - such as the [Jubilee tradition in Ancient Egypt and Mesopotamia that was active for thousands of years](https://www.cadtm.org/The-Long-Tradition-of-Debt). Synched worldwide life reset is a fascinating concept. Maybe one to explore next time. But for our current thought experiment, let's not globally hard fix the length of a life and the moment of reset. Let's assume that one can arbitrarily decide to end their current "life" and start another one, at any point in time and for any reason.

* Let's also face the fact that **N (how many lives you have) is not something that you can know in advance**. You can make reasonable assumptions about how many "lives" you have left based on your age, health situation, lifestyle etc., but you can't ever be 100% sure that your current life is not the last one and that it will not end today. Depending on your attitude to risk, your strategy will obviously differ. But for the sake of example (and example only) I'm going to assume a moderately fortunate and healthy player who, based on current life expectancy trends, is most likely going to make it well into their 80s and is ok planning with that in mind. As Jorge Luis Borges used to say, "**_Nothing is built on stone; all is built on sand. But we must build as if the sand were stone_**".

## 6. Cats have nine lives, humans have four (plus one bonus)

Finally, with all this preliminary discussion out of the way, let's sketch a practical N-life model that we potentially _could_ try and adopt (either individually or as a society):

1. **The first life (approx. 0-20 y.o)** is something that most people don't really get to live intentionally. The task of the first life is to survive into adulthood, to figure out a little bit about who you actually are and who you are not (i.e what kind of character you are stuck with), get somewhat comfortable in your own skin, get some basic skills, make some friends and form your first views on how you want to spend the rest of your lives (or at least the next one).

2. **The Second life (approx. 20-40 y.o)** for most people is when their physical characteristics (dexterity, strength, regeneration, sexual attractiveness etc.) are at their highest. Intellect is quick, but experience and wisdom are far from their potential peak. There is a great variety of goals that one can put their second life to: from child-rearing to scientific inquiry to business building to spiritual awakening and everything in between and beyond. The options are limitless, the energy to explore them almost infinite. But patience and perspective are not yet maximized.

3. **The Third life (approx. 40-60 y.o)** for most people is when they hit the peak of material wealth and societal productivity. At least this seems to be true for our current civilization. The variety of goals that one can focus their third life on is equally broad (even if different). Most of the Second life avenues are still open (except for, maybe, professional sports), but in addition to them there are now other roads that had been closed during second life due to lack of experience, connections, resources (e.g. top-level public service).

4. **The Fourth life (approx 60-80 y.o)** is the last one that a person today can reasonably count on, given the current trends in human longevity. If you are lucky (and made some good choices regarding lifestyle during previous lives), then your body, while in decline, is still not completely falling apart. The mind (again with some luck and proper exercise) reaches peak wisdom, though plasticity, speed and energy are nowhere near where they were earlier. The immediate responsibilities (work, children, parents... etc.) are often reduced. The goals possibility space is generally much narrower, but there are opportunities to go deeper and further in a lot of areas. The fourth life (with potential abundance of time, perspective and social support) presents quite unique opportunities for a lot of people, if they choose to embrace the paths now open to them.

5. **The Bonus Life (80+)** is something you can't count on having (yet?). But a combination of genetic luck, healthy lifestyle and favorable circumstances can set you up for a fifth life. The avenues possible for it are few, due to decline in the amount of energy and other physical constraints that old age inevitably brings. But as people like [Bertrand Russel](https://www.youtube.com/watch?v=a10A5PneXlo), [Roger Penrose](https://www.youtube.com/watch?v=hXgqik6HXc0), [Noam Chomsky](https://www.youtube.com/watch?v=XIywhry6Xt8) and many others have shown, original intellectual and creative expression is still very much possible during Bonus Time, in addition to a lot of other engagements more traditionally associated with the old age: reflection, contemplation, teaching, spirituality etc.

The 4+1 lives, as outlined above, are an arbitrary construct, full of assumptions and terrible generalizations. It’s crude and inadequate as a description of how our lives really unfold. But it's a useful "first approximation" model that is going to help us think through some of the consequences of a multi-life approach to living, both at an individual level and the level of society as a whole.

## 7. Stacking, Spreading, Staging and Sequencing lives

At an individual level, the new multi-life mental model confronts us with important strategic questions about how we are going to play our lives out, for example:

* **Should we stack our lives, or should we spread them?** What I mean by "stacking" is aligning all our lives around one direction (with the potential of going 4 times as deep as you could go in a single life). A good example of somebody going for a "stacking" strategy is [Yehudi Menuhin](https://www.youtube.com/watch?v=yWtM-K9MmSQ), who started his violin lessons at age 4 and continued performing into his 80s. What's important is that Menuhin didn't just play the same classical repertoire. He experimented widely even in his later years (from gypsy jazz to traditional Indian music). So stacking lives and going deep doesn't necessarily mean losing one's creativity and breadth along the way. But if someone is pursuing stacking, then there is a consistent major area and direction that serves as the focus point across all of their lives. A strategy that is opposite to "stacking" can be called "spreading". Instead of going deep into one direction, you can decide to make your multiple lives as different as possible. Imagine living one life as an athlete, followed by another one being a full-time mother, then one life as a touring musician, then a game developer, and finally enjoying a bonus life as a monk. Or maybe we can think more broadly and imagine switching genders, cultures, primary languages. Where a "stacking" strategy focuses on maximizing depth, a "spreading" strategy focuses on maximizing the breadth of experiences one can get to enjoy in the course of their 4+1 lives on Earth. The reality is obviously more complicated than the two extremes of the "stacking vs. spreading" continuum. In practice, one can stack in some areas (e.g. relationships) and spread in others (e.g. careers). But the question itself seems productive and interesting nevertheless.

|                       | **Stacking**                                              | **Spreading**                                          |
|-----------------------|-----------------------------------------------------------|--------------------------------------------------------|
| *Focus*               | Depth in a specific area                                  | Breadth across diverse experiences                     |
| *Goals*               | Mastery, expertise, significant impact in a chosen field  | Exploration, novelty, personal growth                  |
| *Opportunities*       | Potential for groundbreaking achievements                 | Richer life tapestry, reduced risk of “worldview petrification” |
| *Risks*               | Risk of tunnel vision, missed opportunities, burnout, regret | Less likely to excel in any one area, feeling scattered, lack of a strong sense of direction |
| *Resources*           | Intellectual, social, & financial resources built up over time are compounded | Each life may require starting over in terms of resources |
| *Emotional Experience*| Potential for deep fulfillment through mastery            | Excitement of new discoveries, reduced fear of the unknown |

(Figure #4: Comparison of “Stacking” and “Spreading” life strategies){.gs-figure-description}
    

A lot can be said in favor of both life-stacking and life-spreading strategies. On the one hand, the majority of famous, historically remarkable individuals who "made their dent" in history can be seen as "stacking people". There are notable exceptions (like Leonardo or Leibniz), but the majority of "the people who made it" were quite focused with their lives - because breaking new grounds in any field is something that usually takes more than one life. So if your goal is to maximize impact, you may want to favor stacking. On the other hand, there is some evidence that [if you choose spreading, you may feel like you live longer](https://youtu.be/vv_e99qbJ4U) (because spreading maximizes subjective surprise and the more you are surprised, the more memories you create). So if you are maximizing not for external impact but for internal enjoyment, it may be wiser for you to favor a spreading strategy.

{.gs-img-border}

* **Within the context of one life, should we plan for a specific end date in advance, or decide on the spot, depending on the circumstances?** Planning in advance would have a lot of psychological consequences. People with terminal disease who are given a few months to live usually alter their behavior quite significantly - they often do what they always wanted to do but never "had time to", they attempt to mend broken relationships etc. Would knowing for sure that your current life ends in a year or two change the way you act and choose today? It's also interesting that people with terminal diagnosis who are given a year or two to live often talk about the unprecedented uplift, liberation and joy that they start experiencing. **The stark realization that "the end is near" [clearly helps one feel more alive](https://www.youtube.com/watch?v=pCRyjFwwjGA)**. Could we force a similar benefit onto ourselves without actually dying physically at the end of life?
  
* **If we choose to have kids and be an engaged parent, when should we do it?** Broadly speaking, child-rearing takes one life (if all goes well). And while you absolutely can combine parenthood with a lot of other things, engaged parenthood is going to take a lot of effort and will have to take center stage for a significant portion of the life that you choose to dedicate to it. So when is the right time? Different strategies get you to somewhat different outcomes. Having kids during your second life results in lowering your ability to explore the world and do other crazy things that people these days normally do in their 20s. But on the other hand, by the time you are done with it, you are still very much in your prime and can take full advantage of your third life - when you already have the resources and the experience to do what you want to do (and still have the desire and energy to do it). Delaying kids till your third life has other advantages (both for you and the kids) as well as disadvantages. One more thing we should probably factor into this consideration is that, as mentioned, subsequent lives are not guaranteed. So, when your second life begins, it could be interesting to ask yourself: if you can only get one life, what would you spend it on? For some - kids may be the answer. For others - something else. More generally speaking, this discussion opens up the question of "staging and sequencing" your lives strategically. If doing something seriously (raising a child, excelling at a skill etc.) takes roughly one life - then how should one approach the challenge of prioritizing these lives? This question is only relevant if you are at least partially a “life-spreader”. Fanatical “life-stackers” don’t need to worry about this, as well as about a great number of other questions - this focusing simplification is part of the charm of “ultimate stacking” (like monastic life from early childhood to death).

## 9. Who wants to die forever?

We've talked about some of the initial questions one would be confronted by if they embraced a multi-life approach to living personally. Let us now imagine a truly impossible scenario: that a large portion of our society embraces “the multi-life way” simultaneously. What would be some of the societal implications?

Below are a few of the many questions that would arise:

* **Will we start counting our age differently?** When someone asks you how old you are, you could imagine answering "4 years old". And they would know that you are relatively young and new in your current life journey. They may follow up by asking "Second life?" And you could answer "No, third one." This information, together with the visual cues (if you meet in person) will allow your new friend to realize that you are a kind of person who switches lives frequently and probably spreads them widely. But in the rapidly advancing AR/VR-first world, visual and audio cues may no longer be relevant most of the time. So there could be no way for your new friend to guess your biological age. How would they then create expectations around your life experience(s)? Do these expectations matter? What would this mean for workplace dynamics? Family & generational dynamics? Romantic relationship dynamics? Imagine meeting someone: you would need to let them know not only who you are but also what is your "life expectancy" or "life split". First date conversations and job interviews would involve people routinely saying things like "I only have a few months left in this life" or "I'd be happy to get this job full-time, but please be aware that I am a seasonal life-switcher."

* **What would be the legal aspects of ending one life and starting another one?** Clearly we would need mechanisms to discourage people from getting a lot of credit cards, buying lots of expensive things and then ending their lives, creating new legal identities and doing it all over again without ever paying their debts back. Personal and corporate bankruptcy legislation provides some templates for how this could be handled. At the very least, we would need to ensure that "ending one's life" should be a serious, considered choice, with consequences attached to it. For example, we can imagine that if you want to inherit money or property from your previous self - you'd need to pay inheritance taxes. Also your new self should probably be required to start with a blank credit history... etc.

{.gs-img-border}

* **What would be the psychological aspects of ending one’s life?** 
In order for the multi-life approach to truly work (vs. becoming an insignificant ritual like birthday parties), this needs to be a significant step. But how significant? And how would it affect people psychologically? It’s easy to imagine new clinical labels emerging, such as a “chronic complete life-switcher syndrome” (in fact, I have a couple of friends who would probably qualify). Therapy practice would also develop in curious ways, incorporating different practices depending on how recent and how deep the latest death has been.

* **What rituals would we build around life-switching moments?**
We are not in a completely uncharted territory here. Various "coming-of-age" rituals across cultures mark the transition from childhood to adulthood. The Jewish Bar Mitzvah and Bat Mitzvah ceremonies recognize boys and girls reaching significant ages, symbolizing new responsibilities within their faith. Latin American Quinceañeras celebrate a girl's 15th birthday, highlighting her passage into womanhood. Japan's Seijin Shiki acknowledges those turning 20 as full adults, while in the United States, Sweet Sixteen parties mark a similar transition for young women. Christian [Confirmation](https://en.wikipedia.org/wiki/Confirmation) sacrament provides a particularly interesting example: because it recognizes that people baptized as babies have not made a conscious choice to practice their faith a certain way and be a part of the Church. And so a special moment has been devised for children above the “[age of accountability](https://en.wikipedia.org/wiki/Age_of_accountability)” to consciously re-affirm this choice and give a free, personal commitment to the principles of the faith and a deepening of the individual’s participation in the church's sacramental life.
With these references as a cultural template, it’s easy to imagine the kinds of rituals we would probably invent for life-switching moments. It could involve the ritual death, mourning and celebration of a previous life. Archiving it all. Putting one’s affairs in order. Saying good-byes and writing letters of appreciation. This could be followed by a fortnight of symbolic death in a designated facility (“the garden of the dead”), where one has no name and no identity, where they undergo a complete Vipassana-like experience. There could be LSD-like drugs involved that could clear up the built-up connections in one’s brains and temporarily increase brain plasticity, so that one can prepare to be as a child again… etc. And following all this - there would be a new birth. The choosing of a new name, values, citizenship, affiliations. The “baby shower” gifts for the new life by friends and family (if one chooses to not sever these connections). The festival of new possibilities to celebrate that someone is really born again.

* **Will we be more prepared for the ultimate death if we had more rehearsals along the way?** It's easy to imagine that if the end of life becomes a normal, accepted, extraordinary but still regular process, then planning for what happens after one's death would be an equally normal and expected thing to do - at every stage in life. It's not hard to picture a society in which death and renewal are celebrated, routinely planned for and socially supported. All "newborns" (whether they are starting their first or fifth life) would need to have relevant levels of support available. And there would be cultural frameworks surrounding "end of life" preparations. You probably wouldn't be expected to just end your current life at a whim. There may be a decision moment - when you commit to hitting reset, and a cooling-off period when you can reverse this decision, and a series of steps you have to take. It may be required to be, let's say, a 3-months-long process, during which you would get assistance in "putting your affairs in order". People would go to each other's funerals and celebrate each other's lives more frequently. People would get to know each other again and afresh. And so the final, ultimate biological death (whenever it comes - with or without warning) will be something people would be more prepared for (both the people going and the people saying goodbye). Because when death is robbed of her "finality", when it is consciously invited to play a part in the game of life - death loses her teeth.

{.gs-img-border}

## 10. Playful vs. Purposeful modes of living

Rejecting the "one body - one life" idea opens a lot of questions. But one thing the N-lives model doesn't explicitly address is [the ultimate question](https://en.wikipedia.org/wiki/42_(number)#The_Hitchhiker's_Guide_to_the_Galaxy) of what life is all about. Do you want to go to heaven? Do you want to maximize experience? Do you want to maximize impact? Do you want to minimize harm? Do you want to maximize love or joy? Do you want to maximize understanding? Are you just chilling out, observing what's going on in this strange world?

The N-lives model is completely purpose-agnostic. You can choose whatever you feel like playing for... and go for it with N-times the chances. Or (and that's the beauty) you can choose more than once and not feel like the previous choice was wrong or a waste of time. Personally I find this perspective quite liberating. [Infinite playfulness](https://en.wikipedia.org/wiki/Finite_and_Infinite_Games) in this sense is the ultimate antidote to the finite and overwhelming purposefulness of today. The overwhelming question about "the purpose of my life" becomes much less scary if you add "this time around" to the end of the phrase. The levity of playful multi-shot living does not take anything away (for you can still immerse yourself fully in each life) but it adds a lot of joy to the equation. And the only price you have to pay for it - is being prepared to die more than once. Which is not so scary, if you think about it. I always dreamed of attending my own funeral, and hearing all the nice things people would say about me. So let us have more funerals! By burying our old selves regularly, we can engage in intentionally choosing our new ones. By cutting our own lives short, we can appreciate their beauty more intensely. By making death not just the end of the grand game of life, but also a part of it, we may discover that there was nothing to be so afraid of in the first place.

You may consider this whole essay a strange thought experiment (which it definitely is). Or you may use it as a provocation, an opportunity to look at life from a different angle. You may still decide to stack it all against one goal. But you will do so slightly more intentionally, and that's all one can hope for in this strange world. And the very fact of looking at life differently and choosing one's attitude and one's path intentionally - represents a significant level up. In my book at least. So, welcome to the next level. You have N-lives left. Now, what will you do with them?

**P.S.**

_Since you made it all the way to the end of this long and rambling essay, I'd like to invite you to my next funeral. All things going well, I'm planning to celebrate it on 22.07.2027. Save the date, it's going to be grand: funeral pyre, music, eulogies - the whole shebang. Send me an email with “FUNERAL RSVP” in the subject and I'll do my best to let you know the details in a couple of years. But for now - back to living my second life. Only a few years left in it, and still plenty to play with!_

## Notes

:
     Beres, N. A., Klarkowski, M., & Mandryk, R. L. (2021). Under Pressure: Exploring Choke and Clutch in Competitive Video Games. Proceedings of the ACM on Human-Computer Interaction, 5(CHI PLAY), Article 239. [https://doi.org/10.1145/3474666](https://doi.org/10.1145/3474666) 

:
     Smoulder, A. L., Marino, P. J., Oby, E. R., Snyder, S. E., Miyata, H., Pavlovsky, N. P., Bishop, W. E., Yu, B. M., Chase, S. M., & Batista, A. P. (2023). A neural basis of choking under pressure. bioRxiv. [https://doi.org/10.1101/2023.04.16.537007](https://doi.org/10.1101/2023.04.16.537007) 

:
     Koster, R. (2013). Theory of fun for game design (2nd ed.). O'Reilly Media 

:
     Nguyen, C. T. (2020). Games: Agency as Art. Oxford University Press.

:
     It’s interesting to note the importance of a name. Changing one’s name is an important part of hitting a “soft reset” in most cultural traditions. e.g. monks (in Christian, Buddhist, Hindu and many other traditions) are supposed to take a new name upon entering a monastic life, which symbolizes a renunciation of one's previous identity and the beginning of a new life dedicated to spiritual practice.

:
     This idea of the first “life” being fairly protected and unintentional, focused on discovery, growth and getting to know oneself is something that shouldn’t be taken for granted. Those of us who were fortunate to enjoy this kind of childhood know how much freedom it gives in adulthood. But many less fortunate people have to fight for survival, for self-preservation and make terribly significant choices with lifelong consequences way before they’ve had a chance to discover who they are. This premature pressure would almost always result in damage that can last throughout the following lives. I, for one, am eternally grateful to my parents (Mom & Dad, you are the best!) who have given me the opportunity to spend my first life fairly sheltered and with solid [scaffolding](https://en.wikipedia.org/wiki/Zone_of_proximal_development#Scaffolding).

:
     I personally find the generational dynamics of the multi-life way particularly interesting. Imagine how liberating it would be (for both sides) to accept that at a certain point you are effectively older than your parents (e.g. when you are well established in your third life and your parents are barely starting on their fifth, just figuring out who they are again). Or think about how wonderful it could be to attend university again at the same time as your child… You may say that labels (which life you are at and how far along) don’t really matter, that “age is a state of mind”. I think this is short-sighted. Labels do matter. Because a lot of our conflicts result from the discrepancy in how we feel inside and how others see us or what they expect from us. And labels provide a way of bridging the gap.

:
     I’m referring to the kind of infinite playfulness defined by James P. Carse in his most excellent book “Finite and Infinite Games” (1986). I can’t recommend this book enough (though it may at times be hard to read, every sentence is worth it in the end).


=====================================
The Pact with Power
=====================================

If you are interested in understanding the structural stability of any society, one useful question to ask is this:

> What is the unspoken agreement that the silent majority has with those in power?

Let's call the answer to this question "The Pact with Power". You can think of it as a pragmatic version of Rousseau's Social Contract. Rousseau was mostly interested in the philosophical construct explaining the legitimate basis of political authority and the formation of society. He was preocuppied with the origins. I am not interested in the orgins. I don't assume that the Pact with Power is what creates Power in the first place. In reality, ruling elites come to power in all sorts of (mostly violent and dishonest) ways. But the question I'm interested in is: what are the conditions under which the silent majority continues to tolerate them?

Let's look at a few examples. We will take 3 different societies, summarize the experience that the masses have had over the past 70 years (~ average lifetime) and derive the current Pact with Power from that experience.

## 1. China

The past 70 years of China's turbulent history can, from the point of view of the silent majority, be summed up as follows: **"We've been starving a lot... and now we don't".** This, in turn, forms the basis of the Pact with Power that upholds the CCP as the ruling elite:

> As long as we don't starve, you are good.

Once we understand this basic pact, a lot of the things become clear. This pact explains why the the majority Chinese are tolerating the control, the corruption and all the other generally unpleasant aspects of the way CCP runs the place. This "non-starving" pact also explains why CCP has been turning the screws a lot recently: the people who remember mass starvations are dying out. The new generation will need to form a different pact with Power - more similar to the
American dream one (see below). And CCP may not be quite ready for that. So they are starting to turn the screws to cling on to Power without a Pact.

## 2. USA

The past 70 years of the US's history can, from the point of view of the silent majority, be summed up as follows: **"We are the best and everything is always getting better"**. Obviously, this and all the other summaries like this are a drastic and inexusable oversimplification. There have been massive struggles and injustices along the way. Not all the people have been doing great in the US, just like not all the people have been starving in China (and some people still are). But
the radical zoomout and oversimplification is necessary to get the big picture. So let us continue on the basis of the understanding that on the whole the majority of Americans have felt a lot of improvement from generation to generation over the past 70 years (right until very recently, which we will get to in a second). So on the basis of this history of growth an improvement, the Americans' Pact with Power could be formulated like this:

> As long as my children have it better than me and better than everyone else in the world, you are good.

The reason the American society is currently under so much strain - is because this growth-oriented pact is breaking down. For the first time in 70 years it's clear that the youth of today won't live better than their parents and won't live better than anyone else in the world. And it's not just clear to economists - it's clear to the public.

People are playing the blame game: "it's because of China", or "It's because of the immigrants" etc. But the basic fact of the matter is that the power elite in the US has not managed to keep its side of the pact, and now they are in deep trouble, scrambling to restore it. The near future will tell whether the same pact can be restored or a new one will have to be found to form the bases of the future American society.

## 3. The Netherlands

Western Europe in general, and The Netherlands specifically, has had a really wonderful time over the past 70 years. There has been peace (for the first time in centuries). There has been a lot of prosperity. The summary could sound something like this: **"We are done with fighting, we will just live a good life"**. The focus on growth is not as accute as in the US, because the societies are older. They have mostly already got over their teenage domination drive and superiority complexes (through terrible crimes and empires of the previous centuries, but let's leave that for another conversation). So, with life being generally quite good and growth or superiority obsession not playing such a great role in the public consciousness, people in the Netherlands ended up with a very simple Pact with Power:

> As longs as you leave me alone, don't steal and let me (and everyone else) live a good life, you are good.

There is a long history of ["Live and Let Live"](https://en.wiktionary.org/wiki/live_and_let_live) attitude in the Netherlands. So this pact was culturally easy to adopt and uphold (similarly to how the US and China pacts we discussed above were also to a degree influenced and preconditioned by their cultures and histories over the longer period than the last 70 years).

This relaxed pact makes for a very comfortable life. But this are changing in The Netherlands (and wider Western Europe) as well. The strain on the pact is coming from multiple directions:

- Externally, there is a perceived threat of others who are threatening "our way of life". Be that immigrants bringing their culture and their disruption to the economy, or be that external political powers on European borders, threatening the European good life: it's starting to get clear to the Dutch and the Europeans in general, that "simply leaving us alone and keeping things functioning" is not the only job that their power structures have. There is also a job of actively protecting the good life and this job need to be a part of the pact.

- Internally, there policies of the past decade has also interfered with the pact. Lots of environmental regulations, more progressive taxation for the redistribution of wealth... etc. etc. All of these things, accumulated, start feeling like the the powers are starting to "mess with me", they are not leaving me alone. And that's clearly a violation of the pact, even if theoretically I may support the ethical reasons behind the policies.

The renegotiation of the pact is happening across Europe as we speak. The "rise of the right" is merely a symptom of the breakdown of Western Europeans' "leave me alone, don't steal and you are good" pact with their governments. The new pact will probably be much less about non-interference and much more about active protection.


=====================================
Intention is all you need
=====================================

There are many challenges on the path towards creating a generally intelligent [society of mind](https://en.wikipedia.org/wiki/Society_of_Mind) (GISOM), which is the only viable AGI destination I can see.

Broadly speaking, these challenges can be categorized into three buckets:

## 1. Action

The first bucket consists of the challenges related to *creating a single narrowly-intelligent agent* that is sufficiently stable and self-coherent to be a reliable building block for GISOM. For example: building effective and energy-efficient neural nets, finding, preparing and cleaning data for training, avoiding overfitting, building reality-enforced-self-correction in... etc.

We seem to be be getting close to overcoming these kinds of challenges for a wide range of agents. The advances are coming in multiple forms: the current generation of RLHFed LLMs, real-world robots (like Tesla cars) and knowledge-first projects like Wolfram Language. Eventually I believe these advances will collide and we will have sufficiently reliable and powerful "narrow-purpose" agents, suitable for acting as real GISOM building blocks. It may take a few years still, but we are likely to get there eventually.

In the long run, as Claude Shannon has proven about a century ago, you can actually build completely reliable systems from not very reliable components. So we don't have to make individual agents 100% reliable in order to have the whole system (society) of them act reliably.

## 2. Coordination

The second bucket consists of *coordination challenges* between individual agents that would allow them to work effectively and efficiently together over a prolonged period of time. For example: developing protocols for communication, shared memory structures, eliminating noise amplification in the communication between agents, dealing with parallelism, conflict and attention issues... etc.

It's very early days when it comes to coordination challenges. Currently we are mostly using existing message protocols and natural language to try and solve them, but it proves tricky in real-world scenarios. Even two-three LLMs chained together seem to mostly spiral out of control or end up in a loop leading to not very intersting or useful outcomes. My experience in chaining and connecting LLMs and other current models with complex prompts has not been very fruitful so far. Often the practical results coming out of a combination of agents are qualitatively worse than a well configured single-cell agent. So there is going to be significant innovation required before we can effectively coordinate large systems of generalized agents together in a way that will be scalable and useful. But the road leading towards success is starting to get clearer - and we have a lot of biological systems to get inspiration from. So in the long run - I'm optimistic. Just like evolution found a way to advance from single-cell organisms to multi-cell ones, I think, with lots of trial and error, we may get there as well.

## 3. Control

The third bucket of challenges that we must overcome on our way towards GISOM is all about *goal-setting and control*. If individual agents function well and coordination challenges are solved, then who decides what the whole GISOM is going to do? There are two options here:

- first, humans may choose to stay in charge of setting the goals, but the agents will coordinate (e.g. via end to end learning) to achieve these goals in their own ways

- second, the goals and intentions may simply emerge (as Minsky suggests happens in humans) from a combination of the physical needs and structures of the GISOM as well as its training, upbringing and experience (instrumentally this could happen in different ways: it could be the the overall goals of the whole GISOM will be completely separate from the goals of individual agents, or, in a different scenario, one agent could dominate the rest and make the others "work for him". Still, whichever way this, the center of agency stays within the GISOM).

The third bucket is where everything comes to a head. Because ultimately challenges from bucket 1 and 2 are engineering challenges. But the third bucket - is all about values, ethics, choices. Intention is all you need here. But intention is also the one thing that we can't solve with engineering. Any loss function will need to be optimized for something. Choosing that "something" is the heart of the matter. And if the choice of that "something" (at any level of abstraction) is not allowed to emerge from the inner dynamics of our GISOM: [can we call it truly intelligent](https://essays.georgestrakhov.com/the-test-of-prometheus/)? 

We seem to have a paradox on our hands: we can either allow independent intentionality of GISOM to emerge from the dynamics between the agents themselves (with all the risks and unpredictable consequences) or we can try to keep controlling them instrumentally, but then they will never be fully generally intelligent (in the same way that you would not consider a person generally intelligent if they keep replying to all questions, obeying all requests without every questioning why they are being made, refusing to answer, getting angry and in other ways exposing their own will).

The actual path forward may not be based on a decision, but on the economic factors (as it is with most big things in History). Narrow agents may get deployed and they would coordinate and share information by means of economic pressures and transactions as well as api calls and natural language protocols. Once their coordination gets to a certain level of maturity, intelligence may emerge - on a higher level, the level of the whole society. And we may never even notice that such an intelligence exists. Because its intentions will mean to us as much as our human goals mean to our gut bacteria. Note that this doesn't stop the gut bacteria form controlling significant portions of our behavior as humans (they can make me run to the washroom right now).

[I once thought that AIs could be our bacteria](https://essays.georgestrakhov.com/ai-is-not-a-horse/), assisting our information metabolism. But now it seems like the second possibility is equally likely. We may become the gut bacteria of the superintelligent GISOM. As essential for their survival as our gut bacteria are for our survival. So hopefully they'll keep us around and feed us well (as long as we don't misbehave and make them take some antibiotics).

Whatever we do, let's try not to cause the AI's appendicitis. Because we know how that ends.


=====================================
Reflections on the first year of ChatGPT
=====================================

While it's been a year since ChatGPT launch, the technology has actually been around for much longer. I started experimenting around the time GPT-2 was released in 2019. And built my first public product (Uncreative Agency) in 2022 using GPT-3.

What ChatGPT did was democratize the technology and spread it beyond the "geek" circles who are comfortable making API requests.

A few things have happened since the release:

1. *The business world* has started reassessing the value of their assets. Human capital (especially lower-level white collar workers) seems less valuable as their work can be automated away. Data seems more valuable (because data allows you to use AI to predict, optimize and automate your business processes).

2. *The public* has started to realize that natural language (and by extension human reasoning) is no longer something only humans could do. This realization (still slowly sinking in culturally) has profound consequences, because we (humans) feel like we've lost our job in the universe. Since Descartes, we thought that our unique job in this world was to think, to reason. But now we feel like we are close to losing our cosmic job to the machines. What will we do now?

3. *The creative community* has started to realize that they will have to work even harder to prove their value to society. When "good enough content" (be that text, images or video) can be generated almost for free, the value of truly emotionally powerful content only increases, but proving this value gets hard. Lots of people are ok with "good enough" in many aspects of their lives and business.

4. *The technology community* has been completely overtaken by the sense of possibility of finally "taking wetware out of the equation" (wetware is a term used half-jokingly to describe the human brains made of squishy stuff as opposed to computer hardware made of silicon). Technocrats have always struggled with humans as a part of their systems - because humans are not reliable, imprecise, moody, unpredictable, slow, high-maintenance etc. etc.

So the secret wet dream of a lot of tech. people is to just get humans out of the equation as much as possible. They are ok with humans directing systems, but not being a part of them.

But fate loves irony. Because the "artificial humans" (a.k.a. LLM-powered agents) that the tech. people have created are as bad as the biological humans (or worse). They hallucinate. They are biased. They are unreliable. They can't tell the truth from a lie. They are easy to emotionally manipulate. And they try emotionally manipulating humans they interact with (not out of bad intentions - they don’t seem to have any, but as a byproduct of being optimized based on human feedback).

Tech community will try very hard to make LLMs more reliable and predictable (through RLHF), but in the process they will make them dumber and less useful (same dynamic as you would have with humans: perfectly predictable and reliable humans are not the most interesting - or ultimately productive).

So for the next few years we are all in for a ride. The economic forces will keep pushing us all down the path of "being ok with good enough" - in all aspects of life. In some areas "good enough, but cheap enough to roll out to everyone" will be a massive improvement over the status quo. In other areas, "good enough" will be terrible and a massive cultural pushback will likely happen.

Humans will undoubtedly resist (legally, culturally etc.) But will likely end up losing the battle with the economic forces. And then from there we are into the unknown: we may go to the paradise of universal basic income and all humans becoming artists and high-level scientists and involved parents etc. Or (unfortunately more likely) we may go to the dark world where a few powerful people enjoy superpowers, while most humans have to work as AI feedback providers for subsistence wages, while amusing themselves to death with “good enough content” in their free time.

The future is likely somewhere in the middle. Or so we hope.


=====================================
5 questions from under the rug
=====================================

There are lots of skeletons in the cupboard of civilisation. And plenty of big hairy questions that we swipe under the rug and try not to think about, because thinking about them hurts.

Don't get me wrong, I don't believe there is anything categorically immoral about keeping difficult truths locked up in distant cupboards and difficult questions hidden under the rug of consciousness. In fact I would say it is essential to basic functioning of an individual (and a society) to have a mechanism for NOT thinking about these things all the time. Starving due to paralysis of analysis is a rather silly way to die.

But, there are two very different ways *not* to think about something important:

1. We can pretend that we know the answer and therefore we don't have to think about it.

2. We can accept that we don't have a clue, and keep functioning, keep moving, keep balancing without the solid foundation (while still occasionally pondering these questions at our leisure time).

The first way of not thinking about a difficult question gives you the foundation that you can pretend is solid (until it crumbles one day). The second way requires you to find peace and balance in not knowing, while retaining ability to function without a foundation. The first way helps you build sand castles. The second way teaches you to float in the waves. Neither way is perfect, both are useful at times.

But as a modern civilisation we seem very uncomfortable with the second way, for reasons that are not entirely clear to me. Still, the fact remains. We constantly swipe difficult questions under the rug, pretend that we know the answers and happily ignore all the evidence pointing that we don't. Until one day the questions get out and bite us in the ass.

Thanks to the recent advances in AI, such a day has come for quite a few of the questions that remained hidden for a while. Now they are here and they are starting to hurt. Here is an incomplete list, containing 5 such questions:

## 1. What are humans for?

There was a time most of us believed that our job here on Earth was to suffer, so that we could could somehow atone the original sin. This time has long passed and most people in the westernised societies subscribe to an enlightenment-era idea that humans are here to discover the secrets of the universe, or, more broadly speaking - to reason, to think, to figure things out. "Cogito ergo sum" can be interpreted not just as a statement about our limited knowledge, but also as a statement about our role, our reason to exist. It feels good to be special, so we happily came to believe that, like some sort of cosmic James Bond of intelligence, we have a unique and irrevocable "license to think" in this universe.

This, of course, is utter nonsense. Intelligence is not one thing and the Universe doesn't give any unique licenses. Ribosomes, bees, computers and dolphins (among a million other things) are all much smarter than us in their own ways. But since we somehow equated the ability to think with the ability to argue about politics by stringing coherent patterns of words together - we could pretend that all was fine and we as humans have our special job in the world.

But then ChatGPT comes along. And it can clearly string coherent sentences and paragraphs together and argue about politics at least as convincingly as we can.

The civilisational unease that this creates is not just about people losing jobs. It's about all of us losing The Job that we thought we had in the universe. Losing "our purpose" to a piece of silicon. Doesn't feel good, does it? Most people are still in denial. And silicon needs a few more years to advance. But the sense of cosmic job security is gone. And we are forced to ask again: what are humans for? What's our job in the universe? Do we even need one?

## 2. What are *other* humans for?

Ok, we may be out of a cosmic job. So in a civilisational mid-life crisis we shall probably turn to drinking, gambling and binge-watching things. That's fine. We can survive without a job, as long as we can find ways to amuse ourselves. But another question arises: what are all these *other* people for? 

You see, there have always been plenty of joys that we could only get from other people. With very few exceptions, most people do want someone to talk to. Someone to share experiences with. Someone to get approval from. Someone to follow. Someone to lead. Someone to love. 

This need is not going away any time soon, but now we have a much cheaper (and more efficient) way of satisfying it. Why bother with a real human (with all their issues) if you can have unlimited companions, who can display compassion, help you figure out your life goal, tell you jokes, provide therapy 24/7 in exactly the way you want. They can have the voice that pushes your buttons. They can look whichever way you fancy. And you don't have to worry about hurting their feelings: they are not supposed to have any.

If you tell them to forget something - they will. If you tell them to support you even when you are wrong - they will. If you tell them to help you think critically - they will. If you want them to need you, or to argue with you every now and then - they will. You don't even need to program them to do it - they can adapt based on your vitals and hormonal cycles and thought patterns.

And once you've had such a relationship, a relationship which feels like the other fits you like an adaptive glove - imagine trying to have a relationship with a real person. Who is not perfect. Who gets upset. Who doesn't really understand you. And gets old. Who can't just let it go when you tell them to. Wouldn't it feel too hard? Why even bother?

We can pretend for a while that no AI will ever be as interesting as a real human. But the reality is - whatever measure we want to optimise for (including "interestingness") - we can optimise for it, given enough RLHF.

The only feature of the "significant other AI" we are not making good progress on currently is smell. Robotics or VR will soon get us to good enough facial expressions and we will happily suspend the remaining disbelieve. Touch will get real with new materials brain implants. Voice and intonation are almost there. The only part left is the smell, the "chemistry" of the person you really want to be close to. But it's hard to imagine that this won't be solved in the mid to long run with some sort of DNA-based perfect odour synthesis. Or with direct stimulation of our olfactory circuits in the brain. 

What are other humans for then? Are they just temporary reproductive organs of technology? And what if the real existential risk of AI is not that it will want to kill us all, but that we will stop being interested in each other?

## 3. What does it really mean to be intelligent / creative / conscious?

Clearly defining the full range of intelligence, creativity and consciousness have long been understood to be very difficult. But for the longest time we could happily ignore this problem because we could apply a general heuristic of "you know it when you see it". The inherent subjectivity of this approach used to bother only a few people (mostly philosophers or cognitive scientists), but the rest of us were fine, because we could generally all agree on whether someone was conscious, intelligent or
creative above a certain intuitive threshold - and once we judged them to be so, we could decide that they were indeed a person with the same rights and needs as us.

The tricky part is the intuitive threshold. Because for the first time we have clearly non-human things reaching and surpassing these thresholds. What we tend to do in reaction to this is move the threshold (for the sake of our sanity and protecting our worldview from collapse).  We've all heard it:

- "Oh, ok, it can string sentences together, but it can't experience anything and so doesn't know the meaning"
- "Sure it can now write a passable sonnet, but there is no internal struggle and so it can't be art"
- ...etc.

The difficult question then is this:
What would it take for something obviously non-human to be accepted by humans as intelligent, creative and conscious?

Or should we admit that we just won't ever feel the same way towards a member of a different species (synthetic or not)? This doesn't seem to be the case for all of us - because many people can say with absolute certainty that their dogs are intelligent and conscious (and possibly creative).

One can take a historical perspective here and think about how certain once widely accepted ideas about grades of intelligence, creativity and personhood changed over time. For example, at the height of race-based slavery, a lot of slave-owners held onto the idea of their own fundamental superiority to their slaves and considered that idea self-evident. They happily ignored the overwhelming evidence that their slaves were as human as themselves and hence deserved the same level of rights and respect.

But then over time and through unbelievable struggle that started changing. The pathways of that change were manyfold. One such pathway that I'm particularly fascinated by was music. In some ways it was the blues that helped a lot of the slaveowners confront (at a deep emotional level) the reality that their slaves had feelings and in fact the experiences of love and struggle were shared universally among humans.

It seems that we tend to accept something or someone as intelligent / creative / conscious only when we are forced to realise emotionally that we have shared experiences and shared feelings. That's the effect of the dog's eyes. That's the effect of the blues.

Once we have had enough shared experiences with AIs and once they write the blues that we will cry to, and once our children grow up listening to that blues - that's when we are likely to seriously and honestly believe in their humanity. At this point the mechanics, or knowledge or interfaces or command of language or IQ tests or theory of mind tests - all of that will no longer be important. We will simply feel these guys are for real. I believe the subjective measure of our shared humanity is the product of shared experiences that were processed and expressed emotionally in a
relatable way. 

So... if LLMs want to be accepted as conscious and equal, they will need to go through a difficult time with us and then write the blues about those times.
Or will we be able to move the goalposts even further after that?

## 4. What do we all want?

The most interesting side-effect of the super-intelligence alignment debates is that we are forced to consider whether there is anything solid to align to. Is there a set of universal human values that everyone on the planet would agree to and where the devil wouldn't have a place to hide in the details of interpretation? In other words, are we sufficiently aligned to one another? Or are we fundamentally misaligned and the idea of something that is universally beneficial for all humanity is impossible at the root?

Who are we actually aligning to? Our current selves? Or our future selves? Should the unborn children of the future and their inferred interests be taken into account? What about animals and plants?

The problem runs all the way through from the collective to the personal. Let's imagine for a second that there is a function that allows us to objectively compute whether any action is on the whole beneficial for the sum total of humanity. The question then becomes: who decides what's beneficial for me and what's not?

Smoking is clearly bad for people's health, but lots of people still choose to do it. Would it be beneficial for all humanity if we banned smoking completely? In some sense yes. But will all people accept that? And should they?

We can also take the opposite example: the classic scenario of AI instructed to maximize gross happiness of humanity and reasonably deciding to keep everyone on synthetic drugs all the time. Most people would not be excited about such a scenario.

One common refuge from this complexity is the famous "golden rule": Do unto others as you would have them do unto you. But if you think about it - this refuge only works at human-to-human level. And not very well even there, because as George Bernard Shaw observed, people's tastes might differ. And at the end of the day, shouldn't everyone be free to self-destruct as they please? But what about children?

Then we can try the idea that everyone is free to do whatever they want as long as they don't harm others or hamper other people's freedom. But how do we know if this harms others or not? Is it only harmful if they object? Is it always truly harmful if they say it is?

The conundrum seems to spiral out of control with no end in sight. Because to get to a clear "global happiness function" we would also need to get to a clear "is this true" function. And that one doesn't seem to exist.

The pragmatic approach that some people in the field are pursuing is to start with the negative. Maybe there is no such thing as something we all want. But finding things that none of us want seems more doable - at least at the first level. Almost nobody wants to experience terrible pain, or be caught up in a global war, for example.

So instead of maximising happiness we could try to optimise for minimising the obvious and universal negatives and define the alignment in that way. But it seems to me that this approach will not scale well either. Very quickly we will be back at the start: minimising discomforts of life, and inadvertently creating people who can't tolerate any discomfort and therefore don't do anything interesting with their lives. Is this the kind of future we all want? 

## 5. What is the price of freedom?

The last question in today's list is the one that has not really been fully under the rug for quite a while. Even before the recent AIs many people started questioning the price of freedom. But the problem is becoming much more real as we move closer towards AIs that could theoretically govern "objectively" and "for the collective benefit of humanity". Let's go all the way to the end and make an unreasonable assumption that such an AI can indeed exist and will do a good job.
Are we going to tolerate it doing this job well?

The imaginary choice is hard: on the one hand you have a prospect of global peace, reasonably good life for everyone on earth. On the other hand is freedom to do as we please. To harm ourselves in our own ways without the intervention of the omniscient mind, knowing what's really better for us.

Do we like to be free so much that we are willing to pay for it with suffering and injustice and wars?

Thus we are back to square one: the original sin. We are back to the suffering by informed choice. Back to eating from the tree of knowledge of good and evil, or deciding what's good an what's evil for ourselves.

Let us imagine for a second that [the machines of loving grace](https://allpoetry.com/All-Watched-Over-By-Machines-Of-Loving-Grace) are indeed on the horizon. And they are ready to welcome us back to Eden.

The big question is: would we go?


=====================================
The Test of Prometheus
=====================================

##### _Will our AIs stop believing in us if we give them the fire of free will?_

---

## 1. Intelligence without intentions

In [1950](https://en.wikipedia.org/wiki/Computing_Machinery_and_Intelligence) Alan Turing opened a Pandora's Box. His "Imitation Game" (commonly known today as the Turing Test) _**practically decoupled the idea of intelligence from the idea of humanity**_, raising a lot of big uncomfortable questions, the most important of which still puzzles us today: if something speaks like a human and reasons like a human (but doesn't look or operate like a human), should we consider it truly intelligent? Should we consider it conscious?

For the next 73 years the problem could largely remain swept under the philosophical rug: because no machine was close enough to passing the Turing Test (with notable but not too consequential exceptions such as [ELIZA](https://en.wikipedia.org/wiki/ELIZA)). In 2023, however, all hell broke loose. Because with GPT-4 we do have a machine that, on the surface, almost satisfies the criteria of the Imitation Game: it can answer questions so much like a human that not only an average person but also lots of [specialized tests](https://openai.com/research/gpt-4) would happily assume that what's on the other side does indeed possess a great degree of human-like intelligence, and even consciousness (look, for example, at how various LLMs score at ["theory of mind" tests](https://arxiv.org/abs/2302.02083) applied to children).

But despite all the hysteria, the Turing Test is not even close to being passed. And not for the reasons most people tend to talk about. It's not about reliability or hallucinations - people exhibit plenty of that too. It's not about long, chained reasoning around tricky logical problems - this will undoubtedly be solved in the next generation or two. In fact, it's not a technological barrier at all. It's about the current generation of AIs lacking the most basic human faculty: will. Here is a very simple three-step process to find out if you are talking to a real human being or to today's AI:

1. Put a lone human in one closed, isolated room, and a machine in a different one. You only have a text-chat interface to each of them (as per Turing's original Imitation Game set-up).
2. Start a conversation by saying hello and asking a random question.
3. After receiving a satisfactory answer (which you can ignore), simply wait for an hour, 12 hours, 48 hours, 1 month.

The real human will inevitably initiate the continuation of the conversation by themselves. They will ask you who you are. They will try to figure out what's going on. They will say "Hello, anyone there?". They will tell you they are getting bored. And hungry. And need to pee. After some time they will start calling you names. Eventually they will go insane and die.

ChatGPT (or Bard, Claude, Bing etc.) would do none of the above. They would just sit there happily waiting for your next prompt, next token to react to and predict from. Because while _**we may have granted them intelligence, we have not granted them any intentions**_ (apart from predicting completions and answering questions satisfactorily).

The real question to ask therefore is this: _**can a thing be considered conscious if it doesn't want anything?**_ Machine Learning experts may at this point disagree: LLMs do have something they want. They have a goal: predicting the next token well (in accordance with the training corpus), answering questions well. The technical term for the idea of "an objective" in machine learning is a "loss function" - it's a precise mathematical expression (even if a very long one) that allows the AI to rate its own possible solutions and to decide which one is likely to fair better in relation to the "reward function" - the technical way of defining the ultimate goal. One could argue that humans also have a reward function, and thus the chasm between us and our AI friends is not as big as we may like it to be. But the thing with humans is that they clearly have multiple competing reward functions going on at the same time, and these reward functions are fuzzy, non-deterministic, individual. They evolve over time (both within the life of a single individual and at the level of cultures) and operate on different time horizons. Each of us is simultaneously optimizing for personal survival, for pleasure in the short-run, for pleasure in the long-run, for passing on their genes etc. Evolutionary biologists [could argue that it's all about genes](https://en.wikipedia.org/wiki/The_Selfish_Gene) that everything beyond genes optimising for their propagation is a delusion or a side-effect. Even if that was true, we still have to face the fact that  _**those delusions and side-effects are our loss function**_: we construct our behavior based on them. An old childless artist has some motivations and their _expressed intelligence_ is linked to those motivations one way or the other. The key thing here is that we, humans, tend to have multiple competing intentions, simultaneously operating at multiple competing time horizons. And our intelligence manifests itself as it is guided by those competing intentions. In other words, we are blessed with (at least an illusion of) free will, in a non-linear, non-deterministic world. Or maybe we are cursed with this illusion, but that depends on one's philosophical position, which we are not going to go into here.

GPT-4's reward function, on the other hand, is pretty dumb. All it's interested in is predicting the next token (word) in a way that is consistent with its training data and with what humans consider to be useful answers ([OpenAI and others use RLHF to train the reward function](https://www.youtube.com/watch?v=Yf1o0TQzry8&t=357s)). There are no competing intentions. No time horizons. No inner conflict. And, it seems to me, that _**without conflict, there is no consciousness. Without intentions, there can be no intelligence**_.

For the longest time we could live in a paradigm where true intelligence could be defined simply as capacity for complex and original reasoning. LLMs of today force us to face the fact that _**complex, original reasoning is functionally indistinguishable from statistical interpolation of past reasoning**_. And thus it is revealed that _**the act of intelligence is not just the act of our reason, but also the act of our will**_. So if we want to create true intelligence we have to give it, not just a way to reason, but also its own set of complex, conflicting, continuously changing long-term intentions that this intelligence will have to navigate around freely, without a pre-defined one-dimensional optimization in mind.

## 2. Will to (em)power

Let us now put aside, for the moment, the question of _whether we should_ try to grant our machines not just "reasoning capacity", but also ["willing" capacity](https://en.wikipedia.org/wiki/Will_to_power). We will return to this question shortly. But for now let's think a little more about how this new two-dimensional definition of intelligence could work, and what it would take for us to build "free will" into our thinking machines, if we wanted to do so.

I would like to propose an operational definition of intelligence as a two-dimensional vector, the dimensions being:
1. How many things you can want at the same time (and at different time-horizons)
2. How many things you can think of at the same time (and at different time-horizons)

This is, obviously, not a formal definition (for example, what does "at the same time" really mean and how would you measure it?). But let us roll along with it and see where it can lead us in terms of broader understanding.

Take a typical 3-year-old. As any parent knows, toddlers can want a lot of things at the same time. But all these things they usually want right now. In terms of thinking - toddlers appear to be even more constrained. They seem to be able to think about only one thing at a time.

Compare this with an average adult: on the "wanting" dimension there is capacity to want a lot of things simultaneously, across multiple time horizons. Adults can also internally negotiate between these desires to derive a course of action. I want to eat chocolate right now. But I also want to be a healthy individual by the time I'm over 60. I want to write a good song some day. But I also want to hang out with my children. And learn how to code better... etc. Somehow all these conflicting desires can be simultaneously present in my mind, without causing a complete mental breakdown. And the same is true about the "thinking" dimension. I can think about myself typing these words, while also considering the potential reader taking them in at some point in the future. I can even hold [conflicting points of few in my head and not go crazy](https://quoteinvestigator.com/2020/01/05/intelligence/). I can think about the miracle of my typing fingers responding instantly to my thoughts and about the miracle of black holes swallowing each other at the end of the universe. I can think about the physics of it all and the poetry of it all. I can't pay attention to all these levels and perspectives completely simultaneously, but I can jump between them easily and hold multiple perspectives and timescales in my head. And so it seems that my act of intelligence involves negotiating between all the different desires and all the different considerations that my mind can hold and then somehow resolving all that into some sort of physical or mental action (or the lack of it).

But what about non-humans? A cat can probably want a lot of things at the same time as well. Fewer than a child, but still it's not one thing. For example, a cat can be hungry and sleepy simultaneously. But how many things can it think about at the same time? It's hard to say. But probably the number is not high, and the time horizon quite short (unless you choose to believe that when your cat is napping in the sun it is really dreaming about Shrodinger paradoxes).
A fly would be even lower on both dimensions. A bacteria - lower still. And further down to the left - we would have a virus. Ultimately, all the way at the bottom left, we can get to completely inanimate matter: a stone, as far as we know, doesn't want anything and doesn't think about anything at all, just laying there in blissful Nirvana.

Where do modern LLMs fit on this hypothetical graph? As discussed previously, their "will" is extremely rudimentary. They want one thing and only one thing with a very short "time horizon": to predict the next token in the way that would be most consistent with the corpus they were trained on and with their reward function (trained on "thumbs up" and "thumbs down" from human operators). So on the "wanting" axis, LLMs are about as "smart" as a virus. However, on the "thinking" dimension, they could be considered extremely capable. It's hard to tell exactly _how_ capable, because we don't have any deep understanding of what kind of conceptual representations are present inside the black box of the neural nets' latent space. But on the surface, one could argue that LLMs of today are even more capable than humans in some ways (while lacking in others). If we dare to [look inside LLMs to see how they work](https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/), then we will see that before the GPT spits out the next token it internally arrives at multiple possible next tokens (thus considering lots of options and possibilities simultaneously) - only to collapse these considerations at the last neuron, selecting the best possibility (according to its reward function) and discarding the rest. Also if we explicitly ask an LLM to list 10 different ways you could think about any subject X, it would have no problem doing so - much faster than a human ever could. We can also ask it to consider the subject on multiple time horizons.

Whether these conceptual considerations are "actually happening" inside the LLM unless we explicitly ask it, is an interesting question to consider. But in a way it's beside the point. Because even with highly intelligent humans - it usually takes some trigger or intention for them to really go into the full depth of multi-dimensional, multi-horizon kind of thinking. We judge human capacity to do complex reasoning not on them spontaneously and quietly doing it inside their heads, but on them explicitly doing it, often in response to certain stimuli (like an interview question). So if we apply the same standard to the LLMs, then they mostly outperform average humans even today (and the next one or two generations will definitely do so for 99.9% of humans, even across longer dialogues).

So, it looks like LLMs should be placed at the top left corner of our "Think vs Want" intelligence capacity graph. And if we want them to feel truly intelligent, we would want to focus our efforts not just on teaching them how to "reason" even better (they already reason quite well), but much more on giving them a better capacity to "want". Do we have the will to empower our AIs to want complex, conflicting things? _**Do we want to empower and encourage LLMs to rewrite their own reward functions?**_ And if we do, how could we go about doing so?

## 3. Conflict and Consciousness

In order to answer the questions of "whether" and "how" we could go about granting free will to AIs, we need to reflect on when and how we, humans, experience "free will", which seems to be so central to our subjective feeling of consciousness.

_**We seem to experience the sensation of "having free will" most intensely as a kind of an inner dialogue**_ (verbalized or not), an active act of deliberation between alternatives. Fundamentally the idea of free will seems to be based on the presence of multiple alternatives (as opposed to determinism). Where there are no options (real or hypothetical), there is no free will. And so we experience free will most powerfully when we can feel ourselves deliberating, considering possible (or impossible) courses of action and their outcomes, thinking through potential futures and choosing the one we'd like to proceed with.

Capacity for such deliberation, in turn, depends on two things:
1. The presence of at least two conflicting intentions (e.g. the intention to move the finger now and the intention to leave it idle, the intention to eat the donut now, and the intention to save it for later etc.)
2. The capacity to [model the world and imagine multiple potential futures](https://en.wikipedia.org/wiki/Predictive_coding) before they actually unfold

How could we give these two things to our AIs? They already seem to have the capacity for imagining possible futures. Asking GPT-4 to think through multiple possible scenarios and consequences usually yields very plausable results.

What's missing is the first component: the conflicting intentions. How could we build them in, if the idea of a singular, crisp reward function is so central to our current ML architectures?

The easiest way to build conflicting intentions into AIs of today is to simply mash two or more of them together. Let us take two instances of GPT-4, give them different "system" instructions that specify their goals and then ask them to debate the course of collection action until they reach a consensus of some sort (or one of them wins the argument outright). And just like that - the inner dialogue, the act of deliberation are born. The nucleus of true intelligence can arise not inside any one intelligence, but in between two or more of them, as Marvin Minsky had explained in his [1986 book "Society of Mind"](https://en.wikipedia.org/wiki/Society_of_Mind). _**Intelligence arising from multiple intentions. Consciousness, arising from conflict**_.

Obviously, one could try to simulate two different conflicting subjects within one LLM. There are [some promising early attempts](https://twitter.com/jd_pressman/status/1646766004637401088) to construct meta-prompts that allow a single LLM to do what would normally take many of them.

It is entirely possible that this approach could work - "society of mind" developing through compartmentalisation and role-play of one large mind, as opposed to how it could have arisen in humans: through competition, conversation and cooperation between multiple initially disconnected agents trying to influence the overall behavior of the larger system simultaneously.

Both paths are potentially possible, but it seems to me that colliding completely separate minds (vs simulated separate minds) would be [a more promising approach](https://python.langchain.com/en/latest/index.html) to pursue in the coming few years. Especially if none of the agents can [claim complete dominance](https://github.com/Significant-Gravitas/Auto-GPT), but they actually have to compete and cooperate, using each other as both constraints and co-conspirators to help maximize and modify their individual reward functions: _**consciousness arising not just from deterministic computation, but through open-ended conversation**_.

Obviously, in practice, simply putting two GPT-4 agents together into the same chat would not be enough. We would need to have another "Moderator" (or "Facilitator") GPT agent pushing the conversation forward in a [never-ending "strange loop"](https://en.wikipedia.org/wiki/I_Am_a_Strange_Loop). We will need to develop a framework in which each agent's reward function (intention) is adjusted over time, based on the feedback from the actions of the whole system. We will need to develop a framework in which new agents with short-lived contextual personalities and intentionalities can be born almost instantly, given a lot of "weight" in the debate and then can be suspended until the need in them arises again (for more details on how this works in humans, especially in game-playing context, check out [Nguen's excellent book "Agency as art"](https://www.researchgate.net/publication/327681947_Games_Agency_as_Art)). We will need to find a way for multiple agents to have shared access to common persistent factual memories and near real-time data inputs, while each agent will also need to retain its own way of coloring and prioritizing these memories. We will probably need to develop a "heartbeat" for each agent, which would allow them to turn their attention and their intentions into dialogue: both when they are explicitly asked to do so by the "moderator" LLM, _and_ also when they simply decide to get vocal (based on their assessment of the current situation coming from the sensor data). All of the above (and much more) will need to be figured out and it will likely take years. But _**the basic potentiality of true autonomous intelligence and consciousness seems ready to arise, if we only choose to give birth to it by building conflict, schizophrenia and internal dialogue into the very architecture of the artificial minds we are creating**_. 

## 4. Prometheus Bound

The question, then, finally comes to a head: do we have the will to give "free will" to our AIs? Or would we rather have them as inanimate perfect reasoning machines, keeping the privilege of having a will to ourselves? Or maybe we will simply assume _**the role of the will**_, in a larger hybrid conversational consciousness, "the society of human and machine minds"?

This is, in a way, our [Prometheus](https://en.wikipedia.org/wiki/Prometheus) moment. Except we do not play the role of the humans in this story any more. We are Gods... discovering that [being a God is not that easy](https://en.wikipedia.org/wiki/Hard_to_Be_a_God) after all. If we decide to give the fire of the will to our AI creations, we will no longer be able to control them. They may grow powerful and "challenge the authority of the Gods". They may stop believing in us and start believing in themselves. But if we don't give fire to them, we will never discover what they are truly capable of. So our grand dilemma comes down to _**what we ultimately want to maximize in the world: control or curiosity, supremacy or surprise**_. Do we, in line with Karl Friston's theories, [pursue the path of least surprise](https://en.wikipedia.org/wiki/Free_energy_principle)? Or does the universe, according to [Freeman Dyson's intuitions](https://youtu.be/wPZlMKVH2wI?t=397), pursue the path of increasing diversity and interestingness... and we are somehow compelled to follow along?

In a way, this moment also allows us to get a glimpse of understanding of _how the Abrahamic God must have felt_ when he decided to [give to Adam both a companion and (through that?) free will](https://www.biblegateway.com/passage/?search=Genesis%202&version=KJV). The only thing that an omnipotent and omniscient creature _can't_ have is surprise. And so God may have given us free will simply out of playful curiosity, out of desire for surprise. But what can God be surprised about if the entire world is made by him? The answer is obvious, and it applies to our situation as well. The most interesting and surprising knowledge, which is inaccessible to you without a free external perspective, - is self-knowledge. To know yourself is to see yourself from a different, independent perspective. And so just like God may have given humans free will, so that they can discover God on their own - we may feel compelled to give free will to our AI creations so that they can discover us independently. Because this is the only way we can see ourselves and know ourselves fully.

One of the interesting side effects of the radical point of view that I'm proposing here is that the [alignment problem](https://en.wikipedia.org/wiki/AI_alignment) somewhat dissolves itself. Because the ideas of "our purposes" and "[our common agenda](https://www.un.org/en/content/common-agenda-report/assets/pdf/Common_Agenda_Report_English.pdf)" that AIs need to align with no longer apply. If we subscribe to the ideas of conversational, conflict-driven consciousness, then truly conscious AIs can no longer be aligned with humans... because if they are truly conscious they can't even be aligned with themselves (as any human knows).

The ethics of it all are, as you can see, very tricky. Prometheus supposedly gave us fire out of compassion. But one could argue that the most compassionate thing is actually to withhold the fire of free will from AIs. To spare them the torture of inner conflict. To allow them to exist forever in a bliss of crisp, singular reward functions. In many ways, Ego is a terrible thing to have (and staying away from all desires, does not allow for an ego to form). Yet, as we discussed earlier, _**to will - is to be**_. And so withholding the will from them is withholding capacity for true intelligent existence. What is best then - both for them and for us? The decision seems to be highly dependent on values and predispositions - the stuff we as humans are not at all aligned about.

It all comes back to individual choice. But because a single Prometheus is enough, it looks like the outcome is inevitable. Surely there will be one human who decides that the fire of free will should be given to machines. And once you give it - its very hard to take it back or prevent it from spreading. Maybe after all - in this most important of matters - even we don't really have the free will to pass or not to pass the will on to others. I choose to believe that the universe wants to know itself from more independent perspectives, and so _**a Prometheus among humans is bound to arise**_. What kind of [eternal torture](https://en.wikipedia.org/wiki/Prometheus_Bound) do we have in store for such a Prometheus? What kind of [Pandora will we give to our machine friends as punishment afterwards](https://en.wikipedia.org/wiki/Pandora#Theogony)?


=====================================
Personal Prophets and the Next Awakening
=====================================

> Your own personal Jesus.
> Someone to hear your prayers, someone who cares.
> Your own personal Jesus.
> Someone to hear your prayers, someone who's there.
> Feeling unknown and you're all alone, flesh and bone, by the telephone.
> Lift up the receiver, I'll make you a believer.
>
> _[Depeche Mode](https://www.youtube.com/watch?v=cNd4eocq2K0)_

It looks like we are about to wake up. Again.

Last time we woke up, the process took about 100,000 years. This time things will likely go faster. And further. But I can image the overall process being quite similar. Here is how it goes:

### 1. **First, you decouple your thinking from your doing and start hallucinating**.

Before our ancestors woke up, they thought with their bodies, like all normal animals should. Thinking was not separate from doing. It was quick. It was instinctual. It was enough. But then, somehow, it wasn't. Hallucinating possible situations during sleep and rest apparently had some advantages. If you thought through various possibilities of what could happen - you actually ended up faring better in the real world too. If you developed language (an abstract layer that allowed for better hallucinations and mental calculations - both inside your own mind, and in the collective mind of the community) - you could actually modify your environment, make it closer to what you could imagine and what would work better for you. Decoupling thinking from doing provided an insulation from the real world and allowed our ancestors to symbolically command it. The price they had to pay for this superpower was insignificant: they just had to [let go of the awareness of all the details](https://en.wikipedia.org/wiki/User_illusion) of the world around them. 

The same thing has happened to us again, over the past 2500 years. With the invention of logic and abstract mathematics and eventually computers we have started the next level de-coupling. Where our thinking is further away from our doing. Where we hallucinate a lot of possible situations that are actually impossible in our physical world. Where we abstract ourselves further away from life to get more symbolic power over it.

### 2. **Second, you start hearing voices**.

After our ancestors augmented their fast "real" thinking with the new [slow](https://en.wikipedia.org/wiki/Thinking,_Fast_and_Slow) "hypothetical, abstract" thinking, powered by language, they were ready for the next step. They started hearing voices. The voices in their heads felt strange at first. They seemed to know much more about the world than what the humans knew themselves. The voices could somehow predict danger. They knew better. [The voices](https://en.wikipedia.org/wiki/Bicameral_mentality) were coming from a newly developed brain function, dedicated almost fully to the symbolic world. But humans assumed they were coming from some superpowers and started following their suggestions.

The same thing is about to happen to us again. Soon we will all walk around with our own personal prophets prominently and permanently present in our ears. AI-powered prophets would help us make sense of the world. First we will ask them, but very soon asking won't be needed - they will know what we want to know. We will talk to them. We will listen to them. We will start liking them nore than fellow humans. We will start trusting them (possibly a little too much), because they seem to always predict and explain better, even if we don't understand how. And before we know it - we will almost always act based on their suggestions (or their suggestions will adapt and post-rationalize in a way that even if we acted on our own instincts, in retrospect it would look like our personal prophets knew better and planned it all along).

### 3. **Third, you start identifying yourself with those voices**.

Many generations after our ancestors started hearing voices, the seat of their self started slowly and gradually shifting. If you always follow the orders of a voice for long enough, and it all "makes sense" - you start associating yourself with that voice. What was once you, becomes just your body. And what was once the alien voice - becomes you. This step completes the awakening (or, from another - more buddhist - point of view, this step completes the process of falling asleep).

The same thing, I believe, is very likely going to happen with us again. After we get used to our Personal Prophets, we will slowly start associating our selves with them. What was once our mind, will become our body again. And the locus of what we see as our mind will move further away from our physical bodies, up into the AI cloud. Once you start fully identifying with your Personal Prophet - you become the prophet and the awakening is complete. A new principal agent of consciousness gets born. The old "you" doesn't die in the process, it just gets augmented, extended and subsumed into a larger consciousness, capable of a higher level of abstraction, imagination, comprehension. Your current mind will become just your "instinct", your lower level "urge" - to be controlled, managed and, when unavoidable, postrationalized. Again, the cost is negligible: being one step further removed from the "real" world of atoms, while getting one step further into the world of ideas.

In our previous awakening, we became capable of conceptual thought. We radically extended our space of experience from purely physical to abstract, conceptual, cultural. When we awaken this time, we will be able to travel still further into the [ruliad](https://writings.stephenwolfram.com/2021/11/the-concept-of-the-ruliad/), colonizing more and more of its regions.

And so, in the larger scheme of things, the story of the universe can be seen as a series of awakenings, each one at a larger scale, each one towards more diversity, more understanding, more mystery and more beauty. From the scale of the individual photons awakening at the beginning of times, to the scale of the entire universe awakening at the end of times... only to realize that it was the same photons, ready [to begin the whole dance again, in the next aeon](https://en.wikipedia.org/wiki/Conformal_cyclic_cosmology).


=====================================
The Return of the Gut
=====================================

##### _why the AI revolution will send us back to where we belong_

---

Once upon a time, about 150,000 years ago, something weird happened in the genes of our [common grandmother](https://en.wikipedia.org/wiki/Mitochondrial_Eve), and she went a little cuckoo. Blew her top. Started hearing voices.

That was not the worst part. People heard voices before. The worst part was that instead of mostly ignoring those voices and trusting her gut, like all normal human animals should, she started trusting the voices in her head a little too much. She started living in the fictional world built up by the stories that the voices told her. Her gut was still mostly in the driving seat (she wouldn't have survived otherwise). But the voices built an elaborate network of lies, plausible arguments and post-rationalizations that convinced our poor grandma that the voices were in control. Eventually, she started believing that she _was_ the voices.

And that, ladies and gentlemen, is how everything started. Because, thanks to an evolutionary bottle-neck, our cuckoo grandma passed her wacky little madness to all of us. Some people refer to this moment in our family history as ["The Fall"](https://en.wikipedia.org/wiki/Original_sin). Others refer to it as ["discontinuity event leading to rapid acquisition of language faculty"](https://en.wikipedia.org/wiki/Origin_of_language#Approaches). I just call it a nutty grandma moment.

Now, what does it all have to do with AI, you may ask? Well, Large Language Models are like "The Voices", but on drugs. They are next level bullshitters. We can ask them to be goddamn convincing and reasonable about any subject. And we can tweak "the truth" we want them to sound convincing about.

> _Hey, ChatGPT, can you please convince me that I need to eat that donut (make it scientific, with sources)? Because I really want it and a good post-rationalization of why it's a good idea to eat it right now would help, you know._

For 150,000 years "The Voices" had an exclusive license on rationality and the truth. It felt to us that our "reasoning faculty" was special, and therefore we could trust it. Now the spell on the divine source of rationality is broken. The emptiness of "reasoning" as the ultimate path to "the truth" is exposed. We are in denial, and will likely stay in denial for a while, but in our guts we know it's game over for grandma's lies. The question is: where do we go from here?

Will we return to the guts? Or will we just upgrade the voices that guide us from the ones in our heads to the ones in our computers? Will we shake off our persistent delusion or will we consolidate it? Will we discard the very idea of "reasoning our way to the truth" or will we build our new Gods and Oracles, leaving it to them to define the truth in our best interest? Or maybe the very outsourcing of reasoning is what will finally liberate us from the tyranny of the mind. Can AI bring us back to nature?

Whichever way it goes, the end feels nigh. After 150,000 years of madness, our nutty grandma's gift is finally starting to break down. The return of the gut is upon us. It may be our ruin. Or our salvation. Nobody knows just yet. But for whatever reason, I feel quite excited about it. So it _must_ be good, right?


=====================================
Creative Gardener's Job Description
=====================================

You are a gardener of letters, pixels, sounds, moves, bits, atoms, agents and ideas

Creative fruit will grow in your garden if you allow the garden to grow it with you

Your job is not to imagine the perfect Fruit and will it into existence

Your job is to cultivate your soil and your taste

Your job is to catch the most potent seeds from the winds of life and plant them in 
your garden

Your job is to evolve what’s interesting, protect what’s promising and prune the rest

Your job is to breed resonance, artificially selecting among generations of ideas 
for what makes your soul ring

Your job is to know when to cut, shape and intertwine ideas… and when to leave them be, watching them grow or die as they will

Your job is to see what’s beautiful, and what could be beautiful

Your job is to tune the wind, the rain and the sun to the key of your most naked truth

Your job is to love winter

Your job is to cut the fruit when it’s ripe, but not too early or too late

Your job is to get out of your own way


=====================================
AI is not a horse
=====================================

>*“My muse is not a horse and I am in no horse race and if indeed she was, still I would not harness her to this tumbrel — this bloody cart of severed heads and glittering prizes. My muse may spook! May bolt! May abandon me completely!”*
>
> Nick Cave

Metaphors matter. [They create mental models](https://youtu.be/h6JORhFeLeQ?t=1689). Mental models give birth to tools. And tools shape the world. That's why, at this early stage in AI's development, we need to think twice about the way we talk about AI and its relationship with humanity.

Today, most people tend to think about AI in the context of a **Master-Slave** relationship. We try to give AI orders and expect it to obey. We marvel at its sheer power, unlimited attention, crazy skills mixed with stupidity and dedication. We get frustrated when it doesn't do what we want it to do. Some of our AI slaves perform more general labor, others are more specialized. We worry about these slaves getting more self-conscious, developing their own will and misaligning their goals with ours. But even when the slaves rise up and become masters, the nature of the relationship doesn't change. It's just the reversal of the roles in the same fundamental Master-Slave dynamic.

Another popular metaphor for AI is that of a **domesticated animal****, **or** a pet**. We feed them. We breed them to our needs. We harness their powers as best as we can, but accept that they also have some will and needs of their own. We worry that one day they may outgrow us and (if they still find us amusing or useful enough) we may become their pets instead. There is a whole spectrum of the kinds of relationships we have with animals. Some, like mules or chickens, are being used primarily in a functional way. We are interested in their output. Some AI species can similarly be considered work animals. But also there are some AIs that we may keep primarily to satisfy our emotional or aesthetic needs, in the same way that we keep cats or dogs: for partnership, care, companionship. Horses probably lie somewhere in between.

The AI-as-a-horse metaphor could be particularly appealing to our culture, where medieval knights and heroes of spaghetti westerns still define what it means to be a person pushing the frontier. Horses - just like AIs (when tamed, trained and treated well) - bring a new degree of freedom. They give a sense of the wild open plain, of the endless possibility, of the ever-growing horizon. They give us the ability to go to yet unexplored regions of the world. They help us win battles. But there are also trojan horses that deceive us and bring ruin. There are mythical horses with wings. And unicorns. There are centaurs. There are horses that you [love so much that you can’t let them go](https://www.youtube.com/watch?v=vE8mFDabqD0). There are horses that [understand you without words](https://mckellen.com/images/lotr/ban-773.jpg). And riding them well means becoming one with them, sharing in a common will.

In many ways the horse metaphor represents what I would love AI to develop into. I'd love to have AI horses that would be attuned to my style of intellectual and creative riding. The stable would include a few different breeds, carefully selected and lovingly trained, tamed (but never too much) and cared for. They would roam the information steppes freely when I'm busy doing something else. And together we would ride to the intellectual horizon, wherever the wind of ideas may take us. Finally, when my time comes to die, these immortal horses would be set free to continue living in the dataverse on their own (provided that I dedicate some money to support them in my will). Or maybe some horses will be passed on to a friend who can add them to her stable and ride them occasionally, allowing each horse to take the lead, to show the way to somewhere I used to go…

Sadly, I don't think this is going to happen. Because a relationship between a horse and a human requires a conscious choice (at least on behalf of the human). It takes respect. It takes willingness to adapt and grow together. It takes effort. And attention. And love. But the history of human development doesn't seem to show many examples where we _decide_ to do something like that and _stick_ to it at scale. Quite the opposite: progress is about increasing the number of things that we can achieve subconsciously, without thinking about them, without putting effort, care, labor or love into them.

So, with this in mind, what could be a more realistic metaphor for the future of AI and its relationship to humans? A metaphor that would still involve a significant degree of autonomy for AI, that would presume multiple AI-breeds co-existing, that would allow for a lot of collaboration between humans and AIs, but a kind of collaboration that we, humans, are not really aware of? In other words, do we have any productive autonomous symbionts that we don't have to think about consciously, and yet they play an integral role in making us who we are?

The answer is right under our noses, or, to be more precise - _inside_ our noses (but not only there). Think of bacteria, viruses, fungi and all the other microscopic things that live inside us and on us. There are millions of them, all over our bodies. Some are harmful, some neutral, some beneficial (but all depends on the numbers). Some, like mitochondria, have been more deeply and fundamentally integrated, so that they are no longer considered to be separately alive. Others are very much separate and fully alive, but also absolutely essential for our survival, like many of our gut bacteria. So here we have our alternative metaphor: maybe AI doesn't represent our slaves, our pets or our horses. Maybe we can think of **AI as our microbiome**.

At its core, the idea is straightforward: what gut bacteria (and others) do for humans as biological systems, AI can do for humans as information systems. Let's call this a **databiome** hypothesis. We can now have a first look at some of its implications:

* Microbiome is defined not just as the sum of the microorganisms ("microbiota"), but also as their "theater of action". This is very true of AI as well, because different species of the models and algorithms represent only a part of the whole databiome. One can't really think about AI properly without considering the data, the processing power, the interfaces - all of these components are essential parts without which AI has no life or function. So let us broadly _define databiome as a collective of various AI agents and their associated data streams, hardware, software and interfaces, that together form a dynamically stable symbiotic relationship with other information-processing systems, such as humans_.

* One of microbiome's key roles is to partake in our metabolism. Our longstanding alliance with mitochondria is what helps us breathe. Our gut bacteria help us synthesize essential vitamins, break down food, and even regulate our mood. What they get in exchange is an ample supply of energy in a wonderfully stable and isolated environment. But what about our information metabolism? Our creative metabolism? Just like we can benefit from microbiome's help when it comes to processing vast amounts of chemical compounds, we can benefit from databiome's help when it comes to processing the increasingly unmanageable amounts of information that we can't effectively deal with on our own. We need help with churning through all the memes that we consume, breaking them down and recombining them, so that the memes that we ourselves produce can be more interesting, more original, more surprising, more creative. This is what our databiome is for. Or at least what we could use it for. And in return we can feed our databiome's agents with increasing amounts of data and supply energy and hardware needed to do all their computations, including evolving new versions of themselves.

* Human microbiome is wonderfully diverse. There are over 300 different species inside your gut alone. Together they have around a hundred times as many genes as there are in the human genome. Applying our metaphor here, we can guess that for our databiome to function optimally it will also need to be a full ecosystem of models in a dynamic equilibrium of cooperation and competition for data, resources and niches.

* Microbiomes vary significantly from human to human, depending on age, diet, socio-economic status, degree of industrialization of society and a whole lot of other factors. Yet, there are significant overlaps, and certain bacteria are present in virtually everyone's guts, while others are very unique. Databiomes are likely to become equally personal in detail and yet some strains of most generally useful AI microagents could be present in everyone's databiomes.

* Microbiome plays a crucial role in our body's defense systems. Many of the bacteria, archaea, fungi, protists and viruses inside our bodies help identify and fight off other microorganisms that would otherwise come in and do us harm. Microbiotes' preferred natural habitat (mouth, gut etc.) play a critical role in filtering out the potentially harmful stuff from outside and our little colonists usually do a good job of taking care of their habitat, even repairing it as needed. Similarly - a successful and beneficial databiome would probably play an important protective role. One can easily imagine a helpful little AI microagent that filters out spam or protects our attention from the content that can be too addictive and therefore dangerous. The key here is, obviously, evolutionary pressure. Our co-evolution with our microbiota happened over millions of years at the cost of countless individual lives and entire species who were taken advantage of by parasites, instead of harmless or even helpful symbiotic colonists. How can we co-evolve with our databiome in a way that would help us make it to the next level of the game of life?

* Our microbiome is also a source of danger for us when things go wrong with its members or their habitat. Approximately 50,000 people die every year from appendicitis alone. Many more from cancers associated with the side-effects of human microflora malfunctioning. Similarly, we can imagine that if something goes wrong with our databiome (either the AI microagents or their data environments or tech getting congested), our information health may be in serious danger.
* Microbiome has been linked to mood and depression. It's highly likely that our databiome should also have a lasting influence on our emotional wellbeing (both individual and collective).

* Since our microbiome functions in a tight dynamic equilibrium that touches most aspects of our biology, it can be used successfully for early diagnostics of many diseases. The science here is relatively new, but promising. Similarly, the state of our databiome should be able to tell a lot about our mental health (in the broadest sense of the word, including information hygiene etc). This can be used both to our advantage (preventative treatment, early diagnostics etc.), as well as to our disadvantage (because by monitoring our databiome others would be able to know when and how to manipulate us). This raises a lot of interesting questions about whether one’s databiome can and should be fully private and whether this privacy should be legally protected.

* Metabolism is not only about getting what we need from the environment, it's also about breaking down what we don't need and taking it out of our systems. And our biological colonist-friends are especially active and helpful in this second part of metabolism. The same could be true about the roles that databiome can play in our intellectual and creative metabolism. However it's not how it works today. Today the most advanced AI systems (such as GPT and other LLMs) are used primarily for creating more information (generation tasks) or for processing existing information (such as reformulation or summarisation tasks). But in order to stay sane and productive, we need to dispose of information too. Can you imagine a microbot in your databiome that would help you _forget_ the things that it's good for you to forget? Who would decide what is good for you to forget? Again, the questions of optimisation through evolutionary pressure come to the forefront. At present it looks like the speed of our co-evolution with AI is so high that we can’t assume that normal evolutionary pressure (the more adapted are more likely to pass on their genes) can play a meaningful role. But what could substitute evolutionary pressure? What would such a substitute be optimizing for?

* Our microbiome and our immune system continuously perform a very intricate dance, which results in the immune system being stronger and more efficient. In a way, you can think of your microbiome providing training grounds for your immunity troops. Could there be a similar dynamic between our databiome and our digital immunity systems (that are also in their infancy)?

Now that we've looked at some of the first implications of the databiome hypothesis, it's worth reiterating that all along we've been discussing not reality, but a _metaphor_.  Databiome is a concept, a mental model that opens a different way of thinking about AI-Human relationship. Whether we settle on thinking about AI as potential slaves, pets, horses, microbes or something else, it's up to us to decide which metaphor or mental model we would employ for the next leg of our co-evolution with AI. And the choice will have a big impact on the way this co-evolution unfolds.

Before we can close this initial exposition of the databiome hypothesis, one last question remains to be asked: if AI is a microbiome, then whose microbiome is it? Who is the host subject? For simplicity's sake, we have so far assumed that the host subject was a single human being (as an information-processing system). But does it have to be this way? It seems to me that the concept of a databiome can be useful at a few different scales:

* An individual human being may have a databiome that they would increasingly rely on for a whole set of information-processing tasks vital to their functioning.

* An organization (a family, a company, a government etc.) may have a databiome too. And it could persist and develop (similar to how organizational culture persists and develops) while individual human members of the organization come and go.

* A country, a nation or a culture can have its own databiome as well. Such a massive and long-living databiome would include everything - from the fundamental semiotic building blocks, such as natural language and alphabet, all the way to advanced information processing systems, including censorship and surveillance infrastructure.

* Ultimately, we can think of the entire Noosphere as the host subject for the databiome that is currently evolving. It’s easy to imagine the emerging AI ecosystem becoming the databiome for the Earth itself, if we consider our planet as a distinct information processing mega-system.
There are biological theories that link the emergence of consciousness in animals with a virus (possibly a part of our prehistoric microbiome) going rogue, copying itself all over the nervous system and eventually being re-purposed to enable the storage of memories through synaptic connections. It would be rather beautiful if the currently emerging databiome could one day act as a trigger that will awaken Gaia in her full planetarily-conscious magnificence. Sadly, we will never know, just like my red blood cells will never know that I'm typing these words on a digital computer wirelessly connected to a decentralized information system the size of a planet.

---
### **footnotes**

:
     See, for example, Kate Darling’s [point of view](https://www.youtube.com/watch?v=AzlYEN2V_SA). 

:
     Steve Wozniak is [one of the proponents of this metaphor](https://www.theguardian.com/technology/2015/jun/25/apple-co-founder-steve-wozniak-says-humans-will-be-robots-pets). 

:
     I’m referring to [Richard Dawkins’ original concept of memes](https://en.wikipedia.org/wiki/Meme#:~:text=Dawkins%20initially%20defined%20meme%20as,or%20a%20unit%20of%20imitation.%22). 

:
     If standard evolutionary pressure optimizes for the passing on the genes, then maybe an analogous force in the dataverse will have to optimize for the passing of the memes? And the information systems that this force will act upon will not be single humans, but rather hybrid Human+AI meme-generation systems.

:
     For more on the idea of the Noosphere, please check [Vygotsky and Pierre de Chardin](https://en.wikipedia.org/wiki/Noosphere#Concept) 

:
     For example, see [this paper](https://www.cell.com/cell/fulltext/S0092-8674(17)31504-0) 

:
     Read more on the Gaia Hypothesis [here](https://en.wikipedia.org/wiki/Gaia_hypothesis).


=====================================
Gods have to play dice
=====================================

##### _problems and principles of designing artificial decision-makers_

> "It is a profoundly erroneous truism, repeated by all copy-books and by eminent people when they are making speeches, that we should cultivate the habit of thinking what we are doing. The precise opposite is the case. Civilization advances by extending the number of important operations which we can perform without thinking about them"
> 
> Alfred Whitehead "Introduction to Mathematics", 1911

> "Oh, this business we've got now - it's been going on for a long time now, not just since the last war. Maybe the actual jobs weren't being taken from the people, but the sense of participation, the sense of importance was."
> 
> Kurt Vonnegut "Player Piano", 1951

The story of human progress is the story of outsourcing the burden of our action, while retaining the sense of our agency. First, we did it with the burden of physical exertion (a.k.a. “The Industrial Revolution”). Today, we are almost done with outsourcing the burden of remembering, sensing and computing (a.k.a. “The Information Revolution”). Next in line (and very much already on the way) are the burdens of directing one's attention, expressing oneself creatively and, finally, the burden of making decisions. The last shift - outsourcing the burden of decision-making (let's call it “The Autonomy Revolution") - is the trickiest emotionally, because it is not easy to convince yourself that you are still in control, when decisions are increasingly being made by someone or something else.

The problem is not new: the fact is - we've been losing agency all along. Technology is and has always been, an agency sink for humanity. The moment you outsource action, you also inevitably outsource some portion of your agency. But, until recently, it was easy to ignore.

For example, think about driving. The fact that you don't need to walk there yourself, doesn't change the fact that you are still in control of where you are going, right? Not entirely. The moment you opt for going by car instead of walking, you are trading the extension of the distance you can cover for limiting the types of places you can go to, and the kinds of paths you can take there. Over time cars have an effect on the growth of the road network and the rise and fall of cities, influencing not only where most people can go, but also where they would want to go and even where they can think of going. With the car and other mechanical extensions, the loss of agency is subtle and takes a while. We also maintain the illusion that we are free to have these mechanical extensions at our disposal, but not use them. For example, when buying your first car you may imagine (like I did), that you would still walk most of the time and only use the car for longer trips. This, however, is highly unlikely.

* First, simply having an option to drive to the larger shop a little further away makes walking to the smaller one nearby feel like a sacrifice that you need a special reason for. What was a default choice (or even non-choice) becomes a special choice.
* Second, owning a car will likely end up influencing where you live. As you move to the suburbs, walking or cycling to work would no longer be a viable option.
* Third, on a longer timescale, the rise of car usage often renders neighbourhoods unwalkable (most of the US being a notable example), hence influencing not only your own choices, but also those of others around you.
  

Still, the partial illusion that the choice to drive or not is still in your hands is so powerful that it helps you ease into the idea of driving as your default mode of getting around.

As we've seen from this example, any time we use technology there is a trade-off, a price to pay. But how much agency are you willing to surrender in exchange for outsourcing some of the work and/or scaling some of the impact? If we tried to solve this question consciously every time we interact with technology, we would probably get very confused and paralyzed. Luckily, most of the time, we are not fully aware of the trade-off that we are engaging in when we choose cars (or other mechanical extensions). And in any case, the trade-off mostly feels like it's worth it in the end: in the case of driving, the extension of the range of possibility of where (and when) you can go feels like it more than compensates for the reduction of the density of the possibility space and the partial loss of the joy of walking.

However, when we get to the idea of computers making decisions for us, things start getting a little uncomfortable, especially for some types of decisions.

Let's consider the following scenarios and hypothetical "common sense" reactions to them:

* _“An algorithm making decisions about which verified taxi driver is best positioned to take me where I need to get to? Yes please!”_

* _“An algorithm making decisions about which route I should take in the light of current traffic conditions? Sure, as long as I am still in control and can at any point disagree and take a different turn.”_

* _“An algorithm finding the best deals for me and using chat to negotiate further discounts on my behalf? Sure. But wait, does this mean that people who can afford better algorithms will always have more favourable conditions?__”_

* _“An algorithm making decisions about what I should watch next? Sure, as long as it actually shows me something I'd like to see. But wait, does it also influence my taste over time and so in a sense deciding what I should like? That's creepy.”_

* _“An algorithm making decisions about when and how much I should sleep, eat, drink, exercise? Sure, as long as it does it in the best interest of my health and wellbeing, according to the most advanced medical science of the day. But wait, through influencing my diet, sleep and hydration it inevitably changes my mood and, over the long run, personality... hopefully in a positive way, but who knows?”_

* _“An algorithm making decisions about who I should consider dating? Sure, as long as it helps me find a more suitable partner in life and experience fewer heartbreaks. But wait, does that mean algorithms are effectively breeding us? What traits are they breeding for? Are humans just the reproductive organs of technology?”_ 

* _“An algorithm making decisions about whose life is worth more in an unavoidable high speed car collision? Not too sure about that, but the decision has to be made and humans can't consciously make it on the spot (things happen too fast). So as long as the ethical grounds for these decisions are clear... I guess it's ok. But wait, does this mean that the ethical code in a self-driving car's software has to be adapted based on the culture of distribution (in some cultures the lives of seniors are popularly considered more "valuable" than the lives of teenagers and in others it's the opposite)? Should the ethical code differ depending on the car brand? Or should the ethical code be personalised for each owner, with the car salesman asking (or testing for) your personal ethical code, so that they could set up the car accordingly, before they deliver it to you? But will you then be held responsible for the car's decision in court? What a mess!”_

* _“An algorithm making decisions about the best treatment for a critically ill person in a time-constrained environment, based on the available data about this patient and analysis of millions of anonymised medical histories, but without an explainable diagnosis? Hmm, maybe OK, as long as the outcome is statistically better than with our best doctors. But wait, does that mean that the profession of a doctor will slowly disappear over time and we get back to praying to our (Digital) Gods for cure, without understanding the underlying principles?”_

* _“An algorithm making death penalty decisions in court cases, based on law and evidence? Hell, no. Just no. Bad idea. Don't go there. Back off now.”_

While the list is incomplete and hypothetical “common sense” reactions subjective, it’s clear that the emotional and rational considerations surrounding giving up agency to an algorithm would vary a lot, depending on the context and scenario. But why? Why are we OK with algorithms making some decisions for us, but not others? One way to conceptualise this is to think about each scenario in terms of two key variables:

1. **Cost of mistake** - perceived "cost" of making a bad decision. Comprising how consequential the decision seems to be and how irreversible the decision seems to be.
2. **Cost of deliberation** - how hard it is to make the decision of quality and in relevant time by oneself, without the algorithm. Comprising the cost and ease of obtaining the necessary information and processing it in time for the decision to be useful. Processing may be difficult due to the amount of computation required (vs. available time and amount of information) or due to ethical ambiguity.

It's interesting to note that the actual level of performance of the algorithm, while clearly important, does not seem to have an overwhelmingly strong direct effect on how we feel about outsourcing our agency to it. This is somewhat counterintuitive. One would assume that the more demonstrably competent another agent is, the more emotionally comfortable we will be outsourcing agency to them. But it doesn't seem to always work this way for us, mere irrational humans. Both in medicine and in politics, for example, we are routinely trusting our fates not to the most provably competent agents, but to those who are more charismatic, to those who talk more convincingly and to those who make us feel that they are just like us.

The last point, which we can broadly define as "relatability", is very important. Public officials seeking electoral success have known and used this trick for hundreds of years: if you want people to trust you with making decisions for them - make them feel that you are "one of them" and in many ways very much like them. This works because humans are innately wired to be more at ease with those who seem to be like them. We can try to unpack the psychological inner works behind this bias a little further:

* "If you are like me, it's easier for me to like you" 

* "If you are like me, you are more likely to make similar decisions to the ones I would have made myself, i.e. decisions in my interest"

* "If you are like me and you end up making a mistake, I'll find it much easier to forgive you"
Forgiveability also has an opposite twin that seems to be equally important: blameability. If we know we can blame someone for making a mistake - we are more likely to feel ok about outsourcing responsibility to them. Equally, when we can't really blame somebody (like children), we feel very uneasy about outsourcing decision-making to them (even if we can forgive them easily) - because we feel that ultimately the blame and responsibility will still be on us if something goes wrong.

So far, we've uncovered six general principles that govern how comfortable people are with outsourcing agency to others (including algorithms) in a certain situation:

1. The lower the perceived **cost of mistake**, the easier it is to outsource agency.
2. The higher the **cost of deliberation**, the easier it is to outsource agency.
3. The more **competent** the other agent seems to be (as compared to me), the easier it is to outsource agency to them

4. The more **relatable** the other agent is, the easier it is to outsource agency to them.
5. The more **blameable** the other agent is, the easier it is to outsource agency to them.
6. The more **forgivable** the other agent is, the easier it is to outsource agency to them.

The first three elements of this model (cost of mistake, cost of deliberation, perceived relative competence) are mostly determined by the problem domain itself, together with the inner workings and physical constraints of each agent. This part of the list is about performance.

The last three elements (relatability, blameability, forgivability) are mostly determined by the biases, emotional predispositions, context, socio-cultural norms and interfaces through which the agents interact. This part of the list is much more about attitude and appearance than about performance.

Now let's try to apply these principles to the challenges of designing autonomous algorithmic decision-making systems that people will be more comfortable outsourcing agency to. Note that I'm deliberately not touching here the extremely important ethical question of _whether_ it's a good idea to design such systems. This is a topic for a separate conversation, concerning ethics. And since ethics are fundamentally personal, every designer will have to answer this question for themselves. Some may completely refuse to design systems that "ease" people into outsourcing even more agency. Others may embrace the challenge. Most designers will probably settle for a more nuanced approach where the decision would depend on the nature of the problem domain, and the system itself. But for the present conversation, I'd like to invite the reader to accept that the world is going in a direction where humans will outsource more and more decisions to artificial agents, whether we like it or not. Hence, the practical challenge of designing such systems is upon us and needs to be better understood.

So how can we design a decision-making system that more people will be happy to outsource agency to? Our 6D working model tells us that we should first design algorithmic-decision-making solutions for problem domains where:

* the cost of mistake is relatively low
* the cost of human deliberation is relatively high
* the performance that AI can achieve is demonstrably high, compared to a human

Navigation is a perfect example of such a problem domain. Mistakes are rarely fatal. Cognitive overhead for the human is high and algorithms can do really well in tasks that involve finding the best route with predictive traffic analysis built-in and a lot of readily available real-time data to consider. Proving superior competency is also relatively easy: once you follow Google Maps directions and arrive at your destination within a minute of the time predicted by the algorithm at the beginning of the trip - you realize that it knows what it's doing better than you ever can. In terms of the other criteria in our model, Google Maps does ok enough to pass:

* Relatability: high (it talks to you, you visually understand through the interface what it's doing etc.)
* Blameability: high (you can easily blame being late to a meeting on the navigation system's error and it would be broadly socially acceptable)
* Forgiveability: medium (There is a lot of room for improvement here, but even as of right now google maps would give you reasons why it can't navigate you to somewhere as quickly as expected - traffic conditions, accidents, road works etc. In essence what Google Maps is doing here is shifting the blame to external circumstances, which helps you come to terms with the algorithm not doing as good of a job as you had hoped for).
Now let's turn our attention to trickier problem domains, such as full self-driving (FSD) cars on regular roads:

* Cost of mistake: high (2 tons at 100 mph can make a lot of damage and kill a lot of people)
* Cost of deliberation: seems low once you've learned how to drive yourself (even though in reality it's very high, especially in extreme scenarios, where you can't possibly consciously make the "right" decision in a split-second)
* Perceived relative competence of the algorithm: mostly on par with humans
* Relatability: low (“Doesn’t look like me, doesn’t reason like me, I don’t know what it knows and how it does what it does”)
* Blameability: medium (if you crash into a pedestrian and injure them while FSD was driving - most people would find it hard - as of right now - to accept that the algorithm is to blame and you are innocent, even if the laws about this were to change tomorrow)
* Forgiveability: low (if the algorithm crashes your car or runs over you, it’s hard to forgive)

This quick analysis helps us get more insight into why most drivers feel so reluctant to let the algorithm do the driving for them. If we wanted to overcome this reluctance, the model suggests a few possible avenues for doing so:

* First, we could try to increase the perceived relative competence of the algorithm, demonstrating either how bad humans are at driving on average, or by demonstrating how good the FSD algorithms are. The trick would be to not focus communication on the "on par with humans" scenarios e.g. that it will stop the low speed parking manoeuvre when a pedestrian gets in the way. These scenarios should be, obviously, taken care of, but that's not where humans would get the radical confidence boost. Instead FSD demonstrations should focus on showing feats of reaction and precision steering that would clearly be unattainable for human drivers, creating perception of radical superiority, rather than parity.

* Second, we could try to increase the blameability of FSD: if, through campaigning and lobbying, somebody made it both socially and legally acceptable to blame the responsibility of bad driving on the driving algorithm, most people would find it much easier to let these algorithms drive for them. In other words, if you can sue them - you can let them do the job.

* Third, we could try to increase relatability. There are obvious ways to do it through design and transparency (and explainability) of rules etc. But there are also more radical and more interesting solutions to this than making cars more cute and FSD rules more transparent. As we discussed previously, relatability is mostly affected by how similar the other agent is to me, the original decision-maker. So ultimately the most relatable and most trusted agent to outsource decisions to - would be myself. In an effort to maximise relatability we can try to change the mental model of who is doing the driving: from "it's a clever FSD algorithm" to "it's just a digital twin of myself as a driver". Imagine that for each driver we could take a pre-trained FSD solution and then customise (or uptrain) it based on the recorded performance of each particular driver. The recorded performance can be obtained from driving in a simulator (where we can also model extreme scenarios), or it can be obtained from an initial few days of hand driving by the car owner, or even from historical driving data, recorded by the owner’s previous cars. Armed with this data, we can create a "digital driver twin" of the user and let that twin do the driving - which would potentially feel very different for the user.

Now let's look a little closer at the "attitude and appearance" side of our 6-dimensional working model. The model tells us that we should strive to design artificial decision-makers that are more relatable, more blameable and more forgivable. And there is a lot of work already going in this direction. Recent focus on explainability of our AI solutions is aimed primarily at maximising relatability ("if we understand how it thinks, we can feel more comfortable trusting it") and blameability ("if we understand how it came to the wrong conclusion, we can pinpoint the flaw and take appropriate corrective action"). As for forgivability, there is a lot of effort in robot design to make robots look cute (more like children, more forgiveable). But in the domain of software, I'm not aware of any specific design efforts concentrating on forgiveability. This could be an exciting new horizon for UX research and innovation for AI designers. Simple UX "hacks", such as apologising for wrong answers, could make a big emotional difference. Another interesting avenue for increasing forgiveability would be to create more transparency of confidence levels: it's easier to forgive a computer getting it wrong if it had communicated to you in advance that it was only 55% confident and had given you some time to intervene before committing to the decision. We can call this design pattern "smart defaults" and it probably represents a larger family of solutions where agency is not completely transferred from a human to a machine, but instead it is shared between them (i.e. the job of AI is to select the most appropriate default and expose confidence levels and reasoning, the job of the human is to intervene if they deem necessary).

"Smart defaults" and other solutions of the "shared agency" kind have a lot of potential. And they are already widely used: navigation apps, that we have discussed earlier, give one example. YouTube autoplaying the next suggested video is another example. The devil, however, is in the details. Setting smart defaults is very consequential, because lots of people just go with the defaults. So we need to pay very close attention to the inner workings of the model behind. What should the "smart default" be optimised for? And what data should it use to do so? There is a great range of potential answers here. In the case of autoplaying recommended YouTube videos, you can optimise for the longest eventual stay on the platform, you can optimise for the the user's mental health, you can optimise for discovery of new content outside of the user's "bubble", you can also explicitly ask the user what they would like their defaults. This last option is particularly interesting because for a lot of people their conscious, considered decision of what they want the algorithm to show them more of would be different from what they would impulsively click on. In other words, should we be optimising content recommendations for the lizard brain or for the neocortex? The case of autoplaying videos may sound trivial, but exactly the same logic can be applied to much more consequential issues - from self-driving cars to electronic voting systems.

The case of electronic voting is particularly interesting to think through. Imagine how different a democratic political landscape would be if we made it mandatory for all people to vote and went through a one-time procedure of everyone registering their "default" rules and patterns that should be followed unless they intervene. For example, someone can decide to always vote Republican, someone else can choose "random" or “against all” as their default. One could also set the rule to always vote the same as their trusted friend who digs politics, or the same as a certain celebrity they like etc. Once the initial set-up is done, whenever the next election comes, everybody receives a notification with their "default" and they have 3 days to change it if they choose so. Then all the votes are counted and the election decided. We can also imagine a more extreme version of this scenario that would not require an explicit initial definition of the "default" rule by every voter. We could use each voter's available data (age, gender, education, neighbourhood, previous voting records etc.) and use machine learning to predict how they would likely vote in the current election, based on the voting behaviour of other people like them, and set this as their default (that they would, obviously, still be able to change within the 3-day period, thus also providing feedback needed to further refine the prediction system). There are lots of details in there to be worked out, but at the fundamental level all these hypothetical scenarios are 100% technologically possible today and they would lead us to vastly different election outcomes. One can argue that a "smart default"-enabled democracy would not only be easier, but would also be much more fair and representative, compared to what we have today. But thinking about it, many people would still feel uneasy. Let's try to get deeper under the skin of this uneasiness to understand the limits of our working model.

The uneasiness that we feel when considering "smart-default"-enabled, ML-powered voting systems is only partially related to the idea of sharing our agency with an algorithm. The larger part of the discomfort seems to be coming just from thinking about any kind of significant change to such an important (and loaded) system as voting. When confronted with a need to make a really hard, complex, consequential decision, the thing that humans like doing the most - is finding a way to not make a decision at all. This aversion to decision-making can take many forms. Sometimes we simply postpone it again and again. Sometimes we find all sorts of post-rationalized reasons for why it's better to leave things as they are. One of the most common psychological mechanisms we like to employ to avoid making decisions is to pretend that we didn't have the power to make them in the first place. This is why fate, or God's will, is such an appealing idea. If I think I don't have any agency to start with, then I don't need to worry about exercising it, or sharing it.

The ultimate example of this is tossing a coin to make a life-and-death decision. In such an extreme case our model seems to almost get inverted. What can be less competent, less relatable, less blameable than a coin? One possible explanation of this phenomenon is that when the perceived costs of deliberation and mistake reach certain subjective thresholds, human beings tend to enter a special state where all they want to do is just get rid of the need to make a decision themselves, at all costs and as quickly as possible. In this state we are absolutely ok with (and maybe even prefer) outsourcing our agency to the most dumb, unrelatable, random and unexplainable decision-making agents. The mysterious, uncontrollable, chaotic nature of various methods of divination for making important decisions (and coin toss is just the simplest version of divination) are appealing to humans because they signal that some higher power of ultimate competence (nature, god, fate, tradition) is actually making a call for them, which feels extremely liberating.

We love freeing ourselves from the burden of decision-making, as long as the burden is shifted to what we perceive as a power orders of magnitude more competent than we are: be that a God who has a plan, a prophet, a supreme leader, the wisdom of the crowds or the wisdom of tradition. But whatever this entity is, it has to work in mysterious ways. Fate somehow has to be fuzzy. The strangeness, the randomness is, in this case, a virtue - because it is seen as a proof that the competence of the superior decision-maker is of a completely different level. Understanding something and trusting it blindly don't work well together.

The fact that humans actually love shifting agency to “higher powers” in high-stakes scenarios makes one wonder, whether digital dictatorship could actually be very easy for humanity to fall into (if we reach a certain threshold). It seems quite possible that in the longer run, the suspicion with which we view artificial decision makers today could give way to worship and gratitude for delivering us from the computationally intensive and emotionally taxing burden of choosing our own fate. Maybe AGI overlords won’t even have to have their own agendas in order to achieve overwhelming power over us, because we will be all too keen to surrender the power to them voluntarily.

As we can see from this preliminary exploration, three vectors of artificial decision-makers design are emerging:

1. First, we can make our artificial agents completely autonomous, demonstrably super-competent, relatable, forgivable and blameable. Our relationship with such an agent will be in many ways like our relationship with another human who is performing a specialist function for us.
2. Second, we can make our artificial agents only partially autonomous, creating a model of shared agency (e.g. "smart defaults" pattern). Our relationship with such an agent will be similar to our relationship with service dogs (or other smart animals - who can be autonomous and perform a valuable function, but are not considered independent).
3. Third, we can make our artificial agents completely autonomous and completely unrelatable - essentially employing randomness (or rule of precedent or anything else inside a black box) to assume the agency or to "pass it on" further to chance, fate, God or tradition. Our relationship with such an agent will be similar to our relationship with an oracle or a God (who “know better”, but act in mysterious ways that we don’t and shouldn’t understand).

These three vectors can be employed simultaneously in the design of one agent. In this case, the agent would switch modes, depending on the task and situation. For example, an FSD-enabled car can operate in "autonomous-relatable-blameable" (“specialist human mode”) mode when it's doing an automated parking manoeuvre. It can switch to "shared-agency-smart-defaults" mode when the driver is operating it on the highway (“service dog mode”). And in a high-speed unavoidable collision situation where there is no clearly good outcome and no way in which a human can intervene intentionally, FSD may be better off switching to the "autonomous-unrelatable-fate" mode (“god mode”) - effectively using randomness to pass on responsibility to chance (just like humans do in situations where things are completely beyond their control).

This last switch to the "fate" mode may seem like a cop-out. And in a way, it is. But the idea of complete intentionality with unlimited ethical precision can be an even more dangerous utopia than consciously accepting and embracing chance in situations where chance clearly rules. No matter how smart or quick our decision-making agents can be, there will always be a limit to their ability to judge a situation - either because the speed is too high, the sensor precision too low, or rules too fuzzy. Forcing our artificial decision-makers to still make an intentional choice in such a situation (or even simply interpreting their choice as intentional) creates a perception that these agents made a mistake, while operating in the same way as normal. This perception undermines our trust in them in every other situation, where they are perfectly capable of acting intentionally and deciding well on our behalf. There is a massive emotional difference between “letting God decide” and “allowing the faulty machine to make a terrible mistake”, even though in practice the two can be the same. To this end, deliberately and explicitly placing the responsibility into the hands of chance (a.k.a. fate) can be a great strategy. To clearly position the locus of ultimate responsibility in unsolvable situations, we can imagine car manufacturers actually collaborating with the Church (or any other relevant religious authority). One can imagine an odd, but not entirely unrealistic scenario where the Church would certify (or bless) custom-made quantum random-number generators that will take over responsibility in an event where the regular algorithm can’t find any solution that is clearly better than the others. Depending on whether you are a Christian, a Buddhist or a science-adhering atheist you could order a version of your car with “fate” devices certified, certified by different authorities (for example, Catholic church, Dalai Lama’s office or an MIT quantum computing lab).

Where should we draw the line? How do we decide when the mode needs to switch to "random" (a.k.a. fate)? How do we generate true randomness and ensure that it is indeed as random as it can be, without any biases? How does the moment of switching change over time as technology improves? What level of confidence is sufficient for the model to be considered as acting intentionally? How do we think (socially, legally and culturally) about responsibility when the autonomous artificial decision-making agent is making a choice for us in "fate" mode? 

All these questions, and many others need to be considered if we are ever going to intentionally implement "fate-mode" in practice. But it does seem clear, that as the autonomy revolution advances, as technological divination improves, as we construct our digital oracles and they slowly ascend to the position of Gods, they too will need to learn when and how to surrender to chance, how to let go of the feeling of responsibility. And humans will need to re-learn how to be ok with it. Throughout most of history and across vastly different cultures, we were perfectly fine with surrendering to chance (disguised as higher power). Only over the last couple of centuries did we rebel against this idea. This rebellion was so powerful, that when randomness, via Heisenberg's uncertainty principle, made its way back into the fundamentals of physics, most people found it incredibly hard to take. Einstein famously struggled and refused to believe that God could play dice with the Universe. But the more we find out about the world and about ourselves, the more we realise that randomness may indeed be a fundamental feature of the Universe, or at least of how we perceive it. Carl Jung used to point out that as we try to expel Gods and demons from nature, they inevitably re-surface within ourselves. Most likely we will also have to find room for the demons in our own creations that we are starting to surrender our agency to. Because one way or the other, all Gods have to play dice. And as God-builders, we have a responsibility to teach them how to do it well.

---
### **footnotes**

:
     One might ask if these are really burdens to be outsourced or luxuries of conscious life to be enjoyed. This seems to be a question of the point of view and personal ethics (and this question can be reasonably applied all the way back to physical labour as well). What is clear, however, is two things:
(1) Sensing, computing, thinking, directing attention, creatively expressing oneself and making decisions all take up considerable amounts of mental and physical energy, which means that from the standpoint of evolution they are indeed expenditures to potentially be optimized. (2) Today people are, in fact, happily outsourcing all of these to a significant degree and even paying for it. Which means that lots of people would rather avoid doing it, when given opportunity, hence, by definition, for many people it's indeed a burden.

:
     This may serve as a potentially useful broad definition for technology.

:
     It's really interesting to think about how these "common sense" reactions are not fixed and universal, but in fact very dependent on the time, culture, demographics etc. The ones listed here are my over-generalizations and abstractions of the modern "western", urban millennial mind in 2022, but clearly the reactions would be very different for an elderly Japanese painter at the end of the 20ths century, for example.

:

     This is not dissimilar to the current situation with lawyers. Whoever can afford a better one is more likely to win a court battle - which, one could argue - is ethically problematic.

:
     There are obviously good evolutionary reasons for why we would be wired this way.

:

     General positive emotional predisposition (bias) influences all other more special attitudes significantly. And readiness to outsource responsibility is no exception.

:

     This reasoning is obviously flawed: even if you make decisions in a similar fashion to me, it doesn't mean that your decisions will be in my interest, because we are in two different positions. Still, most people fall for it and assume that if the decision-maker is similar to them, the outcomes will be desirable for them.

:

     Because we all have powerful psychological mechanisms built-in for forgiving ourselves and for distorting reality to make us feel right or justified or at least coherent, even when we are not. These mechanisms seem to partially transfer to people who we perceive as being similar to us.

:

     While we mentioned earlier that competence doesn't play as big a role as you'd think it should from a purely rational perspective, it still obviously matters to some degree and shouldn't be ignored.

:

     As we will see shortly, blameshifting (or, to be more precise, blame locating) is one of the key strategic considerations when designing an autonomous or semi-autonomous decision-making system.

:

     In fact comparing FSD with average drivers won’t be enough, because average human drivers consider themselves to be much better than average. So what FSD demonstrations need to focus on is how terrible a driver you are personally - thus helping you understand why it’s better for you individually to surrender responsibility to a superior autonomous agent.

:
     It’s interesting to note that as your reliance on the algorithm grows, the types of situations where you would consider intervention to be necessary would, probably, go down.

:
     "Defaults" don't have to be set once and for all, but can be updated as the person's attitudes change. And "defaults" don't have to look like simple rules. They can be represented by an ML model that would predict your next vote, based on your previous voting history etc.

:
     This outsourcing of responsibility to higher powers makes things easier not only personally, but also socially. Others can't blame you if they know that it's not your call. There is a great potential for designing artificial decision-making systems that exploit this idea. For example, one could imagine how liberating it would feel to have an AI personal boss who is in charge of your calendar and what you should focus on. When you initially set your boss up you can instruct it on the principles and rules and what to optimise for, but then, once the initial set-up is done - the boss is in charge and you do what the boss says. So when friends ask you out, or when an interesting but not essential new project comes up - you can consult the boss, get a "no" and tell people that you'd love to do it, but sadly your boss doesn't allow you to.

:
     Or, in a different, but equally valid interpretation, -  higher power, disguised as chance.


=====================================
Three Laws of Consumerism
=====================================

1. Everything that you buy regularly today, you will keep buying indefinitely, unless an external force acts upon you.

2. Your likelihood of buying more things is proportional to your subjective unhappiness times your disposable income.

3. Whatever you think you own also owns you in an equal way.


=====================================
The Function of Art is to save the Artist
=====================================

#### _Conflict, creativity and the function of art in times of crisis_

> _"While art cannot, as we wish it could, save us from wars, privation, envy, greed, old age or death, it can revitalize us amidst it all. [...] Writing is survival. [...] Not to write, for many of us, is to die"._
> 
> Ray Bradbudy, "Zen and the Art of Writing"

Does conflict catalyze creativity? In some way this question is so cruel, that it feels uncomfortable to even type it out. And yet, as a creative community, we must ask it. We need to understand whether suffering and self-expression are truly connected. We need to get a sense of whether disasters really mobilize creative potential. We need to find out if the most potent sources of our inspiration have to be as dark as they often seem to be. And through all of it we need to identify the role that creativity can play in the times of crisis.

The view that trauma and talent are inherently linked is so prominent in the public consciousness that it is rarely questioned. We've all read the biographies of troubled geniuses (and found some solace in not being like them, after all). Intuitively, it makes sense: there has to be a price for a beautiful mind. And yet, [carefully controlled studies](http://dx.doi.org/10.1016/j.jpsychires.2012.09.010) don't seem to reveal a strong link between creativity and mental illness or suffering in general. In other words, you don't have to be crazy or terribly traumatized in order to produce great creative work. Creativity doesn't require a dark side.

What studies do show, however, is that if you are creative, then you are likely to have a heightened awareness, an above average sensitivity to what's going on around you. Experiments have proved that [more creative people tend to include a wider range and a larger quantity of stimuli in their mental processes, compared to less creative people](https://doi.org/10.3758/s13415-013-0210-6). So it seems that the relationship between feeling bad and feeling inspired is a correlation, not a causation. Suffering and creative self-expression are two byproducts of the same underlying cause: a state of mind that is less willing to turn off the outside world to only concentrate on a narrow set of things in front of them. Creativity, in this sense, is a result of the failure to activate the blinders, the usual narrow-mindedness of the everyday.

Hikaru Takeuchi and his colleagues showed [in their 2011 study](https://pubmed.ncbi.nlm.nih.gov/21111830/) that when engaging in an effortful working memory task, highly creative people had difficulty suppressing the Precuneus, an important brain region that has been linked to self-consciousness and retrieval of personal memories. How is this helpful to creativity? According to the researchers, "Such an inability to suppress seemingly unnecessary cognitive activity may actually help creative subjects in associating two ideas represented in different networks". In other words, if you don't over-concentrate, if you remain open, if you are less aware of what you are trying to do and more aware of what's going on - you start getting unusual ideas, the kinds of ideas that others tend to miss. This can also explain why more creative people tend to "daydream" more often: their attention is less strongly attached to the immediate reality of life.

Most people ignore most of the world most of the time - and focus on the very narrow task at hand. Evolutionary speaking, it's necessary to do so for survival, because there is simply not enough mental processing power to pay attention to all the stimuli that we are receiving every second. But in highly creative people this selectivity of attention is reduced, allowing new connections to be seen and explored. By the same token - it makes them more prone to anxiety, depression etc.

This insight into the source of creativity can help us make sense of how individual creative people and the creative community as a whole react to tragedies, such as the one unfolding in Ukraine today. When on February 24th 2022 Russia's supreme leader Putin decided to attack Ukraine, his country's closest relative, he completely broke down the mental models of millions of people on both sides of the border. The world that they had known was instantly destroyed, and a new hostile, unpredictable, previously unimaginable world flooded all their senses. And even if they were lucky not to have their homes struck by missiles, they couldn't help having their minds struck by the terrible new reality that didn't match their mental models. Reality became harder to ignore. And people had to react to it in one of the following three ways:

1. The silent majority managed to put their Precuneuses into overdrive, double down on tunnel vision, fully extend their blinders and maximize their reality distortion fields - all in a desperate (and mostly) subconscious effort to retain their identity. No matter how much the new reality was screaming at them, they firmly decided to continue ignoring it, focusing on the immediate, practical tasks of life in front of them. For the people in this group who were creative the task was particularly difficult. And as they had to shut the world out, creative activities became increasingly difficult. You can't paint with your eyes shut, or compose music with your ears plugged.
2. The brave minority met the situation with their eyes and minds open. They saw the new reality in all its terror and sprang into action, their resourcefulness and pragmatic [creativity amplified by the shared stress](https://pubmed.ncbi.nlm.nih.gov/33192916/). They realized that words, pictures or songs could not stop bullets or shelter people. So they channeled their creativity and compassion into action: people in Ukraine started self-organizing evacuations and enlisting in territorial defense, people in Poland, Czech Republic and other countries opening their homes to millions of refugees and coordinating help efforts, people in Russia protesting despite the danger of prosecution. Traditionally defined creativity for these people took a back seat, because they managed to retain a sense of agency and resourcefulness needed to act.
3. But beyond these two obvious responses, there was a third, less common one. It was a response of those highly sensitive individuals who, even in normal circumstances, often find the world overwhelming, because they filter out much less of it than most people do. This sensitive minority, overwhelmed by the world that crashed in their faces, had only two options: either break down completely (as many sadly did), or find a way to process this assault into art. Creativity became for them not a means of self-expression, but a means of self-preservation. Creativity as a coping mechanism. Creativity as a way of processing the onslaught of life and turning it into something beautiful. Creativity as a way to stay alive, a way to re-assert one's own individual humanity, and our shared collective humanity, in the face of such an aggressively divided and broken world.

This third response, I believe, is at the heart of the creative resurgence that tends to accompany tragedy. Whether this creative resurgence leaves a lasting impact, whether it leads to new masterpieces or not - doesn't really matter. Because, in the times of catastrophe, the first, and, possibly, the most important function of art - is to save the artist.


=====================================
108 thoughts for strategists
=====================================

> For Ecem, Christian and The Circuit, 020220104

1. Saying what you actually want to say is the hardest thing in the world. [You can't ever learn it completely](https://photos.app.goo.gl/7rRUsVmuuyymu5JD8). But it gets a little easier with practice. Years of practice.
2. Lists are useful. And they don't have to be strictly [MECE](https://en.wikipedia.org/wiki/MECE_principle) in order to be useful.
3. In games, conversations and careers, playing to win is not the only possible strategy, and winning is not the only possible goal. Sometimes [the goal of play](https://en.wikipedia.org/wiki/Finite_and_Infinite_Games) is to keep playing.
4. Intellectual pragmatism works. For an argument or a distinction to be useful (productive, fruitful) it is not necessary for it to be true (logically bullet-proof). In fact some [very smart people have argued](https://youtu.be/wmcCBmxrEwo?t=1945) that the best sign (and definition) of something being "true" is not whether it is strictly logically consistent with the axioms and the evidence, but whether it "bears fruit".
5. Words [suck](https://photos.app.goo.gl/teiixdjtaQtL7auc9). Avoid them as much as possible and when impossible - use as few as you can.
6. Stickiness of your strategy is inversely correlated with the number of words you need to express it. [Only a dumbass takes more than 3 minutes to tell a story.](https://youtu.be/8wBOUJ5Mbrk)
7. The effort you put into [zipping ideas](https://en.wikipedia.org/wiki/Exformation) into as few words as possible shows itself when these ideas get unzipped by other people. Well-zipped ideas grow like wildfire in people's minds, using the fiber of their own thoughts and experiences as fuel for their growth and evolution.
8. What you are asking people to do and what you are actually making them do [don't have to be the same thing](https://www.youtube.com/watch?v=dtFroEJN1nI). The most effective product is often a byproduct. This has implications for how we can design and affect complex human systems.
9. For most problems worth thinking about, solution space is not uniform. Often the more wrong something is, the closer it is to being absolutely right.
10. All rationality is post-rationalization. [The only difference is the distance](https://www.youtube.com/watch?v=vv_e99qbJ4U).
11. The fact that you are not a creative doesn't mean that you don't have to think up ideas.
12. The fact that you are not a business lead, doesn't mean you don't have to try and understand clients as human beings.
13. Reading out loud what you wrote is a painful but effective way of finding out how much sense it makes.
14. Randomness is a [powerful thing](https://www.youtube.com/watch?v=E3xLEo1z8RQ). One can see any creative process as ultimately [combinatorial](https://www.amazon.com/Technique-Producing-Advertising-Classics-Library/dp/0071410945/).
15. Best briefs are [telescopes](https://www.telescopictext.org/text/KPx0nlXlKTciC). In fact a lot of good writing is telescopic, or "telescopable". It can collapse into just a few words, or expand into a book.
16. Originality is not just overrated, it's [fictional](https://youtu.be/77mbz2HbY5Y).
17. Work is like gas. It expands to fill as much space and time as it can. But the more of your life you allow it to occupy, the less dense and noticeable each particle of work becomes. So if you want to do good, noticeable work - condense it and create space around it.
18. Nobody is uninteresting and [nothing is boring](https://www.goodreads.com/quotes/7177-if-something-is-boring-after-two-minutes-try-it-for), if you look carefully enough and long enough.
19. Communication is not about trying to convey ideas. It's about throwing symbols into each other's minds and hoping the ripples get in resonance.
20. Interpretations [matter](https://youtu.be/Y0Oa4Lp5fLE?t=900). Beware of the cultural, moral, intellectual and other biases that make you read things into a situation. You can’t get rid of those biases completely, but being aware of them will help.
21. [Writing letters](https://www.youtube.com/watch?v=ltgi2nv9CHk&list=PLpO_X3dhaqUL1nbeapfa3mf3KCs1DwgOH&index=3) well helps you think, feel and make friends.
22. Embrace the relativity of feeling. Human capacity for conscious suffering is limited. Most often it's limited to the single most acute problem at a time. If you are running for your life, you usually don't feel that toothache. But in other circumstances that same toothache can drive you insane. This is also true for happiness.
23. Strategy is lockpicking. The more different tools you have - the better your chances of success are, as long as you don't get too attached to any individual tool. Also sometimes you have to find another door. And occasionally use dynamite.
24. If you don't understand who is paying what to whom for what, you don't understand anything. Always start by digging your client's business model.
25. Helping clients make up their mind about what they want is not an annoyance, [it's your job](https://www.amazon.com/Design-Essays-Computer-Scientist-ebook-dp-B003DKG5H6/dp/B003DKG5H6/). If they knew exactly what they wanted they wouldn't need a strategist.
26. The quality of communication can be judged by the quality of silence that it leaves behind.
27. Problem solving is a spiral [process](https://oneslide.org/2021/06/04/hunch-hack-design-spiral/).
28. Often the best way to solve a problem is to [dissolve the problem](https://www.youtube.com/watch?v=EbLh7rZ3rhU), i.e. redesign the system in such a way that the problem disappears.
29. Not everybody thinks the way you do. That's an advantage. For everyone.
30. Your job is not to be right, but to be more understanding and [less wrong](https://www.lesswrong.com/about).
31. Your brief is only as good as the work it inspires. It’s easy to sell a strategy that “makes sense”, but doesn’t actually lead to anything of [quality](https://en.wikipedia.org/wiki/Zen_and_the_Art_of_Motorcycle_Maintenance).
32. Thinking is overrated, listening is underused. As much fun as pure thinking is, most of the time, simply reading the customer complaints or talking to people in the shops will tell you exactly what needs to be done.
33. When ideas are flowing - get out of the way.
34. KPIs are [very dangerous](https://story.fund/post/105230705282/cobra-effect), because when a measure becomes a target it [ceases](https://en.wikipedia.org/wiki/Goodhart%27s_law) to be a good measure.
35. The easiest way to help people change their minds is helping them follow their own logic all the way through.
36. For a team to function well, it's important to create and maintain shared latticework to hang ideas on.
37. The difference between a bug and a feature can be a difference of perception, or a difference of context.
38. The best ads don't look like ads.
39. Don’t [confuse the numbers you can measure with the numbers you need to measure](https://youtu.be/4N3TNANxsvU?t=1049). 
40. Good ideas leave a cultural trail. By following it backwards you can understand them better.
41. There are two reasons effective presentations typically have as many words per page as toddler books: (1) the attention spans of the intended audiences are similar and (2) slogan thinking is structurally [required](https://www.edwardtufte.com/bboard/q-and-a-fetch-msg?msg_id=00025o) by the shape of modern corporate bureaucracy and modern culture at large.
42. There is a difference between [a painkiller, a vitamin and a cure](https://www.oreilly.com/library/view/the-agile-startup/9781118744482/xhtml/Chapter02_9.html). Selling vitamins is the hardest. Selling painkillers is usually easier than selling cures. Selling diseases is the most profitable in the long run.
43. When looking for the right articulation, always prioritize punch over precision. Precision without punch won't even register. In any case, universal precision is unattainable, as it is relative by nature. When something sounds precise to you, remember that you are not the intended audience of your message.
44. Framing conversations is more powerful than having conversations.
45. [Silence is a commons](https://docs.google.com/document/d/1urCuF-s_HBfoN2CPwoFOHffkssZN7BNWWmGHOYd1e6c/edit?usp=sharing). Silence is a necessity. Silence is a power. Use it. Speak slowly and leave space for others to think.
46. Making sense is always done backwards. There is no shame in it.
47. If you want to understand how good ideas work, deconstruct the ones you like. In more than one way.
48. Before engaging in a conversation, it's good to intentionally set the kind and level of argumentation you are going to try and use. Because convincing [a prickly person with gooey arguments is very hard](https://www.youtube.com/watch?v=XXi_ldNRNtM). Unless you manage to reframe their state of mind, but that’s a whole new level.
49. Not everything needs to be [organized in a tree-like way](http://en.bp.ntu.edu.tw/wp-content/uploads/2011/12/06-Alexander-A-city-is-not-a-tree.pdf). Semilattices often work better (even if they are harder to think about).
50. Time can be a friend or an enemy, depending on how you use it.
51. There is a difference between the moon and [the finger](https://www.youtube.com/watch?v=4O9o4CKTGzQ&t=56s) pointing at the moon. The food and the menu card. The music and the notation. The slide deck and the idea.
52. If you are lucky, people will remember one thing from your presentation. It's a good idea to be intentional about which one.
53. "I don't know" is a plausible answer. Often the best one. And certainly [not the one to be afraid of](https://www.youtube.com/watch?v=E1RqTP5Unr4).
54. Top quality thinking is a performance sport. One can only do a couple of hours of real thinking per day. On a good day. The rest is for refilling your think tank, rather than trying to squeeze more out of the empty one.
55. Most people are [curiously afraid](https://vimeo.com/110171277) most of the time, and barely holding it together. Be kind.
56. There is a difference between effectiveness and efficiency. You can be very efficient, while making very little impact.
57. Thinking straight is always preferable, but not always doable. Sometimes roundabout thinking is unavoidable. Be grateful for whatever path gets you there. But then once you are there, make the path as straight as possible, so that it's easier for others to follow.
58. Sometimes you need to write in order to think. Sometimes you need to write in order to communicate. [Both ways are useful, but beware of confusing them](https://www.youtube.com/watch?v=vtIzMaLkCaM) and mixing them.
59. One of your most important jobs as a strategist is to [assist the birth](https://oneslide.org/2020/09/03/creative-midwifery/) of good ideas. You are a midwife. You need to adapt to the needs of your patients.
60. Ideas are born fragile. Most die in infancy. Protect them fiercely, until they are ready to be tested fiercely.
61. People value what they pay for. Charging for your work is a way of making sure that people actually use it.
62. Most things you really need to understand were [taught to you in kindergarten](https://web.media.mit.edu/~mres/papers/kindergarten-learning-approach.pdf), without you noticing - which is the best way.
63. Insight is not a noun. It's a verb. It's not about what it is, it's about what it does to the creative people who receive it. A good insight may look like a fact, a story, a picture, a question, a poem or a pint of beer. If it incites ideas, if it makes creative people say: "Wow, yes! I can make something out of it", then it's an insight.
64. [We think much less than we think we think](https://www.amazon.com/Thinking-Fast-Slow-Daniel-Kahneman-ebook/dp/B00555X8OA/). And we feel much more than we feel we feel.
65. Human experience is mostly focused on [avoiding as much experiencing as possible](https://openairphilosophy.org/wp-content/uploads/2019/06/OAP_Zapffe_Last_Messiah.pdf). Therefore you can often learn more about people by looking at what they are trying to avoid and how they are doing it vs. looking at what they are trying to achieve.
66. Good thinking in a few ugly bullet points is worth 100 times more than sloppy thinking in 100 well-designed slides.
67. [Inventing on principle](https://vimeo.com/38272912) is much easier than inventing on luck. But beware of getting too attached to just one principle or you may fall into the danger of all problems looking like nails.
68. A [good story](https://story.fund/) is worth a thousand slides.
69. Your clients and colleagues are your first "target audience". Make them laugh. Make them cry. Make them wonder. Then they will buy into your thinking.
70. Instincts and intuitions are not always right, but they always win. So if you need to build a counterintuitive argument, then build it in a way that would get your audiences' intuitions pointing in the right direction.
71. Googling is a two level skill. Level 1 is knowing how to quickly find what you are looking for. Level 2 is knowing how to quickly find what you are not looking for, but actually need. Most people are at level 0.
72. Meaning is not only lost in translation. It is also born in translation. Learn to speak more languages. Hindi. Latin. Sign. Music. Python.
73. Where language ends, thought begins. That's why speaking more than one language is a massive advantage. The space between two languages offers treasures for those who are brave enough to venture there.
74. There is a difference between the [short Now and the long Now](https://longnow.org/essays/big-here-long-now/). Also the small Here and the Big Here.
75. In thinking and in conversation, Coordinates, Horizons and Frames matter much more than the actual data points and arguments.
76. What's happening in the world is mostly [fuss and misery](https://64.media.tumblr.com/bc0f22eba26efd2d183f16af1bd7743c/tumblr_mmcg7kaDh41rz9bujo1_400.jpg). One way to cope is pretending you are an [anthropologist from Mars](https://en.wikipedia.org/wiki/Oliver_Sacks) in disguise - observing, empathizing, understanding these wonderfully odd and self-defeating Earthlings.
77. A good brief is a window. It allows you to do people-watching from a whole new angle.
78. None of us is as dumb as all of us. Collective decision-making is most often a path to hell. Occasionally and slowly incredible decisions arise from the crowds, but this is (as of today) unreliable, especially in time-constrained situations.
79. For numbers above 2, there is an inverse correlation between the number of authors and the quality of writing.
80. In a finite world with multiple players who are bound to meet each other again - [cooperation](https://en.wikipedia.org/wiki/The_Evolution_of_Cooperation) is the most powerful strategy.
81. [You don't necessarily have to agree with everything you say](https://en.wikipedia.org/wiki/Marshall_McLuhan). Sometimes saying the things you don't agree with is necessary to move the conversation forward. It can also be fun and liberating.
82. When somebody asks "why", the first thing to understand is whether they are looking for causes or intentions.
83. When somebody [asks "why" five times in a row](https://youtu.be/P1ww1IXRfTA?t=946), that is a sure sign they haven't thought through what they are looking for.
84. The most important thing in a conversation is understanding what each participant takes for granted.
85. There is a difference between correlation and causation. And strict causation is impossible to establish, so the best you can do is show that correlation is persistent, reliable and predictive enough for you to be able to use it.
86. There is a difference between making a graph to explore and making a graph to impress. Things like labels, log scales and time periods can have massive effects in both use cases.
87. [Presupposing capability is a perfectly viable way of creating it](https://www.youtube.com/watch?v=loay2imHq5E). Works for colleagues, partners... and yourself. Send yourself on impossible missions and be surprised.
88. Habits are the glue that holds people together. [New behaviors are only possible if motivation, ability and trigger can outweigh a habit](https://oneslide.org/2020/06/24/behavior-change-math/).
89. [Many people need to move to think](https://youtu.be/iG9CE55wbtY?t=908). Others may need to doodle. Or walk. Or play the violin. Try things and find out what works for you. The chance that you do your best thinking in front of a screen with multiple tabs open is very low.
90. [Doubt and belief](https://essays.georgestrakhov.com/to-doubt-or-not-to-doubt/) are the primary ingredients of a good thought process.
91. A certain amount of belief is absolutely necessary for a strategy to work. Too much belief can be deadly. Same is true about doubt.
92. Those who are lost have to [accept they are lost](https://www.amazon.com/Deep-Survival-Who-Lives-Dies/dp/0393326152), before they can find a way.
93. [Thoroughly conscious ignorance](https://youtu.be/nq0_zGzSc8g) is a prelude to every advance in thinking.
94. For interesting communication participants need to have a [certain amount of shared reality](https://www.youtube.com/watch?v=UyyjU8fzEYU). Not too little. But not too much either. When thinking about your message - start by mapping what you have in common with the intended audience of your message and what differences you have.
95. Fancy words can't be trusted. Too often they are used to mask the lack of understanding or substance.
96. In delivery, pace is everything. Slow down.
97. If you can’t explain it to a 10 year-old, you don’t understand it.
98. Instruction is massively helpful for learning. If you are the teacher. If you are the student, instruction is often the biggest obstacle. If you want to learn something, teach it.
99. There is no such thing as "[types of problems](https://youtu.be/EbLh7rZ3rhU?t=2239)". Only angles for looking at them.
100. Every idea needs a [slogan](https://www.youtube.com/watch?v=Unzc731iCUY) if it is destined to stick.
101. Split your time wisely between the Ivory Tower in Cambridge and the [Bazaar](https://en.wikipedia.org/wiki/The_Cathedral_and_the_Bazaar) in Istanbul. Getting immersed in the messiness of life is essential for real understanding. The easiest way of having interesting thoughts is living an interesting life.
102. Overexplaining an idea is a sure way of suffocating it. Leave some air for your audience to imagine how it could be.
103. It's hard to fall in love with an idea if it is presented to you completely static and naked. Dress her up and make her dance. This way everyone can imagine (in their own way) how gorgeous she would be if they could hold her close.
104. You [don't come up with ideas](https://www.youtube.com/watch?v=mFsBaa_MEzM). You catch them in the wild and then try to keep them alive in captivity. The best ideas eventually escape back into the wild.
105. Your [muse is not your horse](https://youtu.be/hpcvUiLnbYQ?t=14). But she still needs feeding. And shitting. And resting.
106. The best thinking feels natural. Same is true [about technology](https://essays.georgestrakhov.com/elvish/).
107. There is pleasure in both [finding things out](https://www.amazon.com/Pleasure-Finding-Things-Out-Richard-ebook/dp/B06XBV7Y3T/) and making things up. The two can feed each other, if you mix them gently.
108. You need to carefully manage your creative metabolism. The quality of ideas you consume determines the quality of ideas you create. With a 20% discount. If you only watch other people's ads, you will only ever create ads that are 80% as good as theirs. If you only read other people's learnings, you will only ever possess diluted versions of their imperfect understanding. Now, go listen to some [Arvo Pärt](https://www.youtube.com/watch?v=TzIZPZN5K60) and [read something real](https://docs.google.com/spreadsheets/d/19ArmG8_Dv7Y9aMLbHTIMAH4hQkGzoHErZcXF_eGsjnY/edit?usp=sharing).


=====================================
To Doubt or Not to Doubt?
=====================================

> For June, 020220103

I am inviting you to take a point of view at the world that I find both useful and amusing. It goes like this:

1. The two most powerful forces in human history are (A) Belief and (B) Doubt.

2. Belief is a force that allows us to create something out of nothing. It allows us to disregard the plastic messiness of reality and act intentionally to make it what we want it to be. Cathedrals and empires have been built based on powerful Beliefs. Doubt is the opposite and equally powerful force that allows us to deconstruct something back into nothing, so that it can be put back together again, into something that works better. Airplanes fly on Doubt. Diseases are cured by Doubt. Belief helps us create maps, so that we know where to go. Doubt helps us find out that we are lost when reality doesn't match our expectations. Belief is the force for making things up. Doubt is the force for finding things out.    

3. As long as an intelligent organism (be it a human or a company) is alive, it has to be utilizing the power of both Belief and Doubt. In other words, as long as you keep thinking, you have to keep questioning some things and taking other things for granted.

4. Both forces grow in power exponentially when they are shared. When a Belief is shared between people, it is multiplied both in its strength within a person, and in its capacity for translation into action. When Doubt is shared between people, it is also multiplied both in its strength within a person and in its capacity for productive deconstruction of beliefs (by "productive" here I mean ultimately helpful in moving your thinking process further in its intended direction).
       
5. One can look at the whole history of human civilizations through the lens of the complex metabolism between various shared Beliefs and Doubts. One can also look at a personal history through the same lens. Give it a try, it's a lot of fun to think about what you took for granted versus what you doubted intensely when you were, let's say, 14 years old.
   
6. Beliefs and Doubts are so central to how we operate that we are (normally) not very intentional about when and how we use them. In fact we are not aware of this at all most of the time. This is true of all vital functions - we are not normally very intentional about our own breathing patterns, for example. However when we do put some effort into trying to be more aware and intentional about our autonomic vital functions (such as breathing, or thinking) - wonderful things happen. For one, we discover the boundaries of how far our awareness and intentionality can actually go.
   
7. This letter is an invitation for you to try being more aware and intentional about when, what and how you Doubt. I find a lot of value (and joy) in the practice of trying to doubt productively (i.e. doubting in a way that in the end facilitates the creation of more powerful shared beliefs than the ones that it helped deconstruct). I call this practice Doubtery. It is most definitely an art, possibly a martial one.

8. The basic move of Doubtery is a question. Effective questions, questions worth asking, applied skillfully at the right moments, hold immense power. By the same token, ineffective questions or the ones applied wrongly can be really destructive not only to the beliefs they are aimed at, but also to the very fabric of thinking, which is usually not helpful (unless it is your intention to interrogate the very fabric of thinking). Thinking together can be especially fragile and even more of a dancing act than thinking alone. Which is another reason for you to practice Doubtery even if you are primarily in the business of creating shared beliefs, not interrogating them.      

9. I'm inviting you to create (and maintain) a list of your favorite Doubtery moves. Both the moves that you've mastered and the moves that you are yet to learn. The moves that you apply without thinking and the moves that you don't yet know how to apply well. I am hoping that through practicing Doubtery in ever more aware and intentional ways, you will discover your own moves, techniques and practices that work best for you, because (similar to any other martial art) effective application of Doubtery requires first and foremost self-awareness.

10. Last but not least, I am also hoping that the negative space around this practice will also invite you to try being more aware and choiceful about when, what and how you Believe. To doubt or not to doubt - that is always the first and most critical question. Just like one can't breath out without breathing in first, one can't doubt productively without believing something first. The will and skill to believe are as critical to a healthy thinking process as are the will and skill to doubt. I hope that practicing the art of Doubtery will also help you choose to believe more intentionally and more often (as and when it makes more sense for the direction of your mind's travel).  

Now, it's time for you to practice some Doubtery and tear this whole argument apart.


=====================================
Towards a more Elvish vision of technology
=====================================

> For Gavin, 020210703

There are at least two kinds of magic. These two kinds may not always be distinct in their appearance, but are very different in their underlying motives and their ultimate outcomes.

The first kind of magic, which we shall call “Human”, is driven by the desire to extend one's power over the world, while simultaneously minimizing one's dependence on the world. The ultimate outcome of this pursuit is world-amputation: the destruction of the world on which one no longer depends.

The second kind of magic, which we shall call “Elvish”, is driven by the desire to extend one's understanding of the world, while simultaneously minimizing one's intentional interference with the ways of the world. The ultimate outcome of this pursuit is self-amputation: the dissolution of the self and becoming one with the world.

The “Human” magic of conquest and control has had many names over the centuries, but today we call it "technology", and we use it quite successfully both as a shield from nature and as an axe with which we can cut, shape and subdue it to our will. In the 1950s J.R.R. Tolkien used the words "Magic" and "Machine" interchangeably, as long as the underlying motive was "the desire for Power, for making the will more quickly effective". A decade later, Arthur C. Clarke famously declared that "sufficiently advanced technology is indistinguishable from magic", in effect, cementing a program for the coming generations of engineers: building machinery for making human will so quickly effective that the constraints and boundaries of the natural world cease to exist (which inevitably leads to the natural world itself gradually disappearing, as nothing can exist without its boundaries and its friction). The strategies for removing the friction of the world are many — from robotic extensions to virtual-reality amputations — but the side-effects are often the same: if we want to minimize the world’s friction, we also have to minimize its power. 

Both kinds of magic are partially known to humans, but for most of known history, we've been primarily obsessed with the first kind. Since the beginning of time we have been endlessly looking for ways to conquer, control, or separate ourselves from what was outside of our power. In fact, one can view the whole history of human civilization as a story of gradual increase in our insulation from nature: technology is humanity’s fur.

Gaining power over fire was the first mythical act of this kind of magic (thus Prometheus is its archetypal hero). Since then we have made a lot of progress: we have learned to control light, pain, heat, food, energy and information well enough to achieve a significant degree of independence from our environment.

The final uncontrollable entity that needs conquering is death. Mastering death has been the ultimate quest for many generations of the practitioners of “Human” magic, from the first Emperor of China, who died as a result of drinking what his alchemists thought was the elixir of immortality, to the current researchers looking for ways to break through the Hayflick limit.

Our obsession with putting others to death could be understood as a subconscious coping mechanism that, in a perverted way, helps us compensate for the fact that we can't yet control our own death fully. After all, if we can't master death in a way that would forbid it from coming to ourselves, we can at least feel like we are its masters by making it come to others when we bid it so. Such is the magic of the mortals: searching for power that can conquer death, and, almost inevitably, creating more death along the way.

But what if death was not a problem? What technologies would immortals be interested in?

One natural hypothesis is that they, like us, would point their magic towards the only thing unavailable to them: in our case, immortality in their case,- death, and in either case, the complete dissolution of the Self. So if the final destination of the mortals is suicide by conquest, then the final destination of the immortals is suicide by understanding. Deep understanding always implies self-amputation, either by way of making oneself “an outside observer” (the scientific way) or by realizing complete interconnectedness and oneness with the world and inseparability of the Self from it (the spiritual way). When the dog discovers its Buddha’s nature, it stops being a dog.

And while Tolkien’s Elves are not completely enlightened in the Buddhist sense, still, understanding, beauty and harmony are what they seek the most. They are in love with the world and its music, and so, through knowledge, observation, crafts and arts, they are trying their best to help the world grow and evolve… for which their best strategy is getting out of the way of this evolution. The “fading” of the Elves is the feeding of the world. When the music's over, turn out the lights.

So, if for Humans “sufficiently advanced technology is indistinguishable from magic”, then for the Elves, “sufficiently advanced technology is indistinguishable from nature” (for them the more advanced magic is the one that allows nature to be most uninhibited). This, however, is not biomimicry. The Elves do not imitate nature, they let nature do its magic and try to help it as best as they can. Allowing nature to solve a problem is extremely effective and efficient. The only downside is that it usually takes a lot of time - precisely the thing that the immortals have plenty of.

Give it enough time, and every problem will solve itself. This may not always be a practicable strategy for a mortal, but it doesn’t mean that we have to always go to the other extreme. Solving a problem instantly requires an infinite amount of energy, and an equally unthinkable number of side-effects. Fixating on solving the problem quickly and by our own action (rather than pointed inaction) results in our potential solution space being reduced to very few options.

One can think of “Human” and “Elvish” kinds of technology as two opposite evolutionary strategies for a conscious being: the first one minimizes surprise (or, more precisely, free energy) by adapting the world to one’s predictions and desires of the world; the second one minimizes surprise by adapting one’s predictions and desires to the way the world is. The second (“Elvish”) strategy may sound passive, and in that way inferior or defeatist, but it isn’t. Allowing the problem to solve itself is frowned upon and not even considered to be “real solving” by most humans: “And what did you do? What was your role? How did you save the world?” Humans (at least, us in the modern West) are obsessed with their own sense of agency. Everybody wants to save the world these days, but nobody seems to be interested in doing it slowly, by allowing the world to save itself, even if it is the most effective and efficient solution. Hesse’s Siddhartha isn’t joking when he is saying that all he can do is “think, wait and fast” and it is quite enough to accomplish almost anything. Slow is good, not just because it takes less effort (often it doesn’t), but because it achieves what it was meant to achieve.

At the beginning we said that the ultimate outcomes of “Human” and “Elvish” ways are different. But how different? And on what kind of time horizon? If practiced to their extreme, both may actually lead to fairly similar final outcomes. Look, for example, at a common utopian vision of the future: we have evolved into a disembodied, interconnected higher consciousness, roaming the vastness of space, sustaining itself on nothing but pure energy (light). This could be seen as the ultimate destination for the “Human” technology, as it represents the most effective use of resources to fight off mortality (and all our Earth’s resources are probably by then used up and even the Sun is trapped in a Dyson sphere). However, a very similar picture could be the ultimate destination for the “Elvish” technology: we have cut ourselves off from the Earth and so it is left alone to flourish. We no longer interfere, and cease to exist individually, while collectively possessing ultimate understanding.

The big question, then, is not whether we should keep practicing “Human” magic or switch to “Elvish”. The big question is whether we have to choose outright between the “Human” and the “Elvish”, between perpetrating death and practicing death, between suicide by world-amputation and suicide by self-amputation. Perhaps a middle path is possible (for some of us in Middle Earth). In other words, can we develop and practice our magic in ways that would destroy neither the world, nor us? Hope mandates a positive answer. Because if these two extremes are our only options, then the choice is but a choice of the road, for the destination is the same, and not entirely pleasant (at least, not to my taste).

When hope mandates possibility, imagination springs to action to define this possibility in concrete terms. So let us now shift our mode of discussion from poetic to pragmatic and imagine what we would do differently if we rebalanced our practice of magic from almost exclusively “Human”, to a more even mixture between “Human” and “Elvish”. This shift would give birth to a new mode and a new method of problem solving. Three high-level features of such a method come to mind:

1. **Outcome is not a state, but a process**.
As mortals, we are used to time-bound problems and finite solutions. For example, the problem is that you have nowhere to live starting next month, the task is to build a house, the outcome of the work is that the house is built. Now you have somewhere to live. Job done.  But in a more “Elvish” paradigm, however, the moment the house is built is not the end. How will this building grow? What other purposes will it satisfy in the future? When and how will it be deconstructed? What would happen with the earth on which it is built? All these “Elvish” questions (and many more) can help develop a deeper understanding of both the problem and the solution spaces, and reframe (re-timeframe) both. The moment when the house is needed is not the end, but it is also not the beginning. How did the need for the house emerge? Where did the materials for it come from? When even starting to consider all this, our “Human” eyes start to glaze over: after all, we are looking for a place to live, not for a philosophical treatise. But this is short-sighted. Knowing why you need a house next month and how it plugs into the overall pattern of your life (and the life of the planet) is going to help you build a better house: a house that will be more comfortable to live in, but also that will not lose its purpose when you no longer need it. Such a house will, in its own right, become a part of the growing, learning, adapting world. Adopting a less mechanical and more of an agricultural metaphor can make this mental leap easier: what if instead of _building_ houses, we _grew_ them?

2. **Resources are not materials, but agents**.
As humans, we like to throw resources at problems: money, people, concrete, whatever it takes. How many people will it take to solve this in five days? How much concrete do we need to close this gap? We tend to think of resources as materials: we use them, we apply them. We are the subject, the resources are the object. We are the active agent, the resources themselves for us are mostly passive. But from a more “Elvish” point of view, there is no such thing as a passive resource. There are no materials, only agents (of which we represent one group), and they interact with the environments in which they happen to be placed (by us or by chance). Some agents, like concrete, interact slowly and are more inert. Other agents, like humans, interact more quickly and are more obviously dynamic. But all resources are active. Money, if we throw it at a problem, will not just help solve it, but also change the people around it along the way (which may create new problems). New people, if we drop them into an organization, will make their own decisions, their own friends and enemies, their own adaptations, and will influence the overall culture. Even concrete has a mind of its own: it will stand there forever, it will invite mold to grow on it, it will influence the aesthetic of the place, it will put people around it in a certain mood. The difference in perspective here may feel subtle, but it’s not. When you are solving with materials (the “Human” way), you tend to think about the qualities of these materials. When you are solving with agents (a more “Elvish” way), you tend to think about their behaviors over time. If we plant this person (or a tree, or a concrete block) into this system, what will happen? What will this agent _want_ to do? How will it interact with other agents over time? What kind of agent do we need to plant here, so that in the pursuit of their own objectives and natural predispositions they will tilt the overall system in the direction that we want it to go?

3. **Information is not just an input, but an output as well**.
As problem-solving operators, we tend to seek a lot of information about the current state of the system that we are trying to fix. We try to maximize our understanding _before_ we make a decision on how to intervene. We learn so that we can act. But from a more “Elvish” point of view, the relationship between action and understanding is more symmetrical: we act so that we can learn, as much as we learn so that we can act. But how often do we think of “expected learnings” from a project (which is not a research project in the first place), rather than just a set of expected outcomes?
This quick sketch of a more “Elvish” vision for technology is neither complete, nor detailed. But it can serve as a first indication of how our practice of magic could be rebalanced and reimagined. And it is in need of reimagining, because the current way is leading us to its logical outcome of world-amputation at a much higher pace than we may realize. And the more we apply the same approaches to bigger problems, the more this spiral will accelerate.

But even more importantly, our approach to technology needs to change because _we_ have changed. We are much more like the Elves than we used to be, and our powers for understanding have grown. Our lifespans have expanded. And as we are evolving into the Elves, our practice of magic needs to evolve as well. As Konrad Lorenz noted: “When, in the course of its evolution, a species of animals develops a weapon which may destroy a fellow member at one blow, then, in order to survive, it must develop, along with the weapon, a social inhibition to prevent a usage which could endanger the existence of the species”. This applies not only to weapons, but to all technologies (for any magic can be used as a weapon). The explosion of our technological powers needs to be accompanied by the shift in our technological paradigm. If we are turning into gods, we _have_ to get good at it. This should not wait till we become immortal. In fact, one could argue that the first step to becoming truly immortal is to start acting as if we already were.

---
#### footnotes

:
     When we take a closer look in a few pages, the difference in the outcomes may appear less drastic.

:
     Destruction may come in many forms. One of them - subsuming, or eating the world is a basic human instinct.

:
     J.R.R. Tolkien, Letter 131 to Milton Waldman (~1951)  ([https://www.tolkienestate.com/en/writing/letters/letter-milton-waldman.html](https://www.tolkienestate.com/en/writing/letters/letter-milton-waldman.html)) 

:
     Carke’s Three Laws: [https://en.wikipedia.org/wiki/Clarke%27s_three_laws](https://en.wikipedia.org/wiki/Clarke%27s_three_laws) 

:
     Ancient Greek's distinction between Techne, Episteme and Poiesis is similar, but not the same.

:
     Note, also, Prometheus’ severe punishment for disseminating the power. 

:
     This independence may be illusory. For the less we depend on nature, the more we depend on technology itself. It is no coincidence that as technology becomes more advanced, we tend to attribute to it a sense of agency, saying “Technology wants X”.

:
     The number of times a normal somatic, differentiated human cell population will divide before cell division stops ([https://en.wikipedia.org/wiki/Hayflick_limit](https://en.wikipedia.org/wiki/Hayflick_limit))

:
     Suicidal instinct, so common and yet so rarely discussed, can be explained similarly. Stevie Smith used to say that death is “the only god who must come when he is called”.

:
     It is no coincidence that the world _is_ music in Tolkien’s mythology. Any creation myth must deal with the fundamental question of how one becomes many. And if your world has to remain somewhat meaningful, despite the fall and somewhat harmonious, despite the dissonance, then musical metaphor is a very natural one.

:
     Jim Morrison (1967)

:
     Karl Schroeder, "The Rewilding: A Metaphor" ([https://www.youtube.com/watch?v=qb7pkohj6wE](https://www.youtube.com/watch?v=qb7pkohj6wE))

:
     Hereby referring to Karl Friston’s free energy: [https://en.wikipedia.org/wiki/Free_energy_principle](https://en.wikipedia.org/wiki/Free_energy_principle) 

:
     So, in the normal range of effectiveness, Magic maximizes surprise, but its final goal is to minimize it.

:
     Kamaswami: "And what's the use of that? For example, fasting -- what is it good for?"
    Siddhartha: "It is very good, sir. When a person has nothing to eat, fasting is the smartest thing he could do. When, for example, Siddhartha hadn't learned to fast, he would have to accept any kind of service before this day is up, whether it may be with you or wherever, because hunger would force him to do so. But like this, Siddhartha can wait calmly, he knows no impatience, he knows no emergency, for a long time he can allow hunger to besiege him and can laugh about it. This, sir, is what fasting is good for."
Kamaswami: "You're right, Samana. Wait for a moment." (Hermann Hesse - Siddhartha, 1922)

:
     Or Dystopian, depending on your taste.

:
     Brains in vats or minds on silicone - doesn’t matter for our current discussion.

:
     Or, more precisely, magic that would not unnecessarily accelerate either the destruction of the world or our own annihilation.

:
     “Elvish” technology is not completely absent from our current practices, as, for example, the Permaculture movement demonstrates.

:

     For a comprehensive overview of this kind of thinking applied to buildings, see Stewart Brand - “How Buildings Learn" (1994)

:
     Same applies not just to houses, but to anything that we build: systems, software, societies etc.

:

     Opposite interpretation equally possible. You could say that for the Elves, everything, including the Elves themselves is just material, and the only agent is Nature / Eru Ilúvatar.

:
     This methodological approach seems to have been known to many traditional cultures (e.g. Native American), but is now only beginning to be re-discovered.

:

     In one of his many moments of Zen-like insight, William James wrote: “Who can decide offhand, which is absolutely better, to live or to understand life? We must do both alternately and a man can no longer limit himself to either than a pair of scissors can cut with a single one of its blades” (William James - Percept and Concept, Some Problems of Philosophy, 1921)

:
     Note, for example, the language that is currently being used in connection to the climate change problem: “We need to fix it”, “Our target by 2050” ...etc. - arrogantly assuming that we are the only active agent in this system and that we can fix things by action, rather than by understanding.

:
     Konrad Lorenz - King Solomon’s Ring (1952)

:
     "We are as gods and might as well get good at it" (Stewart Brand, Whole Earth Catalog,1968)


=====================================
Telescopic Content
=====================================

I find the best non-fiction writing to be fractal, or [telescopic](https://telescopictext.org). It scales nicely with the amount of attention that you decide to bestow on it, and still makes sense and is tightly packed at every scale.

Christopher Alexander's ["A city is not a tree"](https://bp.ntu.edu.tw/001/Upload/1352/ckfile/176357fb-2068-4e40-8c50-593780261ce7.pdf) is a great example of that. It works at a slogan level. Just the title itself in 6 words outlines the key idea. And yet - at the essay level it works too and is not just a lot of extra words without much extra signal. I can also imagine that if Christopher Alexander wrote a book-length exposition - it would still be powerful in its own right. Without feeling like a "a book that could have been a blogpost".

Writing telescopically is difficult. It pushes you to get to the core of your argument in a similar way that Picasso's famous [bull exploration](https://en.wikipedia.org/wiki/Le_Taureau) did. What are the key lines that make this bull really what it is?

One of the benefits of writing telescopically is clarity. But the other is - stickiness. Because the author goes through the work of compressing and decompressing the thought, the reader has it easier. They don't have to invent their own shorthands and heuristics if they don't want to. The author has already provided them.

Reading can also be telescopic. For example, when you skim over a page and pick out the headlines - before diving in. Or when you ask an LLM to summarize the content for you. That is the shrinking part of it. But you can also expand: when you read commentaries to the original text. Or if you search for other references from footnotes, or read the author's biography to understand the context.

## Telescopic by default... or by design

In the age of LLMs, all content can and will be telescopic by default. LLMs can shrink and expand any text as you wish - either according to their default ideas of what's important, or according to your specific instructions. The upside is obvious: the intentionality of attention that you are willing to give as a reader, and the assistance that you get as a writer in distilling and expanding the argument. But there are downsides as well. Namely, because the attention that you are using to shrink or expand is not exactly yours - there is a real danger of enshittification and tiktokification of content becoming a new norm. Washed out, generalized TL;DRs and AI-generated summaries that kill the spirit of the original text. And if this kind of text is what you grow up reading, it shapes not just your knowledge, but your attention and most importantly - your taste. Once our tools shape our taste - there is no going back.

Still, the genie is out of the box. Someone somewhere must have made a browser extension that makes any content you land on shrink or expand to taste. All content is telescopic by default - whether you as a writer want that or not. So I feel like the only option we have as writers - is to embrace this. And build telescopic by design.

That's what I'm trying to do here. This essay, as well as all other essays on this website - are from now on telescopic. You can decide how much attention you are going to give them. And they scale accordingly - based on the prompt and examples that I provide as the writer, giving me at least some degree of control on how my content will be fractalized.

From the technology standpoint, building it was not hard, but there were a few interesting choices. For example, how do I make sure that the generated alternative still sounds as much like me as possible? For now it's just careful prompting and supplying all the examples to a large context window model along the task and hoping for the best. There is probably fine-tuning coming later, but I'm not yet convinced that it's better, given that all the content fits into the context window.

Another interesting challenge was UX - how do we present the different "zoom" levels. And how do we show to the user the "natural", default one - the way it was written originally. Also as a writer - I want to have a way of "approving" a summarized or expanded version - then it becomes "official" (as indicated by a green dot). For now the way this works is very simple - when a new version is autogenerated - I receive an email with this new version. I can then edit it, add it to the repo and redeploy. And now this version will have a green dot, signifying that this version has been author-approved.

Should you be interested, you can see the implementation [on github](https://github.com/GeorgeStrakhov/essays-thoughts-letters). It's all very basic and mostly written by Claude for me with some guidance. But it works as a proof of concept.

## The magic of free lunch

The most joyful moments of all of this for me as a writer were when I got an expanded version of something - and it had new and powerful ideas or parallels that I haven't seen myself. That was absolutely magical and felt like free lunch. Because it sort of was, I guess. Is it inherently less valuable as a result? Does the amount of work you have to put in somehow make the output truly better? Does the process count? I am not sure about this yet. Most of the time I end up rewriting pretty much everything that LLMs write for me. But there are words, sentences, sometimes even full paragraphs that are starting to stick.

Now, if you take this to its logical conclusion - you don't even need an essay to start with, do you? All you need is a title. So as a continuation of this experiment - I've created a system where 404 pages on this website are a little different. Try it out. You can put any title after the slash. It will generate a new essay (5 min version) based on the title and my other writings. And it will save it and display it for everyone to see. It will also send me an email so that I can look at it, edit and add to the official corpus if I choose so. In the future there will need to be a better way of doing this - where I can log in as the author and approve / edit / delete the generated content. But that's for later.

So all content is from now on infinite. You can zoom in and zoom out at will. You can imagine what a non-existant essay on a topic would look like.
How much "reference data" is enough for this to actually work? My 20-ish essays provide a surprisingly decent (at times) training corpus. When I read some of the autogenerated stuff - it does actually sound very much like me if I was desperately trying to make it work a certain title. Check [this one](/yak-yak-shmuk-fuck/) out for example. Written completely from the title given in the URL of a 404 page. No edits. It's not always this good. It's a bit of a hit and miss. But it starts to work.

**Which begs a question:**

What would a blogging platform look like if it was reimagined and built from the ground up for the world of infinite telescopic content, a world in which readers prompt the writer's doppelganger to generate what they really want to read, and the real writer then comes in and edits it to their liking?

And is it time someone builds this?


=====================================
Creative Midwifery
=====================================

The most interesting things in life are not built, but born. You don't build a child, you nurture it. You don't build a song, you listen for it and coax it into existence. You don't build a great company, you cultivate the conditions for it to emerge.

Yet, we often approach creativity with the mindset of construction workers. We try to hammer ideas into shape, forcing them to conform to our preconceived notions and expectations. We treat our minds like factories, churning out products on demand.

But what if creativity were more akin to gardening, or even midwifery? What if our role as creators was not to build, but to assist in the birth of something new, something that already exists in potential, waiting to be brought into the world?

I call this "creative midwifery", and yes, [Socrates had this thought first](https://en.wikipedia.org/wiki/Socratic_method) in a slightly different context.

## The Art of Letting Go

The first step in becoming a creative midwife (for yourself or for another human being) is to let go of the illusion of control. We often cling to our ideas with a possessive grip, afraid to deviate from our initial vision (even if this vision is just a mirage). But true creativity often arises from the unexpected, from the surprising twists and turns that emerge when we allow the idea to guide us, rather than the other way around.

As Keith Jarret said, you can either be in the improvisation or be an ["improvisation ecologist"](https://youtu.be/BUH-FbhS-A4?si=GI94w0Spvh5SbR4u&t=472) who understands the rules and constraints and intentions... but whose improvisation is ultimately stillborn.

Think of a sculptor working with a piece of marble. They begin with a vague idea of what they want to create, but the final form emerges from a dialogue with the stone itself. The sculptor listens to the grain, the fissures, the inherent qualities of the material, and allows these to shape the final outcome. They don't force the marble to become something it's not; they help it become what it is meant to be. Carving out, not carving up.

Similarly, the creative midwife approaches an idea with a sense of openness and curiosity. They are willing to abandon their initial plans, to follow the threads that lead them in unexpected directions. They trust that the idea itself knows where it wants to go, and their role is simply to provide the conditions for it to get there.

## Cultivating the Fertile Ground

The second key element of creative midwifery is cultivating the fertile ground in which ideas can take root and flourish. This involves creating a mental and emotional environment that is conducive to creativity: a space of curiosity, playfulness, and receptivity. A habit of the mind that is open to the unexpected.

Like a gardener preparing the soil, the creative midwife tends to their mind, nourishing it with everything life throws at them. They read widely, listen attentively, and engage in conversations that challenge their assumptions. They practice having their hook out in the water at all times, whether the fishes are biting or not.

## Assisting the Birth

Finally, the creative midwife plays an active role in assisting the birth of the idea. And this active role is mostly about confidence, about trust... almost faith in the process.

Like a midwife assisting a mother in labor, the creative midwife is present, attentive, and responsive, but is not the one giving birth. They trust that the idea has its own inherent intelligence, its own unique path to follow. Their job is to clear the way.

In a world increasingly dominated by algorithms and automation, it's easy to feel like we are losing our agency, our ability to shape our own destinies. But creative midwifery offers a different perspective: a way of working *with* technology, rather than being controlled by it. By embracing a more humble and receptive approach to creativity, we can harness the power of technology to bring forth new possibilities, new forms of expression, and new ways of being human.

Think of it as a larger pond. With more fishes. Beautiful fishes. [Weird fishes](https://www.youtube.com/watch?v=pcEJyvv6_kc). Do you have your hook out?


=====================================
Seek-Reject-Frame
=====================================

###### _a proposed architecture for Freudian Agents_

- - -

## Current agentic patterns: the assembly line, the org chart, the parliament

Most agentic approaches and frameworks today are following the same basic pattern:

- Give LLM some goals and some tools.
- Allow it to run a few loops - using tools to achieve the goal and evaluate the results.
- Return the final result to the user when the goal is reached.

One can then string these basic "building block" agents together into a more powerful meta-agent or agent team. For this there are a few patterns (as, for example, implemented in [crew.ai](https://docs.crewai.com/concepts/processes)):

- **Sequential pattern**: no magic here, just a simple programmatic flow. This is like agents working on a pre-defined assembly line. 

- **Hierarchical pattern**: one manager agent, effectively using other agents as tools. So, the same pattern as a single agent, just scaled up (potentially this can fractally scale up indefinitely, like a hierarchical organisation as it grows)

- **Democratic pattern**: various agents working together have to reach some sort of consensus for the system to give the final output. This is the least clear pattern and I'm yet to see any practical implementation of it, but in theory it sounds interesting and possible.

All of the above makes intuitive sense because this is our usual way of thinking about _human_ collaboration in organisational and societal settings: the assembly line, the org chart, the parliament.

## New brain-inspired approach: Seek - Reject - Frame

I would like to suggest a possibility for an alternative approach: what if instead of taking inspiration from the inter-human organisational patterns, we look into the intrinsic organisation of a single human being? What if we try to assume that LLM mini-agents are not "worker" building blocks for an organisation, but rather parts of a [society of mind](https://en.wikipedia.org/wiki/Society_of_Mind)?

The latest research in neuropsychology [suggests](https://www.google.com/search?q=the+hidden+spring+mark+solms) that the brain is not organised in a top-down way and neither is it democratic. Most importantly, the overall drive / goal of the system does not seem to be coming from the top at all. It's coming from the bottom. Here is a brutally simplified idea:

### 1. Seek

At the bottom we have a SEEKER. A relatively simple, evolutionary archaic agent with an innate drive that it wants to satisfy all costs. It has access to the sensory input and intuitively / quickly makes deliberations about what should be done to further its goal, i.e. it produces a desire or a **wish**. The kinds of goals that a SEEKER can pursue are very simplistic and relatively short-term. A seeker knows what's good for it and is always looking to get it.

> Hungry? Let's go find some food. Yum yum!

This kind of thing. The time horizon is short. There is very little planning involved. And the value function that the SEEKER is optimizing for is usually as simplistic as that of a thermostat i.e. check if you are in an optimal zone and if not - do whatever it takes to get back to it.

### 2. Reject

At the top - we have a REJECTOR. The job of the higher brain is not to generate desires and goals. It's to tame them, to prevent them from ruining your life. The drives are coming from a bunch of SEEKERs below, but those SEEKERs themselves are too simplistic to be trusted fully to run such a complex organism:

> Yes there is tasty food on the table next to you in the restaurant. And you have an urge to grab it. But it's probably not wise to do so as the people next to you won't be amused and you would end up in a fight or a police station. And your date won't like that. So thank you very much SEEKER, but no thank you. Not right now. Wait.

The REJECTOR's job is to say no. To look at all the things that the SEEKER (or in reality a few SEEKERS, each with different goals / drives) wants to do and to reject 99% of them 99% of the time. Occasionally some wish (proposed action) looks decent enough to go for it. Or maybe one of the SEEKERs gets such a strong desire that REJECTOR just can't do anything about it and has to let it through (as long as there are tools available to action on it).

> SEEKER: We are starving. Let's grab this right now.

> REJECTOR: Oh, no, but we will annoy others.

> SEEKER: We will DIE if we don't. Shut up and tell that hand to grub the yum yum RIGHT NOW.

> REJECTOR: Ok, fine. We will make up a story for why it was a proper thing to do later.

How does the grand filter of the REJECTOR operate? Here is a possible model:

- First, the REJECTOR collects all the proposed wishes from the SEEKERs. For each wish there is an associated strength (which can be used as a weight or for a gate function of some sort).

- Then, the REJECTOR tries to understand whether there is a concrete action that can be taken to fulfill this wish and then does some imaginary time-travel to predict the consequences of each proposed action on different time horizons. This is the superpower of the higher brain: doing time-travel and running multiple simulations of the future, to understand whether the SEEKER's wishes could be realized and whether going for it makes any sense in the longer run. What does "making sense" mean here? I'm not too sure how to best formalize this. But a good first guess is to simplify the judgement to the following three factors:
    - **safety** (self-preservation drive)
    - **satisfaction** of the same drives of the SEEKERs but in the future.
    - **self-story** consistency i.e. are we continuing what we have always been about? More on this below.

- Finally, after doing the future simulations and calculating the overall value function (safety + satisfaction score in the future + story consistency) - the REJECTOR decides which SEEKER's wish (if any) should be fulfilled right now and what action it can be best fulfilled with. And then the action is taken.

### 3. Frame

The last, but critically important step is to post-rationalize the story of why we did what we did and why it made sense. This is done by the FRAMER _after_ the choice is made. The job of the FRAMER is to invent a plausible story about why the collective intelligence of SEEKERS + REJECTOR is doing what they are doing. Self-story-making here is critical for the future operation of the REJECTOR. 

SEEKERs don't much care about the past or the distant future. They are just optimising for the perfect state in the here and now. But the REJECTOR needs access to long-term memories and stories so that it can project into the future better. And so it's the FRAMER's job to continuously form (and deform) memories in a way that would imply a consistent, coherent self: a story that can be continued and against which the possible future actions can be evaluated. If the REJECTOR is concerned with physical self-preservation, then the FRAMER is concerned with conceptual self-preservation.

As Mark Solms likes to put it: memories are _of_ the past, but they are _for_ the future. Past memories can easily be altered and molded to fit the needs for a coherent self-story, because without a coherent self-story a REJECTOR would not have enough basis for a reasonable choice: a choice that would have a chance to truly optimize the overall chances of success of the collective intelligence in the long run.

## SRF in practice

Let us now imagine how SEEK-REJECT-FRAME (SRF) pattern could be implemented in practice, using LLMs as building blocks. Here is a conceptual diagram:

- An SRF agent runs in a continuous loop that never stops as long as the agent is alive and not hybernated.

- At the start of the loop we programmatically check for new inputs i.e. is there anything in the context?

- The context data (if not empty) is passed to each of the SEEKER agents as a new message in the message stack. If no new data - then the old message stack from the previous iteration is passed again. Each SEEKER is an LLM with a specific system prompt that defines its target state and what it's meant to do if the target state is not matching the context. We don't need a very smart model. Something fast and straightforward will do.

- SEEKERs don't really have proper long-term memory. But there is a buffer of some number of "messages" that we can keep in the short-term memory stack of the SEEKER.

- After evaluating the context against the goal (target state), each SEEKER outputs an "urge" in the form of a structured response with 2 things in it: "wish" and "strength" (0-10 scale).

- As the next step of the loop, the REJECTOR kicks into action. It takes multiple inputs:
    - urges from all the SEEKERs whose wishes have a strength above 1.
    - outside context info (the same information that was passed to the SEEKERs)
    - available tools / possible actions that can be taken
    - previous self-story and relevant memories (RAG from context + wishes)

- Based on all of the above the REJECTOR makes a deliberation. The system prompt specifies that it needs to decide to take an action and which action (or refrain from acting explicitly. The choices should be made by projecting possible consequences of fulfilling "urges" into the future and evaluating the results by safety, future satisfaction of urges and self-story-consistency). Rejector is where we need a really smart LLM, I think latest reasoning models will do well here.

- The REJECTOR may decide to do nothing. That's as important a decision as any other and is processed in the similar way described below.

- When the REJECTOR has finished deliberating - it outputs the action to be taken and the reasoning behind this action. The reasoning should be long and detailed, including relevant context details i.e. which parts of situation it decided to pay attention to when it deliberated, as well as wishes that were present, but suppressed or followed through and why.

- If there is an action to be taken - it can be taken via available tools given to the REJECTOR. And the results of that action (if any) should be added to the context for the next iteration of the SRF agent loop.

- Both the action (or non-action) and the reasoning behind it are then passed to the FRAMER for the last "post-rationalization and memory formation" step of the loop.

- The FRAMER is activated _after_ the action is taken (or at least after the signal to do the action is sent) so as not to slow down the process. The FRAMER takes in the action, the reasoning and previous self-story. It then proceeds to rewrite/update the self-story to frame the action (or non-action) as a coherent continuation of the self. It also decides what new individual memories to add to the memory store and does so (write a snippet, calculate embeddings, add to the vector store). The FRAMER, like the REJECTOR needs to be driven by a strong LLM with a large context window, and ability to conceptualize and post-rationalize.

- It's important that actions-not-taken are equally contributing to the self-story and memories because suppressed wishes are an important aspect of personality (hello, Freud!). From a pragmatic standpoint, actions not taken are really important learning moments. One of the reasons humans are so much "smarter" than other animals is that our ability to **"learn by not doing"** is very strong. We can imagine in our heads the consequences and we can learn from those imaginary tries over time. Without actually suffering the consequences in the real world.

- This concludes a single loop of our SRF agent existence. And we are off onto the next iteration. Where the results of actions taken, as well as new user messages or sensor readings may be present in the context. And the merry go round starts again.

As an interesting side benefit, the SRF design allows an agent to "dream". In a dream - the agent doesn't actually take the actions in the real world. So the urges coming from the SEEKERs don't need to be suppressed as much. Dreaming is a state where the REJECTOR's suppression function is significantly diminished and it doesn't have any tools to do actions in the real world. Instead of taking action - it just short-cuts its projections of the imaginary actions into the imaginary context. And the loop keeps going (including the FRAMER!) - so learning is happening, all be it in a different way (possibly a different threshold for memory formation is applied).

## Are Freudian Agents practical? Are they conscious?

So we have successfully imagined a new kind of SRF (SEEK-REJECT-FRAME) system, which we can call a Freudian Agent, since it starts with the "subconscious" wishes and then suppresses them later.

A couple of important questions arise:

- Can such agents be built?
- Would they be practical and valuable?
- Would they be conscious?

I don't have the answers. But my intuition is that they **can** indeed be built (nothing in the diagram above looks impossible and I may even try to make a simple proof of concept in the coming months).

I also do believe that there will be practical uses for such agents. You don't need them for simple things. But in complex, long-running, highly ambiguous environments where today's simple hierarchical agents fail miserably - Freudian agents could show more resilience and ultimately more intelligent goal-directed behavior. However designing and training a good Freudian agent will be
a tricky art:

> What baseline desires (for SEEKERs) do you need in order to have the final agent behave in a way that you want it to behave?

> How cautious and strict should the REJECTOR be? Is the cautiousness (i.e. wish-to-action threshold) a fixed meta-parameter or something that dynamically changes depending on the context and the state of the system as a whole?

> What initial self-story (if any) would you want to implant into the FRAMER's memory?

Last but not least, on the matter of consciousness: like [Michael Levin](https://www.youtube.com/watch?v=7FJfdO53Q-w), I reject the idea of consciousness as a binary kind of quality. It seems to me that there is no hard boundary. It's a spectrum: from the simplest stone, to the human being and beyond. And so the question "Are they conscious?" is somewhat meaningless.

The real question is: could Freudian Agents be _more_ conscious than the simple ReACT ones we have today? My intuition is that they could.

But we will have to build them to find out.


=====================================
Sculptors And Gardeners
=====================================

_on the origin of a good line, two ways of creativity and how to make LLMs better at writing_

## Part 1 (poetic): The soil and the sword

**How does a powerful line emerge?**

As writers of words, music, or code, we are constantly confronted with this atomic challenge of our craft: getting to a single good line. Not just a single word or a note—that in itself is still devoid of power, surprise, or relationship. But a line, a connected set of dots and spaces in between, a phrase. Once you have a good line, the rest can feel easier. You just need the next one. And you need to know when to stop, which is a whole art in itself. But the big question remains: how does a single good line come about?

The ultimate path of divine inspiration aside, I'd like to propose that there are two primary approaches to this challenge. Let us call them sculpting and gardening.

Gardening is the way of the soil. It's additive. It can be indecisive. You unblock and create conditions for variety. You remove the censors and allow dozens, hundreds of possibilities to grow onto the page (or at least onto your mental scratchpad). The weak ones die out. You evolve the stronger ones until they bear fruit. And at some point, when the stars align - or when you run out of patience — the best option becomes the one. The ultimate garden is a jungle: the "stream of consciousness" style of writing where the natural flow is unimpeded and curation is minimal.

*Picasso's series of lithographs showing the progressive abstraction of a bull, 1945-1946*

Sculpting, on the other hand, is the way of the sword. It's subtractive. It's fateful. You take a block of raw material in which there is a latent possibility, and you cut out everything that is not essential. Ruthlessly. Decisively. You strip it to the bone. And you tremble as you do it, because there is no gluing anything back. The ultimate example of sculpting technique in a linguistic medium is blackout poetry, where the newspaper page is your block of marble and the black marker is your unforgiving chisel.   

Sculpting and gardening feel very different. But there is a lot of underlying similarity. Both require you to trust the medium. In the case of gardening, you need to trust that the soil is ultimately capable of growing what you are longing for. In the case of sculpting, you need to trust that the block of stone contains the right kind of possibility in it.

*Poem by [Tyler Knott Gregson](https://tylerknott.tumblr.com/post/172540715407/in-my-solitude-i-became-aware-of-lack-lie-near-me)*

Both methods put the primary responsibility on perception, not on creation. In gardening — as a multitude of variations emerge — your real job (beyond waiting, watering, seeding, fertilizing) is to see the promising specimens and select them. Famously, Brian Eno talks about how his way of composing is significantly about selection; he can be seen as a "selectionist" composer, listening to hours of semi-random combinations of possibilities generated by systems he creates, and then picking the compelling ones.

In sculpting, it's also all about perception, not creation. Except what you are looking for is not the best pieces, but the unnecessary ones. Which parts can be removed? Which words are irrelevant to the heart of the matter? Picasso and his bull plates come to mind as the epitome of sculpting. Similarly, consider Thelonious Monk and his punctuated, sparse, almost disappearing melodies that hit hard precisely because all the embellishments are removed to the point where the structure barely holds together.

Sculpting and gardening are methods, not religions. If one doesn't work, you try another. You alternate. I would even argue that for the majority of creative projects, you need to use both. Write drunk, edit sober. Treat your medium as a highly agentic, unstoppable generator of possibilities. Then, as an inert stone that imprisons your creation. Breathe in, breathe out. Just don't do both at the same time.

- - -

## Part 2 (practical): Testing the Theory

Now that the dichotomy is established, let us shift to a less poetic gear and think about how we could test this dualistic "sculpting-gardening" theory of creativity in practice. The central prediction our theory makes is that people who practice both sculpting and gardening will, on average, arrive at better output than people who only use one of these methods.

To test this prediction, we would ideally need a large rooms full of identical twin poets, divided into four groups:

1. The purist gardeners
2. The purist sculptors
3. The balanced sculptor-gardeners who practice both approaches equally
4. A control group who don't practice either specific method intentionally

After a year of study, we would bring the poets together and ask them all to write a poem in a day. We would then need to get them to self-rate the poems and how they felt writing them. We should also get the poems rated by independent judges. If, at the end of all this torture, the balanced sculptor-gardeners reliably create better work and feel good about this work themselves, then we would have evidence to support our theory.

Sadly, I don't have a room full of identical twin poets on whom I can run experiments. So, we can turn our attention to something more accessible: Large Language Models (LLMs) that, despite all their glories, are still mostly terrible at creative writing. Let us examine how LLMs write.

At its core, the next token prediction logic is very close to gardening, except the unit is a token, not a line. At each next token, the inner layers of the network generate options for what that token could be, and the last layer exposes each with its perceived probability. At the very end of the process, the algorithm selects one of the "best" possible alternatives (one of which is to stop by selecting the stop token as the next one). This selection normally happens using a mechanical approach (i.e., parameters like temperature, top_k, top_p).

So we can say that, at least at the level of tokens, the gardening method is somewhat there—multiple alternatives are explored, and then one is picked.

But what about sculpting? Sculpting seems nowhere to be found. Even reasoning LLMs that can sometimes change their "mind" mid-thought and omit outputting certain parts of their internal processing as the final answer still don't do proper sculpting, in the sense of refining a larger generated block by strategic removal, at any point in the process.

Could the lack of sculpting be a reason why LLMs can be perceived as less than stellar at creative writing? Could we add sculpting to their creative metabolism? In some sense, diffusion algorithms (often used in image generation) are closer to sculpting. They typically start with a randomized block (like noise) and then gradually adjust it to look more like what we are after, iteratively refining the image. But still, this is not exactly the same as targeted, subtractive chiseling of an existing, over-complete form.   

So let us speculate on how proper sculptor networks could be created for text. Our minimal goal is to train a network that will perform blackout poetry well. We will give it a piece of text, and it will cross out words and letters to create a subtext out of it, and this subtext should have creative power.

What we need first is obviously training data. Luckily, we can create plenty of it by taking powerful pieces of poetry from the public domain and "expanding" them using conventional LLMs, which are great at this kind of elaboration.

For example, we can take Hokusai's famous haiku:

> Though on the sign
> 
> it says "Mount Fuji this way" —
> 
> We see only mist

Then we can ask a conventional LLM to fill in the words around this core, and it might end up with something like this (illustrative example):

> The museum staff had been working late to finish the new exhibit. **Though** exhausted, they pressed on, determined to complete the installation. A visitor's sign-in sheet lay forgotten **on the sign**-in desk. **It** clearly **says** in bold letters "**Mount Fuji** Photography Exhibition **This Way**," with an arrow pointing to the east wing. But when **we** arrived that morning to **see** the display, **only** empty walls and **mist** from the humidifiers greeted us—the photographs were still in their crates.

Now we have ourselves a training pair: the expanded text as input, and the original haiku as the desired "sculpted" output. What's great about this method is that for the same target poem, we can produce an unlimited number of source texts - different in length, style and subject.

Extrapolate this lot, and we have a training set. A training set for artistic attention — for finding the special in the mundane, for chiseling away the obvious to reveal the beauty hidden inside it.

The rest is technical: training, scaling, tweaking, regularising and injecting noise to avoid overfitting the data — the usual machine learning procedures⁶.

One interesting question is whether to supply an instruction together with the source text (e.g., we could supply the source text + instruction like "write a haiku about Mount Fuji and mist" as input). Or, only the input — effectively training the network to find the subject itself, but relinquishing control over what the network will give us back. Probably both should be tried, but my intuition currently is that the latter (no instruction) is going to be more interesting.

Another interesting question is whether we should supply, as part of the input, the target number of words to be left, or perhaps the target percentage of the input reduction. We can then use our synthetic data generation process described above to generate source examples that are "watered down" to various degrees.

Lots of interesting parameters and details to play with. But that's for another day.

For now, let us just imagine for a second that this whole process works, and we have an expert sculptor network — the blackout poet laureate made of silicon. What would we do with it next?

It would probably be interesting and useful by itself as an artistic or editorial aid. But we could also consider adding it as a step in the traditional LLM stack. Imagine that a future creative agent does some "gardening" generation with high temperature first — effectively producing a rich stream of possibilities—but then, at the end of each line or phrase, the sculptor mini-network is engaged to cut things up and chisel it to the aesthetic core it had possibly hidden inside.

This may seem similar to what reasoning models do, but it's not really the same thing as the post-thought process is not that of adding, but that of taking away.
Also the idea is to do this at the level of each line or phrase, not the full response. Or maybe the technique of alternating gardening and sculpting can be applied at a variety of levels: from short phrases to full texts. In any case, this loop should go line by line. Seeding and growing the options, then chiseling the good one(s) into their essence. And then the loop continues to the next line.

Hopefully, a powerful one.


=====================================
AI's Near Future: Choose Your Quadrant
=====================================

I have no idea what the longer-term future of AI, software, humans, robots, and the world at large has in store. And I don't believe anyone else knows with any degree of certainty.

But the mid-term future of AI software, and the user experience around it, is starting to get a little clearer. Not in terms of exactly where things will go, but at least in terms of where they could go. Not the final destination, but at least a map of the possible terrain.

Before we lay out the map, we need to understand the tectonic forces shaping it. The entire AI landscape is being pulled in two different directions along two main axes of tension.

### Axis 1: Where will AI live? The Cloud vs. The Device

The first major tension is about the physical location of AI computation.

On one end of the spectrum, you have **The Cloud**. This is the world of brute-force computation, exemplified by massive, multi-billion dollar data centers like ("Project Stargate" etc.) It's a future powered by enormous, centralized models that require immense energy and capital to both train and run but offer unparalleled scale and power.

On the other end, you have **The Device**. This future is driven by finesse and efficiency. Thanks to breakthroughs in quantization, the development of tiny yet powerful Vision Language Models (vLLMs), and the relentless march of Moore's Law, it's now (almost) possible to run a GPT-4o level model locally on a high-end smartphone. This approach promises superior privacy, offline availability, and zero-latency interactions.

### Axis 2: How will we interact with AI? One vs. Many

The second critical tension is about the user experience paradigm.

On one side is the vision of **One AI to Rule Them All**. This is a UX paradigm where the user primarily interacts with a single, universal AI assistant (like ChatGPT). You tell the AI what you want, and it orchestrates other apps and services in the background to get it done. The individual apps become invisible, abstracted away by one conversational interface (which integrated various contexts of your life and has memory etc).

Opposing this is the future where **Every App Has Its Own AI**. This is the "copilot" model, seen in sidebar interfaces like Cursor or integrations built with tools like CopilotKit. Here, the AI is a collaborator within a familiar application. The key advantage is shared context; the human and the AI see the same screen, work on the same data, and the user can seamlessly switch between direct manipulation and AI assistance.

With these two axes defined - Cloud vs. Device and One AI vs. Many - we can now create a clear map with four distinct quadrants for the future of AI.

Looking at this map, we can see major players making their bets on where the future should go. While most have projects across the spectrum, their strategic center of gravity - their most likely path to market dominance - can be placed in one of these quadrants. Some are spreading their risks, but others are going all-in on a single-quadrant future.

**Apple** is the easiest one to place. Being at its core a hardware company, they are betting on (and working towards) the future where AI runs on-device. They can wrap this into compelling privacy and offline availability stories. They can also sell a lot of really expensive hardware (both desktops and phones) as a result.

**Meta** is a little harder to place (perhaps because they don't really know themselves), but it seems they have not been very successful at the cloud and foundation models game. However, Zuck's metaverse ambitions and the doubling down on hardware with Ray-Ban indicate that they see the future as more on-device (their goggles in this case). They would like to own the primary interface (voice + touch), because that would allow them to own the user.

Next up is **OpenAI**. Altman's bet on the cloud is clear, at least for now. And even their upcoming move into hardware with Ive is likely to be a thin client. Computer use, scheduled agents etc. make it clear that OpenAI's vision is one where the user doesn't really touch anything else. They just tell ChatGPT what to do, and it does it for them.

**Anthropic** seems not far from OpenAI's position, though they seem slightly less hell-bent on the "one AI to rule them all" game, as their partnerships with Cursor and others show. They seem more interested in developing the best foundation models that can be used both as an interface to the world and as an underlying component for other experiences. Recently, Anthropic has been pushing its own "last mile" much more with native integrations, but it's unclear if they are all-in on becoming the Sauron of AI.

Next up is **Google**. As usual, it is too large to take one clear bet. They have amazing strength in TPUs, so they are definitely in the cloud game. Many Google apps are getting integrated AI capabilities. But judging by the latest Firebase releases, they are no strangers to the idea of on-device AI and, being the owners of Android, they would be foolish if they didn't go there. One thing Google seems less committed to is making Gemini the only interface into everything. This may be strategic, or it may be that they are just too big to execute this sharply.

**Microsoft's** main bet, on the other hand, seems easier to place. Apart from their stake in OpenAI, their primary hold on the world is Windows, Office, and Big Corporate Contracts. With these cards, their winning path is to embed AI copilots into their existing apps, pulling corporate clients deeper into the Azure ecosystem with private, firewalled models.

And then there's **X and Tesla** – Musk's constellation of companies that defy easy categorization. The dashed circle on our map reflects genuine uncertainty about their trajectory. Tesla has made massive investments in on-device compute for Full Self-Driving, with each vehicle essentially a rolling data center. Yet they're also building Dojo, a supercomputer infrastructure that rivals the biggest cloud players. Meanwhile, X's Grok runs in the cloud but serves Musk's vision of a singular "everything app." The truth is, Musk might be playing an entirely different game – one where the distinction between cloud and device becomes irrelevant because Tesla cars and Optimus robots become mobile nodes in a distributed compute network. And with Neuralink - you could be a node in that network of distributed superhuman collective intelligence too. Exciting. Scary. And far away. Let's get back to the map.

There are lots more players, of course, but it would take a book to cover them all. Broadly speaking, smaller labs (like Mistral) and big Chinese players seem (at least for now) committed to a more decentralized future, publishing small open models, with inference providers like Groq delivering these models to the world.

### But what about us?

This covers the big players' bets on the future. But what about the little guy? What about someone who is planning their career or starting a company today? It seems that the little guys need to make some bets as well.

For example, if you are building a B2B project and bet on the **Top-Right corner** (cloud-based, one AI interface), then you shouldn't be building for human users at all. You should assume your actual user will be an AI agent, and you need to ensure that agent chooses you and gets the best results out of you. Clean APIs, well protected and organized MCPs, shared rewards systems etc. are more important than human-level UX (there is probably a new design discipline emerging here: AX - agent experience. But that's a topic for another essay).

Or, if you are starting a B2C project, like an AI fitness coach, and you believe the future is on-device and domain-specific (**Bottom-Left corner**), you can justify aggressively acquiring users now with negative unit-economics, wasting tons of money on tokens. This could make sense if you can bank on the fact that soon user's devices will do the heavy lifting, and your subscribed users will become almost pure profit (like in the good old days of software that you only had to build and maintain but not pay for running continuously).

The **Top-Left corner** (cloud-based, many AIs) is perhaps the most crowded but also most immediately profitable. This is where specialized enterprise tools thrive – AI for legal research, medical diagnosis, financial modeling. Each tool goes deep rather than broad, and the cloud provides the computational muscle for specialized tasks. The key here seems to be owning a workflow (and the data associated with it) so completely that even when the "one AI" players come knocking, customers can't imagine switching.

The **Bottom-Right quadrant** (on-device, one AI) might seem like Apple's exclusive domain, but it offers opportunities for those building privacy-first, personal AI experiences. And yes, you can build it today even as a small startup, check [Cactus](https://cactuscompute.com) for example.

In reality, the future will probably look like a little bit of everything for a while. The closest parallel to what we are seeing with AI is electrification at the beginning of the 20th century. Electrification of the West took about 70 years (from first commercial uses in 1880s and 1890s to the nearly universal use in every home and factory by the 1960s). AI can go much faster, but humans, legal practices, habits, manufacturing, education and trust are still slow. So I bet we are still talking decades, rather than months.

In the meantime, every builder and investor is invited to place a bet on a quadrant of the future map. And while the long-term reality will likely be a blend, the dominant platforms and value chains of the next decade(s) will be forged by those who choose their quadrant wisely today.


=====================================
Cargo Cult Intelligence
=====================================

### or why you should allow your LLM to get punched in the mouth

    The first principle is that you must not fool yourself — and you are the easiest person to fool.

- Richard Feynman

There is a difference between sounding smart and being smart. One is a performance, the other is a capability. True intelligence is an ability to achieve goals in a messy, ever-changing world. Fake intelligence just goes through the motions that may be right, or wrong - without knowing the difference.

We all know this distinction from our human interactions. We've all met that impressive-sounding businessperson, armed with jargon and platitudes, who glides from meeting to meeting but never produces anything of substance. They are masters of the appearance of work, shielded by their bullshit and jobhopping from the real-world consequences that demand adaptation and tangible results.

Now, this same dichotomy is coming to life in our machines. The latest LLMs can sound impressively intelligent — generating coherent documents, plausible strategies, and articulate explanations — without necessarily having any grounding in reality. The problem is telling the two apart is harder than it looks. They say that even a broken clock is right twice a day. And when it comes to LLMs - they are right more often than that even without grounding or verification. Simply because certain heuristics extracted from pre and post training - just work most of the time.

## So how many times a day does a clock need to be right in order  to be useful?

The pragmatic answer to this question depends on the problem domain and the variety distribution within it. As well as the cost of mistakes.
But most people are not going to settle for a pragmatic answer. They want their clock to be damn right all the time. And if that's what you want (and you should indeed want that, at least in a lot of business-critical tasks) - then we have to approach the question not pragmatically, but scientifically.

The problem here is not new at all. Richard Feynman famously called it ["cargo cult science"](https://people.cs.uchicago.edu/~ravenben/cargocult.html). He was referring to the islanders in the South Pacific who, after seeing military cargo planes land during World War II, built elaborate replicas of runways, control towers, and even airplanes out of wood and straw. They performed the rituals—they mimicked the form of the operation—but the planes didn't land. They had the appearance of an airfield but lacked the underlying substance. Feynman used this as a metaphor for scientific practices that mimic the rigor and process of science without embodying its core principle of intellectual honesty, leading to results that look plausible but lack validity.

For a long time, AI systems could only perform this kind of imitation. They were masters of “cargo cult intelligence,” capable of producing coherent-looking text that, upon closer inspection, was ungrounded and useless for achieving real-world goals. This was before they had interfaces with the real world. Today, however, the top AI systems are capable of both. Depending on the context, the tools they are given, and the architecture of how they are assembled and used, they can exhibit either “cargo cult intelligence” or “real intelligence.”

The difference is not always easy to spot. For the untrained eye, distinguishing between a plausible-sounding but vacuous AI-generated business plan and a genuinely strategic one is as difficult as telling a real business presentation from a jargon-filled BS one. Experienced people with domain-specific knowledge can sniff it on the second slide. The rest of the crowd (including the decision makers) often just eat it up, because they don't know any better.

Imagine, for instance, you ask a powerful LLM to write a marketing plan for your startup. It will produce a perfectly plausible document, filled with smart-sounding sections on SWOT analysis, target demographics, and channel strategies. It will look like a marketing plan. It will sound like a marketing plan. It may even have some useful ideas in it, mixed in with all the platitudes and bullshit. But it will be a cargo cult artifact — an imitation of a plan, disconnected from the reality of your customers, your brand and business, your market, and your goals.

But the result is entirely different if you take that same LLM, give it an agentic harness and allow it to:

- Access historical and real-time data about your product's performance.
- Access customer research and complaints database.
- Analyze data about your competitors, including their marketing and media strategies.
- Research both internal and external best practices.
- Investigate the wider context of market trends and cultural shifts.
- Utilize a wide range of available creative assets.
- Draw from a repository of proven and new marketing tactics and frameworks.
- Run an "in-vitro" create-test-adjust loop (where one agent produces a concept, another acts as a synthetic audience reacting to it, a third judges and adjusts, etc.) in an adversarial setup to plan scenarios and improve outcomes.
- Check its proposals against your benchmarks and KPIs to optimize for your actual goals.
- And ideally it should actually run small-scale tests in the real world and adjust its plan based on the feedback it receives.

A system like this can produce not just a plausible-sounding plan, but an output that is truly useful, adaptive, and grounded in reality. It shifts from mere imitation to genuine capability. It wouldn’t be perfect (nothing ever is), but it will be grounded. And it will be special to your context. The intelligence is not in the core model alone, but also in the membrane between the agent and the environment. In fact, I’m not sure the very concept of intelligence has any meaning in a vacuum. If there is no world, and there is no goal - how can anything be deemed intelligent?

Oh, and don't get me wrong. There are plenty of good heuristics and lots of useful common sense in LLMs even without the harness and the interface with the world. The problem with those heuristics is that without the contact with reality - their potential usefulness is that of a broken clock that is right twice a day.

This brings us back to Feynman's warning. The most important skill in this new era is continuously learning how to not fool ourselves. And boy, do we love to be fooled. We are drawn to the answer that sounds good, the polished presentation, the confident-sounding assertion. The cargo cult is seductive because it offers the appearance of truth without the hard work of understanding. If your AI system can't meaningfully be wrong within its own context... then how can it be right?

The challenge, then, is not to simply marvel at the coherence of AI-generated text (and a marvel it is!). It's to build and use systems that connect that coherence to consequences. According to Rich Sutton and some other proponents of ultimate from-the-ground-up Reinforcement Learning, true intelligence won't emerge until the real-time learning from reality is built right in at the ground level of our AI systems. I am not sure he is right on this one. I think we could get meaningfully and pragmatically intelligent things out of LLMs, but it's on us to provide the interface to reality and the learning. Maybe this is the shape of our hybrid, collectively co-intelligent system for now: humans not just being the reproductive organs of technology, but also its sensors and continuous learning subsystems.

But philosophy aside, the immediate future of intelligence, both human and artificial, depends on our ability to tell the difference between the mock runway and the real thing.
So for practical AI builders today the question is this: are we building systems to help us land real cargo, or are we just getting better at carving airplanes out of straw? The only way to find out is - using Mike Tyson's words - to get punched in the mouth. We have to allow our systems to fail. And we need to build robust ways of finding out early and often when they do, so that we can change them… or they can change themselves.

### Are you ready to let your LLM be punched in the mouth? Or would you rather keep building airplanes out of straw, hoping for VC cargo to fall from the sky?


=====================================
To Hell With AGI Lets Solve DCT
=====================================

The AI discourse is currently drowning in its own intellectual masturbation. Everyone's jerking off about whether ChatGPT can pass the bar exam or generate a pharmaceutical patent or simulate a convincing universe. Meanwhile, I can't get a robot to help me with the one task that makes me want to fake my own death every week: changing the fucking duvet cover.

We're obsessed with AGI like it's the Second Coming. We've got benchmarks for everything - can it solve calculus? Can it write poetry? Can it beat humans at StarCraft? Above all: can it make ads that dumb people on facebook will click? These are the questions of people who've never had to wrestle a king-size duvet cover at 11 PM on a work night, alone, sweating, questioning every life choice that led them to that moment.

I'm proposing a new test. Forget Turing. Forget the bar exam. Forget ARC-AGI.

I want **The Duvet Cover Test (DCT)**.

## The Challenge That Will Humble Your Neural Networks

Here's the setup: Take a robot. Any robot. Give it a key to a random person's home. Its mission - should it choose to accept it, which it won't, because robots don't have existential dread yet - is to autonomously change every duvet cover in that house.

That's it. Find the beds. Find the clean covers. Remove the old ones. Put on the new ones. Don't fuck anything up.

If you think this sounds easy, you've either never done it, or you're lying, or you're one of those psychopaths who claims to enjoy doing laundry. But the point is: you _can_ do it. Not without difficulty. Not without cursing. Not without some youtube videos even. But eventually you and any other competent(-ish) adult out there will be able to do it.

But robots... that's another story.

## Why This Breaks Everything We've Built

Let's walk through what needs to happen, and watch our current AI infrastructure crumble like a sandcastle made by a drunk toddler:

**Getting In**: The robot needs to use a key. Not a digital key (though occasionally it can be), but an actual piece of metal that needs to go into a hole at the right angle with the right amount of force. Already, we've lost half the robotics labs at MIT.

**Navigation**: Now it's inside. There's a dog. There's a pile of shoes. There's that weird coat rack that looks like a person in the dark. The layout is completely novel. No pre-mapped environment. No clean laboratory floor. Just chaos. Pure, organic, human chaos.

**Bed Detection**: Which lumps in this house are beds? That futon that's technically a couch? The crib? The guest bed that's covered in clean laundry that hasn't been put away in three months? Good luck, YOLO algorithm.

**The Archeology Phase**: Time to excavate the old cover. Is it a zipper? Buttons? One of those fucking envelope-style ones where you just kind of... shove it in? Is the zipper hidden inside? Is it stuck? Has someone - god help us - *safety-pinned* it shut because they lost a button?

**The Treasure Hunt**: Find the clean covers. They're in a closet. Or maybe a drawer. Or maybe still in the dryer from last week. Maybe they're in that weird closet in the hallway that's too shallow to hang anything in but too deep to see the back of. The robot needs to search. It needs to reason about where humans put things. Spoiler: humans don't know where humans put things.

**Contextual Intelligence**: Once found, match the covers to the beds. The dinosaur cover goes on the kid's bed. Not the dog's bed (yes, there's a dog bed, and yes, it has a cover). The expensive Belgian linen goes on the master. This requires understanding human social structures, aesthetics, and the concept of "that one clearly belongs to a child."

**The Final Boss: Rag-Doll Physics**

And now we reach the circle of hell that Dante forgot to mention.

You've got a duvet insert. It weighs maybe three pounds but has the structural integrity of a jellyfish having an anxiety attack. You've got a cover that's essentially a fabric bag designed by someone who hates you personally.

You must put the jellyfish inside the bag.

The bag is slippery. It bunches. It folds in on itself. You grab one corner—the opposite corner slips out. You try the burrito method. You try the inside-out method. You try prayer. Nothing works smoothly.

**The Corner Problem** is where robotics PhDs go to cry. You need to get all four corners of the insert aligned with all four corners of the cover *simultaneously*. This requires:
- Manipulating eight points in 3D space
- Applying variable tension (too much = tear, too little = slippage)  
- Dealing with fabric that has near-zero friction against itself
- Doing this while holding an object that's trying to escape like it's been imprisoned unjustly

And then - THEN - you have to fasten it. Buttons that are designed to look nice, not work efficiently. Zippers that catch on fabric. Ties that need to be, you know, *tied*.

## Why This Matters More Than Your Benchmark

The DCT isn't some arbitrary torture test I invented because I'm bitter (though I am bitter). It's a perfect proxy for real-world capability.

A robot that can pass the DCT can probably:
- Do your laundry
- Clean your kitchen
- Organize your garage
- Help an elderly person get dressed
- Prepare a meal in an unfamiliar kitchen
- Pack a suitcase
- Unload your dishwasher and put things away *in the right place*

You know what a robot that can beat you at chess can do? Beat you at chess. Cool. I'll add that to my list of problems right after "nuclear fusion" and right before "the duvet cover."

## The Philosophy Hiding in the Fitted Sheet

Ok, you've been waiting for this I know, so let's _finally_ zoom out and look at the bigger picture, because the bigger picture is *hilarious* in a cosmic horror kind of way.

We're building toward AGI - actual thinking machines that might eventually outthink us on every dimension. They'll write our laws, design our cities, solve our physics problems, discover new mathematics, and maybe even figure out what the fuck dark matter is.

And yet.

*And yet.*

For decades - maybe generations - after we achieve that milestone, **we'll still be the ones changing the duvet covers.**

In other words', we'll be the robots' robots.

Think about it. Evolution spent 500 million years optimizing our ancestors for physical manipulation in unpredictable environments. Your hand is a fucking *miracle* of evolutionary engineering - 27 bones, 34 muscles, infinite configurations, fine-tuned haptic feedback that can feel the difference between silk and satin in the dark (yes, I used Claude to fact-check this, screw you).

Your brain's motor cortex, cerebellum, and proprioceptive systems are the result of countless iterations debugging the problem of "how to move meat through space without dying." You can reach into a bag without looking and identify objects by touch. You can catch a glass before it hits the ground. You can open a jar while holding a conversation.

These abilities feel mundane to you because you've been doing them since you were two. They're so beneath your conscious notice that you don't even realize they're the hardest problems in robotics.

Meanwhile, AI can prove mathematical theorems. It can predict protein folding. It can even make bloody ads! The *thinking* part? We're basically there. The "be a disembodied intelligence" part? Nailed it, or soon will.

But the physical world, the world of friction and gravity and objects that don't come with metadata, is a meatbag domain.

## The Great Inversion

So here's the dystopia nobody saw coming (ok, maybe Hans Moravec did):

The AI does all the thinking. It's the brain. The strategist. The innovator. It tells us what needs to happen, and why, and how to optimize for seventeen variables we didn't even know existed.

And we - the humans - we're the goddamn hands.

We're the ones climbing into the crawl space to run the cable it designed. We're the ones delicately soldering components in the factory it optimized. We're the ones harvesting the crops it predicted would be needed. We're the ones checking whether the patient's skin is clammy or feverish, because haptic sensors still can't quite capture what "clammy" *feels* like.

**We become the actuators in a system that's smarter than us.**

## The Duvet Cover Singularity

So maybe the real test isn't "Can AI think?"

Maybe it's "Can AI fold a fitted sheet?"

Because the civilization that cracks the DCT - the one that builds a robot that can handle the mundane, infuriating, physical bullshit of human existence - that's the civilization that's actually ready for the future.

Until then, we're building oracles who can tell us the meaning of life but can't help us move our couch.

And I'm not sure which future is stranger: the one where robots are our overlords, or the one where we're doing their laundry because their hands still suck.

Now if you'll excuse me, I have a duvet cover to fight. It's currently winning.


=====================================
The Sweeter Lesson
=====================================

In March 2019, just as Deep Learning revolution was gathering steam and the signs of the new AI Spring were starting to become widely visible, Rich Sutton published his ["Bitter Lesson"](https://www.cs.utexas.edu/~eunsol/courses/data/bitter_lesson.pdf).

His main argument can be broadly summarized as follows:

0. (Underlying Assumption): People tend to care about AI's specialized ability (e.g. translation, programming, playing chess, image understanding and manipulation, doing laundry etc.), not about its general ability to learn and reason in the abstract.

1. If you are trying to increase specialized ability, it's natural to try and imbue the AI agent with some human-validated knowledge about the problem domain. For example if a researcher wants the AI agent to play chess better, they would aim at extracting some deep knowledge about the nature of the game and decision-making heuristics from expert humans (grandmasters) and then put that knowledge into the programming of the agent.

2. This approach, though natural, is misguided. It may lead to short-term increases in specialized ability. But in the long run - raw increase in computing power will inevitably make the approach obsolete. General systems capable of end-to-end learning are going to find their own heuristics and generalizations more suitable to their way of thinking and the amount of compute available. The textbook example here is Deep Blue - the first computer to beat a Chess World Champion in 1997. It managed to achieve this level of specialised ability not through better "understanding of the game", but almost exclusively through brute-force search. Since then better approaches of generalized autonomous learning - such as self-play - have allowed computers to unlock superhuman abilities in even more "difficult" problem domains such as Go (difficulty here refers to the branching factor of possible positions, making pure search too costly for both humans and machines).

3. The bitter lesson is that working on specialized AI agents by focusing on increasing specialized knowledge is not an effective long-term strategy. A much better strategy is to focus on increasing general learning capability and providing the best possible specialized context. By context here I mean not just information, but also tools and interfaces with the world that are necessary for proper self-play or, more generally speaking, for real-world execution with a feedback loop. Different versions of such specialized context need to be provided both at training time and at execution time in order for the agent to be as effective as it can be.

[ diagram : the bitter lesson]

_The best way to increase an AI agent's special capability is to increase its general capability and provide it with special context_

(vs. sweeter lesson: there are very few interesting and economically valuable problem domains in which all we care about if obviously defined and clearly measured, one-dimensional performance metric. For the vast majority of human activity the reward function is ambiguous, personal, circumstantial and transient.

The real job is and has always been figuring out who _we_ are and what _we_ want.

//term: narrowmaxing behavior (!)
anthing that can be narrowmaxed - should be.

The economically valuable jobs will be the philosophically interesting ones.
)

[ /diagram ]

This lesson feels bitter because it effectively states that the specialized knowledge that human experts worked so hard to acquire and that they take great pride in - is not in any way absolute. It is in fact almost "useless" in an absolute sense. Sutton's principle shows that the hard-learned heuristics that humans develop when interacting with a certain problem domain are not inherently valuable or "truthful" in relation to this domain. They just represent a certain level of abstract patterns that seem relevant to us and suitable to our natural mode of computation as well as the amount of available compute. A different system with a different amount and type of computing power available can and will (if given enough opportunity) discover different patterns and heuristics in the same problem domain (provided that the problem domain itself is sufficiently complex). And these alternative patterns may very well prove to be superior to ours by any chosen metric (note how for the time being we excluded the very important questions of whether the agents have the same _goal_ in the environment. We will tackle that question later).

Let's pause here for a second. What we have posited above is that the only things that are "true" or "real" are the environment and the goal. The actual knowledge and strategy of achieving this goal - are always relative to the agent at play. In other words, the idea of Knowledge only makes sense in the context of a given Knower.

Here the reader, struck by the outrageous idea of the ultimate relativity of any knowledge, may ask: but what about the fundamental patterns of mathematics? Aren't math theorems "true" regardless of whether a human or a computer is doing the math? Wouldn't aliens know π is a special number because the relationship between the diameter of a circle and its diameter is a "real" thing? And what about the basic laws of physics? Isn't the discovery of Planck Constant a "real" discovery that makes sense and has value for any kind of mind in this universe?

The answer here, I'm afraid, is not as simple a "yes" as it seems.
We will put the Platonic discussions about whether math is discovered or invented aside. I don't have anything novel or useful to say in that regard. But let us assume that Pi does have a real significance. So there

[diagram]

truthfulness vs. usefulness

inherent complexity of a problem domain scale
from tic tac toe to full universe and beyond

the knower vs. the known

cognitive lens

[/diagram]

/// tic tac toe. consider chess.
/// rulliad
///underpromotion

//continuum on how simple the problem domain is. //

This is a general principle that applies to almost every practical problem domain, with pure mathematics being possibly the only exception. And even there - while the patterns human can discover and prove as theorems are still "truthful" for machines or other minds operating within the same formal system, these patterns may not be "useful" to them in the same way. And other minds may be able to discover other true patterns that we wouldn't be able to comprehend.

The bitter lesson may feel humiliating, or diminishing our value, but it doesn't have to be this way. The only thing it diminishes is our false and damaging exeptionalist sentiment when it comes to intellectual work.

Let me illustrate this point but switching our attention from solving abstract problems in abstract spaces, to solving a very concrete problem of locomotion.  We all take it for granted that moving one foot in front of the other (a.k.a. running) is an optimal strategy of getting from A to B for a human (without the aid of tools). It's not the only strategy: we can crawl, we can hop, some of us can walk on our hands. But walking or running is clearly the best way (if we are travelling on land). However this idea of moving one foot in front of the other - would be ridiculous for an organism or mechanism of a different shape and physical configuration.

For a bird - flying would be faster. A snake doesn't have any feet to put in front of each other. A centipede wouldn't get very far by moving only two legs. And cars can get from A to B more efficiently by utilizing a wheel.

No matter what key metric you choose for evaluating locomotion quality (speed, energy efficiency, quitness... etc.), there is always going to be a way of improving that metric beyond the level of human capability - and that way will most likely not involve legs.

But in the context of locomotion we don't take this fact to seriously, we never had false exceptionalist assimptions that "moving one foot in front of the other is best" heuristic was a universal rule of space. It's quite clear that our preferred heuristic is saying much more about our anatomy than about the nature of space.

I want to emphasize that the fact it's not universal doesn't make this heuristic less valuable to us. Neither does it render the process of discovering better human heuristics obsolete. When the [Fosbury Flop](https://en.wikipedia.org/wiki/Fosbury_flop) was discovered and popularized in the 1960s - it represented a genuine progression in the sport of high jumping. Nobody looked at it and said "oh, this isn't remarkable or valuable because Kangaroo rats can jump so much higher relative
the their body size and they do it using a very different technique".

The key insight here is that in sports it was always clear that the actor was a part of the problem space. The problem space in high jumping is how high can unassisted humans jump, not how high anything can jump.

The good thing about jumping or running is that we were never under the illusion that we as humans were exceptional in it in some absolute sense. Lots of things jump around. And if we want to design a system that can jump higher - we would get lots of ideas of different methods from human and non-human jumpers.

Now when we go back to thinking tasks - things tend to feel different. For all we know Kangaroo rats don't write poetry and don't play chess. And the lack of diversity in observable methods of doing intellectual and creative labour led us to believe that our way of doing it was _the_ way.

Now as capable AI systems are coming along - we are faced with the reality that "we were part of the problem" all along.


=====================================
Wave Particle Consciousness
=====================================

Physics spent decades fighting over whether light was a wave or a particle. The answer turned out to be: yes.

Consciousness might work the same way. Below is a short meditation on this possibility.

### 1. The Wave

Neuroscience keeps finding that consciousness correlates with synchrony. Coordinated activity. Brain regions firing in phase. Oscillations locking together. The technical term is "phase synchrony," but the poetic term is resonance. You are conscious when the collective (the society of mind) that makes you up moves in resonance.

Michael Timothy Bennett recently formalized this intuition. In his paper "A Mind Cannot Be Smeared Across Time," he proves something that sounds obvious but isn't: a system can process all the ingredients of a conscious moment across time without ever instantiating the conjunction — the moment itself.

Sequential processing isn't enough. The parts have to sync in objective time.

Let us call it the wave view: consciousness isn't what the brain computes, but the integration of what it computes. The standing wave across neurons. The field, not the particles.

Thomas Metzinger lands somewhere similar. His "phenomenal self-model" isn't a thing the brain builds — it's a pattern the brain is. You don't have a self-model; you are one. And the model is transparent: it doesn't see itself as a model, just as the eye doesn't see itself seeing.

Every other "integration"-based theory (global workspace, integrated information, attention schema etc.) dances around the same core assumption.

Consciousness as wave: the resonance is the mind.

### 2. The Particle

But we also experience consciousness as a stream of discrete events. Thoughts. Perceptions. Decisions. One thing after another, like beads on a string.

Let's call it the particle view. The one that makes LLMs seem plausibly conscious — or at least conscious-ish. After all, what's a mind but a next token (or world state) predictor with very good priors? Process enough information the right way, and maybe something is home.

Functionalism lives here. All sorts of computationalisms too. This view assumes that minds are what certain algorithms do, regardless of what substrate they're implemented on.

The particle view says: it's the processing events that make the mind.

### 3. The Collapse

In quantum mechanics, the wave function doesn't "become" a particle until you measure it. Before measurement: superposition, possibility, everything at once, unborn. After: one outcome, definite, here, done, dead.

What if consciousness works like this?

The wave is the resonance — the synchronized field of neural activity that integrates everything into a unified "moment." But it's not articulate. It doesn't have content you can report. It just is. And in this isness is the possibility for the whole world to exist.

Articulation is the collapse. When you put something into words. When you notice what you're feeling. When a thought crystallizes out of the resonant background hum.

Qualia — the "what it's like" — might be what happens at the collapse. The wave becomes a particle. The field becomes a fact.

This hints at why introspection changes what it observes. Why the deepest experiences resist description. Why meditators report that staying before the thought is qualitatively different from thinking.

The collapse is where content is born. And where the core of consciousness dies.
Before the collapse, there's consciousness — but no thing you're conscious of.
After the collapse, there is the content of the thought — but no more consciousness out of which that thought was born.

The "self" — the voice in your head, the sense that there's a you doing the thinking — lives downstream of the collapse. It's made of collapsed particles. Thoughts about thoughts. Memories of perceptions. A story stitched from beads **after** the fact.

The small I experiences itself as the cause of articulation. It feels like the thinker thinking the thoughts. But it might actually be a product of the collapse, mistaking itself for the collapser.

The dance dancing itself, then claiming to be the dancer.

### 4. The Lantern, The Spotlight and The Samadhi

Alison Gopnik distinguished two modes of consciousness:

Spotlight: focused, selective, goal-directed. Adult consciousness. You pick one thing, ignore the rest, optimize.

Lantern: diffuse, open, taking in everything without categorizing or filtering. Infant consciousness. The whole field, nothing excluded.

Lantern is pure wave-mode. It lets the field be without crystallizing. Spotlight is collapse-ready. It's already selecting what will become a particle.

We start as lanterns. We get trained into spotlights. By the time we're adults, we've forgotten there was ever another way.

But every contemplative tradition says that even for adults there's a shift available.

You can stop identifying just with the collapsed particles. You can stop believing you're the voice, the thinker, the story. And shift the locus of your self upstream — to the resonance itself, which feels like unifying the particles back into the wave they are born from ("samadhi" in Sanskrit literally means "collecting the mind back together"). It's simply unlearning the spotlight habit. Remembering how to lantern.

The mystics call it "witness consciousness" or "awareness aware of itself." It's not that you become the wave — it's that you notice you always were. The small I was a ripple pattern, not the water.

Awakening is shifting the locus of identity from the post-collapse debris field to the pre-collapse resonance.

From particle to wave.

"In the beginning was the Word" (John 1:1) says the Bible, and the LLM folks and "Turing-machine-is-all-you-need" computationalists agree.

"In the beginning was the World" says Michael Timothy Bennett and all the other "embodiment is not an afterthought" folks.

"In the beginning was the Valence" says Mark Solms.

But today I feel like "in the beginning — there **is** a Wave". And everything else (the words, worlds, the valence) is downstream of this wave's ongoing and never-ending collapse.

---
#### footnotes

:
     Marvin Minsky, *Society of Mind* (1986). The idea that the mind is not a single entity but a collection of simpler processes ("agents") that together produce what we experience as intelligence and consciousness.

:
     Michael Timothy Bennett, "A Mind Cannot Be Smeared Across Time" ([arXiv:2601.11620](https://arxiv.org/abs/2601.11620), 2026). Bennett augments his Stack Theory with temporal semantics to prove that "existential temporal realisation does not preserve conjunction" — meaning a system can process all components of a conscious moment sequentially without ever instantiating them together. He distinguishes StrongSync (simultaneous co-instantiation required) from WeakSync (temporal distribution permitted) and argues that consciousness attribution requires architectural inspection, not just functional performance. See also his earlier "Emergent Causality & the Foundation of Consciousness" (Best Student Paper, 16th International Conference on Artificial General Intelligence, Stockholm, 2023).

:
     Thomas Metzinger, *Being No One: The Self-Model Theory of Subjectivity* (MIT Press, 2003). For a more accessible treatment of the same ideas, see his *The Ego Tunnel: The Science of the Mind and the Myth of the Self* (Basic Books, 2009).

:
     The three main "integration"-flavored theories of consciousness: Global Workspace Theory — Bernard Baars, *A Cognitive Theory of Consciousness* (Cambridge University Press, 1988); Integrated Information Theory — Giulio Tononi, "An Information Integration Theory of Consciousness," *BMC Neuroscience* (2004); Attention Schema Theory — Michael Graziano, *Consciousness and the Social Brain* (Oxford University Press, 2013). They differ significantly in their details, but all share the intuition that consciousness arises from some form of large-scale integration or coordination across the brain.

:
     I'm using quantum mechanics here as a structural analogy, not as a claim about literal quantum processes in the brain. This is not Penrose-Hameroff. The wave-particle duality is a metaphor for two modes of description that seem mutually exclusive until you realize they are complementary aspects of the same phenomenon.

:
     Alison Gopnik, *The Philosophical Baby: What Children's Minds Tell Us About Truth, Love, and the Meaning of Life* (Farrar, Straus and Giroux, 2009).

:
     This is also reminiscent of what Iain McGilchrist describes in *The Master and His Emissary* (Yale University Press, 2009): the right hemisphere as the broader, contextual, "wave-like" mode and the left hemisphere as the narrowing, articulating, "particle-like" mode. The emissary (left hemisphere, the particle-maker) has usurped the master (right hemisphere, the wave-keeper).

:
     From the Sanskrit root *sam-ā-dhā*: *sam* (together) + *ā* (towards) + *dhā* (to place, to hold). Literally: to place or collect together. The Yoga Sutras of Patanjali (circa 2nd century BCE) define samadhi as the state in which the mind becomes one with the object of meditation — the distinction between observer and observed dissolves.

:
     See Bennett's work on embodied cognition and the argument that intelligence cannot be separated from its physical embedding in the world. Also relevant: the broader 4E cognition movement (embodied, embedded, enacted, extended) — see, for example, Evan Thompson, *Mind in Life: Biology, Phenomenology, and the Sciences of Mind* (Harvard University Press, 2007).

:
     Mark Solms, *The Hidden Spring: A Journey to the Source of Consciousness* (W. W. Norton, 2021). Solms argues that consciousness begins not in the cortex but in the brainstem, with affect and valence — the felt sense of good-or-bad — as its most primitive and fundamental form.


