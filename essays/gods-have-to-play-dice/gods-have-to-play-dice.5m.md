The story of human progress is one of outsourcing burdens while trying to maintain a sense of control. We started by outsourcing physical labor during the Industrial Revolution. Then came the Information Revolution, where we outsourced remembering and computing. Now we're entering the Autonomy Revolution - outsourcing the very act of decision-making itself.

This final frontier is the trickiest because it strikes at the heart of human agency. Technology has always been an agency sink - every time we delegate an action, we give up some control. Take driving: when you switch from walking to driving, you trade expanded range for limited path choices. Over time, this shapes not just where you can go, but where you want to go and even what destinations you can imagine.

We maintain the illusion of choice - "I could still walk if I wanted to" - but the reality is more complex. Having a car makes walking feel like an unnecessary sacrifice. Living arrangements adapt around car ownership. Entire neighborhoods become unwalkable. Yet we accept this trade-off because the benefits seem worth it.

But when it comes to algorithms making decisions for us, our comfort level varies dramatically based on context. We're fine with AI choosing our taxi driver or navigation route. We get nervous when it starts influencing our entertainment choices or dating options. We reject it outright for death penalty decisions.

Why this variation? It comes down to six key factors:

1. Cost of mistake - how bad would a wrong decision be?
2. Cost of deliberation - how hard is it to decide ourselves?
3. Perceived competence - how capable is the AI compared to humans?
4. Relatability - how well can we understand and identify with it?
5. Blameability - can we hold it accountable for errors?
6. Forgivability - how easily can we move past its mistakes?

The first three factors are about performance, while the last three are about psychology and social dynamics. This framework suggests three possible approaches to designing AI decision-makers:

1. The Specialist - Highly competent, relatable, and accountable systems that function like trusted human experts
2. The Assistant - Partially autonomous systems that suggest "smart defaults" while leaving final control with humans
3. The Oracle - Completely autonomous but intentionally mysterious systems that act as higher powers

Each approach has its place. A self-driving car might use all three: acting as a specialist during parking, an assistant on highways, and an oracle in unavoidable crash scenarios where human deliberation is impossible.

This last "oracle mode" might seem like a cop-out, but it addresses a deep human need. When decisions become too complex or consequential, we often prefer to surrender agency to fate or higher powers. This is why we still flip coins for life-changing choices - it's not about the coin's competence, but about escaping the burden of choice.

As we build increasingly autonomous systems, we must recognize this paradox: humans sometimes prefer mysterious, unaccountable decision-makers over transparent ones. Our digital gods, like their predecessors, may need to maintain some mystery. They must learn when to be precise and when to embrace randomness, when to explain and when to simply decree.

The challenge isn't just technical but deeply psychological. We're not just building decision-making systems - we're creating new relationships with agency itself. Understanding these relationships will be crucial as we navigate the Autonomy Revolution.